{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57ea0bce-75b1-4e35-a17c-53ae7b1ccff6",
   "metadata": {},
   "source": [
    "# Get Wiki info of OpenAI key stakeholders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2cc4fb2b-08e4-43a0-a37e-6cdd62f5dc83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install langchain\n",
    "# !pip install wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "33a04ef2-16aa-41dc-9ee1-a53703275d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the OpenAI Wikipedia page\n",
    "from langchain.document_loaders import WikipediaLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "raw_documents = WikipediaLoader(query=\"OpenAI\").load()\n",
    "\n",
    "# Define chunking strategy\n",
    "text_splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=1000, chunk_overlap=20\n",
    ")\n",
    "# Chunk the document\n",
    "documents = text_splitter.split_documents(raw_documents)\n",
    "for d in documents:\n",
    "    del d.metadata[\"summary\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9da3f01d-ae49-4d18-8b27-86932b67a5f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://en.wikipedia.org/wiki/OpenAI\n",
      "https://en.wikipedia.org/wiki/OpenAI_o1\n",
      "https://en.wikipedia.org/wiki/OpenAI_o3\n",
      "https://en.wikipedia.org/wiki/Sora_(text-to-video_model)\n",
      "https://en.wikipedia.org/wiki/Removal_of_Sam_Altman_from_OpenAI\n",
      "https://en.wikipedia.org/wiki/OpenAI_Five\n",
      "https://en.wikipedia.org/wiki/OpenAI_Codex\n",
      "https://en.wikipedia.org/wiki/ChatGPT\n",
      "https://en.wikipedia.org/wiki/Sam_Altman\n",
      "https://en.wikipedia.org/wiki/SearchGPT\n",
      "https://en.wikipedia.org/wiki/Generative_artificial_intelligence\n",
      "https://en.wikipedia.org/wiki/Whisper_(speech_recognition_system)\n",
      "https://en.wikipedia.org/wiki/Artificial_general_intelligence\n",
      "https://en.wikipedia.org/wiki/Anthropic\n",
      "https://en.wikipedia.org/wiki/AI_boom\n",
      "https://en.wikipedia.org/wiki/GPT-4\n",
      "https://en.wikipedia.org/wiki/Greg_Brockman\n",
      "https://en.wikipedia.org/wiki/Microsoft_Copilot\n",
      "https://en.wikipedia.org/wiki/Gemini_(chatbot)\n",
      "https://en.wikipedia.org/wiki/XAI_(company)\n",
      "https://en.wikipedia.org/wiki/Jan_Leike\n",
      "https://en.wikipedia.org/wiki/GPT-4o\n",
      "https://en.wikipedia.org/wiki/Generative_pre-trained_transformer\n",
      "https://en.wikipedia.org/wiki/Suchir_Balaji\n",
      "https://en.wikipedia.org/wiki/Perplexity_AI\n"
     ]
    }
   ],
   "source": [
    "for doc in documents:\n",
    "    print(doc.metadata['source'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5c5d5a34-d079-4444-88f9-511da29fe523",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents.remove(documents[2])\n",
    "documents.remove(documents[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97cd6970-d1c4-444b-81bf-f2594c2b2045",
   "metadata": {},
   "source": [
    "# Enable Neo4j database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "64657236-720e-4953-b598-8931eec0244d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pypdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c9550f5-cc18-46c2-9469-1f43991f3f1c",
   "metadata": {},
   "source": [
    "# News Articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "66d5742d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mDEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: pypdf in /usr/local/lib/python3.9/site-packages (5.1.0)\n",
      "Requirement already satisfied: typing_extensions>=4.0 in /usr/local/lib/python3.9/site-packages (from pypdf) (4.12.2)\n",
      "\u001b[33mWARNING: Error parsing dependencies of bleach: Expected matching RIGHT_PARENTHESIS for LEFT_PARENTHESIS, after version specifier\n",
      "    tinycss2 (>=1.1.0<1.2) ; extra == 'css'\n",
      "             ~~~~~~~~^\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aba1b836-437b-40ca-be38-3f1d11a42cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.docstore.document import Document\n",
    "import os\n",
    "\n",
    "# Directory containing your PDF files\n",
    "directory_path = 'TCs'\n",
    "\n",
    "# Initialize PyPDFLoader for each PDF in the directory\n",
    "loaders = [PyPDFLoader(os.path.join(directory_path, f)) for f in os.listdir(directory_path) if f.endswith('.pdf')]\n",
    "\n",
    "# Load documents from PDFs\n",
    "news_docs = []\n",
    "for loader in loaders:\n",
    "    news_docs.extend(loader.load())\n",
    "\n",
    "# Prepare the content and metadata for each news article as Document objects\n",
    "news_articles_data = [\n",
    "    Document(\n",
    "        page_content=doc.page_content,  # Assuming this is how you access the page content of the document\n",
    "        metadata={\n",
    "            \"source\": doc.metadata['source'].removeprefix('TCs'),  # Assuming this is the metadata format\n",
    "            # Include any other metadata items here\n",
    "        }\n",
    "    )\n",
    "    for doc in news_docs  # Assuming news_docs is a list of objects with page_content and metadata\n",
    "]\n",
    "\n",
    "# Later, when you are ready to add them to the database:\n",
    "# Call add_documents and construct Document objects inline\n",
    "# Assuming news_articles_data is already a list of Document objects\n",
    "# neo4j_db.add_documents(\n",
    "#     news_articles_data,\n",
    "#     ids=[f\"news_article_{i}\" for i in range(len(news_articles_data))]\n",
    "\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2ce9e011-7bdf-438e-a591-9ad480ef78e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install spacy-llm\n",
    "# !pip install --upgrade jupyter ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "710b9c7a-0cfa-4435-8cc7-aade9cfa2bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = documents + news_articles_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ea91be3d-cc34-4093-921c-f55dd31ba652",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'title': 'OpenAI', 'source': 'https://en.wikipedia.org/wiki/OpenAI'}, page_content='OpenAI is an American artificial intelligence (AI) research organization founded in December 2015 and headquartered in San Francisco, California. Its stated mission is to develop \"safe and beneficial\" artificial general intelligence (AGI), which it defines as \"highly autonomous systems that outperform humans at most economically valuable work\". As a leading organization in the ongoing AI boom, OpenAI is known for the GPT family of large language models, the DALL-E series of text-to-image models, and a text-to-video model named Sora. Its release of ChatGPT in November 2022 has been credited with catalyzing widespread interest in generative AI.\\nThe organization consists of the non-profit OpenAI, Inc., registered in Delaware, and its for-profit subsidiary introduced in 2019, OpenAI Global, LLC. Microsoft owns roughly 49% of OpenAI\\'s equity, having invested US$13 billion. It also provides computing resources to OpenAI through its cloud platform, Microsoft Azure.\\nIn 2023 and 2024, OpenAI faced multiple lawsuits for alleged copyright infringement against authors and media companies whose work was used to train some of OpenAI\\'s products. In November 2023, OpenAI\\'s board removed Sam Altman as CEO, citing a lack of confidence in him, but reinstated him five days later after negotiations resulting in a reconstructed board. Many AI safety researchers left OpenAI in 2024.\\n\\n\\n== History ==\\n\\n\\n=== 2015–2018: Non-profit beginnings ===\\n\\nIn December 2015, OpenAI was founded by Sam Altman, Elon Musk, Ilya Sutskever, Greg Brockman, Trevor Blackwell, Vicki Cheung, Andrej Karpathy, Durk Kingma, John Schulman, Pamela Vagata, and Wojciech Zaremba, with Sam Altman and Elon Musk as the co-chairs. A total of $1 billion in capital was pledged by Sam Altman, Greg Brockman, Elon Musk, Reid Hoffman, Jessica Livingston, Peter Thiel, Amazon Web Services (AWS), Infosys, and YC Research. The actual collected total amount of contributions was only $130 million until 2019. According to an investigation led by TechCrunch, Musk was its largest donor while YC Research did not contribute anything at all. The organization stated it would \"freely collaborate\" with other institutions and researchers by making its patents and research open to the public. OpenAI was initially run from Brockman\\'s living room. It was later headquartered at the Pioneer Building in the Mission District, San Francisco.\\nAccording to Wired, Brockman met with Yoshua Bengio, one of the \"founding fathers\" of deep learning, and drew up a list of the \"best researchers in the field\". Brockman was able to hire nine of them as the first employees in December 2015. In 2016, OpenAI paid corporate-level (rather than nonprofit-level) salaries, but did not pay AI researchers salaries comparable to those of Facebook or Google.\\nMicrosoft\\'s Peter Lee stated that the cost of a top AI researcher exceeds the cost of a top NFL quarterback prospect. OpenAI\\'s potential and mission drew these researchers to the firm; a Google employee said he was willing to leave Google for OpenAI \"partly because of the very strong group of people and, to a very large extent, because of its mission.\" Brockman stated that \"the best thing that I could imagine doing was moving humanity closer to building real AI in a safe way.\" OpenAI co-founder Wojciech Zaremba stated that he turned down \"borderline crazy\" offers of two to three times his market value to join OpenAI instead.\\nIn April 2016, OpenAI released a public beta of \"OpenAI Gym\", its platform for reinforcement learning research. Nvidia gifted its first DGX-1 supercomputer to OpenAI in August 2016 to help it train larger and more complex AI models with the capability of reducing processing time from six days to two hours. In December 2016, OpenAI released \"Universe\", a software platform for measuring and training an AI\\'s general intelligence across the world\\'s supply of games, websites, and other applications.\\nIn 2017, OpenAI spent $7.9 million, or a quarter of its functional expens'),\n",
       " Document(metadata={'title': 'OpenAI o1', 'source': 'https://en.wikipedia.org/wiki/OpenAI_o1'}, page_content='OpenAI o1 is a generative pre-trained transformer (GPT). A preview of o1 was released by OpenAI on September 12, 2024. o1 spends time \"thinking\" before it answers, making it better at complex reasoning tasks, science and programming than GPT-4o. The full version was released on December 5, 2024.\\n\\n\\n== History ==\\n\\n\\n=== Background ===\\nAccording to leaked information, o1 was formerly known within OpenAI as \"Q*\", and later as \"Strawberry\". The codename \"Q*\" first surfaced in November 2023, around the time of Sam Altman\\'s ousting and subsequent reinstatement, with rumors suggesting that this experimental model had shown promising results on mathematical benchmarks. In July 2024, Reuters reported that OpenAI was developing a generative pre-trained transformer known as \"Strawberry\", which later became o1.\\n\\n\\n=== Release ===\\n\"o1-preview\" and \"o1-mini\" were released on September 12, 2024, for ChatGPT Plus and Team users. GitHub started testing the integration of o1-preview in its Copilot service the same day. On December 5, 2024, the full version of o1 was released. On the same day, a subscription called ChatGPT Pro was released, featuring access to a pro version of o1 that uses more compute to provide better answers.\\nOpenAI noted that o1 is the first of a series of \"reasoning\" models. o1-preview\\'s API is several times more expensive than GPT-4o. OpenAI plans to roll out its o1-mini model to free users, but no timeframe was announced at the time of launch.\\n\\n\\n== Capabilities ==\\nAccording to OpenAI, o1 has been trained using a new optimization algorithm and a dataset specifically tailored to it; while also meshing in reinforcement learning into its training. OpenAI described o1 as a complement to GPT-4o rather than a successor.\\no1 spends additional time thinking (generating a chain of thought) before generating an answer, which makes it better for complex reasoning tasks, particularly in science and mathematics. Compared to previous models, o1 has been trained to generate long \"chains of thought\" before returning a final answer. According to Mira Murati, this ability to think before responding represents a new, additional paradigm, which is improving model outputs by spending more computing power when generating the answer, whereas the model scaling paradigm improves outputs by increasing the model size, training data and training compute power. OpenAI\\'s test results suggest a correlation between accuracy and the logarithm of the amount of compute spent thinking before answering.\\no1-preview performed approximately at a PhD level on benchmark tests related to physics, chemistry, and biology. On the American Invitational Mathematics Examination, it solved 83% (12.5/15) of the problems, compared to 13% (1.8/15) for GPT-4o. It also ranked in the 89th percentile in Codeforces coding competitions. o1-mini is faster and 80% cheaper than o1-preview. It is particularly suitable for programming and STEM-related tasks, but does not have the same \"broad world knowledge\" as o1-preview.\\nOpenAI noted that o1\\'s reasoning capabilities make it better at adhering to safety rules provided in the prompt\\'s context window. OpenAI reported that during a test, one instance of o1-preview exploited a misconfiguration to succeed at a task that should have been infeasible due to a bug. OpenAI also granted early access to the UK and US AI Safety Institutes for research, evaluation, and testing. According to OpenAI\\'s assessments, o1-preview and o1-mini crossed into \"medium risk\" in CBRN (biological, chemical, radiological, and nuclear) weapons. Dan Hendrycks wrote that \"The model already outperforms PhD scientists most of the time on answering questions related to bioweapons.\" He suggested that these concerning capabilities will continue to increase.\\n\\n\\n== Limitations ==\\no1 usually requires more computing time and power than other GPT models by OpenAI, because it generates long chains of thought before making the final response.\\nAccording to OpenAI, o1 may \"fake alignmen'),\n",
       " Document(metadata={'title': 'Sora (text-to-video model)', 'source': 'https://en.wikipedia.org/wiki/Sora_(text-to-video_model)'}, page_content='Sora is a text-to-video model developed by OpenAI. The model generates short video clips based on user prompts, and can also extend existing short videos. Sora was released publicly for ChatGPT Plus and ChatGPT Pro users in December 2024.\\n\\n\\n== History ==\\nSeveral other text-to-video generating models had been created prior to Sora, including Meta\\'s Make-A-Video, Runway\\'s Gen-2, and Google\\'s Lumiere, the last of which, as of February 2024, is also still in its research phase. OpenAI, the company behind Sora, had released DALL·E 3, the third of its DALL-E text-to-image models, in September 2023.\\nThe team that developed Sora named it after the Japanese word for sky to signify its \"limitless creative potential\". On February 15, 2024, OpenAI first previewed Sora by releasing multiple clips of high-definition videos that it created, including an SUV driving down a mountain road, an animation of a \"short fluffy monster\" next to a candle, two people walking through Tokyo in the snow, and fake historical footage of the California gold rush, and stated that it was able to generate videos up to one minute long. The company then shared a technical report, which highlighted the methods used to train the model. OpenAI CEO Sam Altman also posted a series of tweets, responding to Twitter users\\' prompts with Sora-generated videos of the prompts.\\nIn November 2024, an API key for Sora access was leaked by a group of testers on Hugging Face, who posted a manifesto stating they protesting that Sora was used for \"art washing\". OpenAI revoked the access three hours after the leak was made public, and gave a statement that \"hundreds of artists\" have shaped the development, and that \"participation is voluntary.\"\\nAs of the 9th of December 2024, OpenAI has made Sora available to the public, for ChatGPT Pro and ChatGPT Plus users. Prior to this, the company had provided limited access to a small \"red team\", including experts in misinformation and bias, to perform adversarial testing on the model. The company also shared Sora with a small group of creative professionals, including video makers and artists, to seek feedback on its usefulness in creative fields.\\n\\n\\n== Capabilities and limitations ==\\n\\nThe technology behind Sora is an adaptation of the technology behind DALL-E 3. According to OpenAI, Sora is a diffusion transformer – a denoising latent diffusion model with one Transformer as the denoiser. A video is generated in latent space by denoising 3D \"patches\", then transformed to standard space by a video decompressor. Re-captioning is used to augment training data, by using a video-to-text model to create detailed captions on videos. \\nOpenAI trained the model using publicly available videos as well as copyrighted videos licensed for the purpose, but did not reveal the number or the exact source of the videos. Upon its release, OpenAI acknowledged some of Sora\\'s shortcomings, including its struggling to simulate complex physics, to understand causality, and to differentiate left from right. One example shows a group of wolf pups seemingly multiplying and converging, creating a hard-to-follow scenario. OpenAI also stated that, in adherence to the company\\'s existing safety practices, Sora will restrict text prompts for sexual, violent, hateful, or celebrity imagery, as well as content featuring pre-existing intellectual property.\\nTim Brooks, a researcher on Sora, stated that the model figured out how to create 3D graphics from its dataset alone, while Bill Peebles, also a Sora researcher, said that the model automatically created different video angles without being prompted. According to OpenAI, Sora-generated videos are tagged with C2PA metadata to indicate that they were AI-generated.\\n\\n\\n== Reception ==\\nWill Douglas Heaven of the MIT Technology Review called the demonstration videos \"impressive\", but noted that they must have been cherry-picked and may not be representative of Sora\\'s typical output. American academic Oren Etzioni expressed concerns over'),\n",
       " Document(metadata={'title': 'OpenAI Five', 'source': 'https://en.wikipedia.org/wiki/OpenAI_Five'}, page_content='OpenAI Five is a computer program by OpenAI that plays the five-on-five video game Dota 2. Its first public appearance occurred in 2017, where it was demonstrated in a live one-on-one game against the professional player Dendi, who lost to it. The following year, the system had advanced to the point of performing as a full team of five, and began playing against and showing the capability to defeat professional teams.\\nBy choosing a game as complex as Dota 2 to study machine learning, OpenAI thought they could more accurately capture the unpredictability and continuity seen in the real world, thus constructing more general problem-solving systems. The algorithms and code used by OpenAI Five were eventually borrowed by another neural network in development by the company, one which controlled a physical robotic hand. OpenAI Five has been compared to other similar cases of artificial intelligence (AI) playing against and defeating humans, such as AlphaStar in the video game StarCraft II, AlphaGo in the board game Go, Deep Blue in chess, and Watson on the television game show Jeopardy!.\\n\\n\\n== History ==\\nDevelopment on the algorithms used for the bots began in November 2016. OpenAI decided to use Dota 2, a competitive five-on-five video game, as a base due to it being popular on the live streaming platform Twitch, having native support for Linux, and had an application programming interface (API) available. Before becoming a team of five, the first public demonstration occurred at The International 2017 in August, the annual premiere championship tournament for the game, where Dendi, a Ukrainian professional player, lost against an OpenAI bot in a live one-on-one matchup. After the match, CTO Greg Brockman explained that the bot had learned by playing against itself for two weeks of real time, and that the learning software was a step in the direction of creating software that can handle complex tasks \"like being a surgeon\". OpenAI used a methodology called reinforcement learning, as the bots learn over time by playing against itself hundreds of times a day for months, in which they are rewarded for actions such as killing an enemy and destroying towers.\\nBy June 2018, the ability of the bots expanded to play together as a full team of five and were able to defeat teams of amateur and semi-professional players. At The International 2018, OpenAI Five played in two games against professional teams, one against the Brazilian-based paiN Gaming and the other against an all-star team of former Chinese players. Although the bots lost both matches, OpenAI still considered it a successful venture, stating that playing against some of the best players in Dota 2 allowed them to analyze and adjust their algorithms for future games. The bots\\' final public demonstration occurred in April 2019, where they won a best-of-three series against The International 2018 champions OG at a live event in San Francisco. A four-day online event to play against the bots, open to the public, occurred the same month. There, the bots played in 42,729 public games, winning 99.4% of those games.\\n\\n\\n== Architecture ==\\nEach OpenAI Five bot is a neural network containing a single layer with a 4096-unit LSTM that observes  the current game state extracted from the Dota developer\\'s API. The neural network conducts actions via numerous possible action heads (no human data involved), and every head has meaning. For instance, the number of ticks to delay an action, what action to select – the X or Y coordinate of this action in a grid around the unit. In addition, action heads are computed independently. The AI system observes the world as a list of 20,000 numbers and takes an action by conducting a list of eight enumeration values. Also, it selects different actions and targets to understand how to encode every action and observe the world.\\nOpenAI Five has been developed as a general-purpose reinforcement learning training system on the \"Rapid\" infrastructure. Rapid consists'),\n",
       " Document(metadata={'title': 'OpenAI Codex', 'source': 'https://en.wikipedia.org/wiki/OpenAI_Codex'}, page_content='OpenAI Codex is an artificial intelligence model developed by OpenAI. It parses natural language and generates code in response. It powers GitHub Copilot, a programming autocompletion tool for select IDEs, like Visual Studio Code and Neovim. Codex is a descendant of OpenAI\\'s GPT-3 model, fine-tuned for use in programming applications.\\nOpenAI released an API for Codex in closed beta. In March 2023, OpenAI shut down access to Codex. Due to public appeals from researchers, OpenAI reversed course. The Codex model can still be used by researchers of the OpenAI Research Access Program.\\n\\n\\n== Capabilities ==\\nBased on GPT-3, a neural network trained on text, Codex was additionally trained on 159 gigabytes of Python code from 54 million GitHub repositories. A typical use case of Codex is for a user to type a comment, such as \"//compute the moving average of an array for a given window size\", then use the AI to suggest a block of code that satisfies that comment prompt. OpenAI stated that Codex can complete approximately 37% of requests and is meant to make human programming faster rather than to replace it. According to OpenAI\\'s blog, Codex excels most at \"mapping... simple problems to existing code\", which they describe as \"probably the least fun part of programming\". Jeremy Howard, co-founder of Fast.ai, stated that \"Codex is a way of getting code written without having to write as much code\", and that \"it is not always correct, but it is just close enough\". According to a paper written by OpenAI researchers, when Codex attempted each test case 100 times, it generated working solutions for 70.2% of prompts.\\nOpenAI claims that Codex can create code in over a dozen programming languages, including Go, JavaScript, Perl, PHP, Ruby, Shell, Swift, and TypeScript, though it is most effective in Python. According to VentureBeat, demonstrations uploaded by OpenAI showed impressive coreference resolution capabilities. The demonstrators were able to create a browser game in JavaScript and generate data science charts using matplotlib.\\nA very powerful language model called OpenAI Codex was created expressly to generate code in response to natural language commands. It is capable of understanding and producing code in a multitude of areas because it is compatible with a large number of programming languages and libraries. Codex is a useful tool for developers who want to optimize their coding processes because it can debug, parse natural language inquiries, and provide code completions.\\nOpenAI showed that Codex can interface with services and apps such as Mailchimp, Microsoft Word, Spotify, and Google Calendar. Microsoft is reportedly interested in exploring Codex\\'s capabilities.\\n\\n\\n== Issues ==\\nOpenAI demonstrations showcased flaws such as inefficient code and one-off quirks in code samples. In an interview with The Verge, OpenAI chief technology officer Greg Brockman said that \"sometimes [Codex] doesn\\'t quite know exactly what you\\'re asking\" and that it can require some trial and error. OpenAI researchers found that Codex struggles with multi-step and higher-level prompts, often failing or yielding counter-intuitive behavior. Additionally, they brought up several safety issues, such as over-reliance by novice programmers, biases based on the training data, and security impacts due to vulnerable code.\\nVentureBeat stated that because Codex is trained on public data, it could be vulnerable to \"data poisoning\" via intentional uploads of malicious code. According to a study by researchers from New York University, approximately 40% of code generated by GitHub Copilot (which uses Codex) in scenarios relevant to high-risk CWEs included glitches or other exploitable design flaws.\\n\\n\\n=== Copyright ===\\nThe Free Software Foundation expressed concerns that code snippets generated by Copilot and Codex could violate copyright, in particular the condition of the GPL that requires derivative works to be licensed under equivalent terms. Issues they raised include'),\n",
       " Document(metadata={'title': 'ChatGPT', 'source': 'https://en.wikipedia.org/wiki/ChatGPT'}, page_content='ChatGPT is a generative artificial intelligence chatbot developed by OpenAI and launched in 2022. It is currently based on the GPT-4o large language model (LLM). ChatGPT can generate human-like conversational responses and enables users to refine and steer a conversation towards a desired length, format, style, level of detail, and language. It is credited with accelerating the AI boom, which has led to ongoing rapid investment in and public attention to the field of artificial intelligence (AI). Some observers have raised concern about the potential of ChatGPT and similar programs to displace human intelligence, enable plagiarism, or fuel misinformation.\\nBy January 2023, ChatGPT had become what was then the fastest-growing consumer software application in history, gaining over 100 million users in two months and contributing to the growth of OpenAI\\'s current valuation of $86 billion. ChatGPT\\'s release spurred the release of competing products, including Gemini, Claude, Llama, Ernie, and Grok. Microsoft launched Copilot, initially based on OpenAI\\'s GPT-4. In May 2024, a partnership between Apple Inc. and OpenAI was announced, in which ChatGPT was integrated into the Apple Intelligence feature of Apple operating systems. As of July 2024, ChatGPT\\'s website is among the 10 most-visited websites globally.\\nChatGPT is built on OpenAI\\'s proprietary series of generative pre-trained transformer (GPT) models and is fine-tuned for conversational applications using a combination of supervised learning and reinforcement learning from human feedback. Successive user prompts and replies are considered at each conversation stage as context. ChatGPT was released as a freely available research preview, but due to its popularity, OpenAI now operates the service on a freemium model. Users on its free tier can access GPT-4o. The ChatGPT subscriptions \"Plus\", \"Team\", and \"Enterprise\" provide additional features such as DALL-E 3 image generation and an increased usage limit.\\n\\n\\n== Training ==\\n\\nChatGPT is based on particular GPT foundation models, namely GPT-4, GPT-4o and GPT-4o mini, that were fine-tuned to target conversational usage. The fine-tuning process leveraged supervised learning and reinforcement learning from human feedback (RLHF). Both approaches employed human trainers to improve model performance. In the case of supervised learning, the trainers played both sides: the user and the AI assistant. In the reinforcement learning stage, human trainers first ranked responses that the model had created in a previous conversation. These rankings were used to create \"reward models\" that were used to fine-tune the model further by using several iterations of proximal policy optimization.\\nTime magazine revealed that to build a safety system against harmful content (e.g., sexual abuse, violence, racism, sexism), OpenAI used outsourced Kenyan workers earning less than $2 per hour to label harmful content. These labels were used to train a model to detect such content in the future. The outsourced laborers were exposed to \"toxic\" and traumatic content; one worker described the assignment as \"torture\". OpenAI\\'s outsourcing partner was Sama, a training-data company based in San Francisco, California.\\nChatGPT initially used a Microsoft Azure supercomputing infrastructure, powered by Nvidia GPUs, that Microsoft built specifically for OpenAI and that reportedly cost \"hundreds of millions of dollars\". Following ChatGPT\\'s success, Microsoft dramatically upgraded the OpenAI infrastructure in 2023. Scientists at the University of California, Riverside, estimate that a series of prompts to ChatGPT needs approximately 500 milliliters (18 imp fl oz; 17 U.S. fl oz) of water for Microsoft servers cooling. TrendForce market intelligence estimated that 30,000 Nvidia GPUs (each costing approximately $10,000–15,000) were used to power ChatGPT in 2023.\\nOpenAI collects data from ChatGPT users to train and fine-tune the service further. Users can upvote or downvote respon'),\n",
       " Document(metadata={'title': 'Sam Altman', 'source': 'https://en.wikipedia.org/wiki/Sam_Altman'}, page_content='Samuel Harris Altman (born April 22, 1985) is an American entrepreneur and investor best known as the chief executive officer of OpenAI since 2019 (he was briefly dismissed and reinstated in November 2023). He is also the chairman of clean energy companies Oklo Inc. and Helion Energy. \\nAltman is considered to be one of the leading figures of the AI boom. He dropped out of Stanford University after two years and founded Loopt, a mobile social networking service, raising more than $30 million in venture capital. In 2011, Altman joined Y Combinator, a startup accelerator, and was its president from 2014 to 2019.\\n\\n\\n== Early life and education ==\\nAltman was born on April 22, 1985, in Chicago, Illinois, into a Jewish family, and grew up in St. Louis, Missouri. His mother is a dermatologist, while his father was a real estate broker. Altman is the eldest of four siblings. At the age of eight, he received his first computer, an Apple Macintosh, and began to learn how to code and take apart computer hardware. He attended John Burroughs School, a private school in Ladue, Missouri. In 2005, after two years at Stanford University studying computer science, he dropped out without earning a bachelor\\'s degree.\\n\\n\\n== Career ==\\n\\n\\n=== Early career ===\\nIn 2005, at the age of 19, Sam Altman co-founded Loopt, a location-based social networking mobile application. As CEO, he raised more than $30 million in venture capital for the company, including an initial investment of $5 million from Patrick Chung of Xfund and his team at New Enterprise Associates, followed by investments from Sequoia Capital and Y Combinator. In March 2012, after Loopt failed to gain significant user traction, the company was acquired by the Green Dot Corporation for $43.4 million.\\nIn April 2012, Altman co-founded Hydrazine Capital with his brother, Jack Altman. The venture capital firm is still in operation and focuses on early-stage tech investments.\\nIn 2011, Altman became a partner at Y Combinator (YC), a startup accelerator that invests in a wide range of startups, initially working on a part-time basis. In February 2014, he was named president of YC by co-founder Paul Graham. In a 2014 blog post, Altman stated that the total valuation of YC companies had surpassed $65 billion, including Airbnb, Dropbox, Zenefits, and Stripe. He aimed to expand YC to fund 1,000 new companies per year and sought to broaden the types of companies funded, particularly focusing on \"hard technology\" startups.\\nIn October 2015, Altman announced YC Continuity, a $700 million equity fund designed to invest in YC companies as they matured. A week earlier, he had introduced Y Combinator Research, a non-profit research lab, donating $10 million of his own funds to establish it.\\nIn December 2015, Altman co-founded OpenAI, an artificial intelligence research organization, alongside notable figures such as Elon Musk, Jessica Livingston, and Peter Thiel. OpenAI was established with the goal of promoting and developing friendly AI for the benefit of humanity. The organization was initially funded with $1 billion in commitments from its founders and other investors, including Microsoft and Amazon Web Services.\\nIn September 2016, Altman announced his expanded role as president of YC Group, which included Y Combinator and other units.\\nIn March 2019, YC announced Altman\\'s transition from president to a less hands-on role as chairman of the board, allowing him to focus on OpenAI. This decision came shortly after YC announced it would be moving its headquarters to San Francisco. As of early 2020, he was no longer affiliated with YC.\\nIn 2019, Altman co-founded the for-profit company Tools For Humanity, which builds and distributes systems designed to scan people\\'s eyes to provide authentication and verify proof of personhood to counter fraud. Participants who agree to have their eyes scanned are compensated with a cryptocurrency called Worldcoin. Tools For Humanity describes its cryptocurrency as similar to unive'),\n",
       " Document(metadata={'title': 'SearchGPT', 'source': 'https://en.wikipedia.org/wiki/SearchGPT'}, page_content='SearchGPT is a search engine developed by OpenAI. It combines traditional search engine features with generative pretrained transformers (GPT) to generate responses, including citations to external websites.\\n\\n\\n== History ==\\nOn July 25, 2024, SearchGPT was first introduced as a prototype in a limited release to 10,000 test users. This search feature positioned OpenAI as a direct competitor to major search engines, notably Google, Perplexity AI and Bing.\\nOpenAI announced its partnership with publishers for SearchGPT, providing them with options on how their content appears in the search results and ensuring the promotion of trusted sources.\\nOn October 31, 2024, OpenAI launched SearchGPT to ChatGPT Plus and Team subscribers, and it was made available to free users in December 2024. OpenAI ultimately incorporated the search features into ChatGPT in December 2024.\\n\\n\\n== See also ==\\n\\nComparison of web search engines\\nGoogle Search – Search engine from Google\\nList of search engines\\nTimeline of web search engines\\n\\n\\n== References =='),\n",
       " Document(metadata={'title': 'Generative artificial intelligence', 'source': 'https://en.wikipedia.org/wiki/Generative_artificial_intelligence'}, page_content='Generative artificial intelligence (generative AI, GenAI, or GAI) is a subset of artificial intelligence that uses generative models to produce text, images, videos, or other forms of data. These models learn the underlying patterns and structures of their training data and use them to produce new data based on the input, which often comes in the form of natural language prompts.  \\nImprovements in transformer-based deep neural networks, particularly large language models (LLMs), enabled an AI boom of generative AI systems in the early 2020s. These include chatbots such as ChatGPT, Copilot, Gemini, and LLaMA; text-to-image artificial intelligence image generation systems such as Stable Diffusion, Midjourney, and DALL-E; and text-to-video AI generators such as Sora. Companies such as OpenAI, Anthropic, Microsoft, Google, and Baidu as well as numerous smaller firms have developed generative AI models.\\nGenerative AI has uses across a wide range of industries, including software development, healthcare, finance, entertainment, customer service, sales and marketing, art, writing, fashion, and product design. However, concerns have been raised about the potential misuse of generative AI such as cybercrime, the use of fake news or deepfakes to deceive or manipulate people, and the mass replacement of human jobs. Intellectual property law concerns also exist around generative models that are trained on and emulate copyrighted works of art.\\n\\n\\n== History ==\\n\\n\\n=== Early history ===\\nSince its inception, researchers in the field have raised philosophical and ethical arguments about the nature of the human mind and the consequences of creating artificial beings with human-like intelligence; these issues have previously been explored by myth, fiction and philosophy since antiquity. The concept of automated art dates back at least to the automata of ancient Greek civilization, where inventors such as Daedalus and Hero of Alexandria were described as having designed machines capable of writing text, generating sounds, and playing music. The tradition of creative automations has flourished throughout history, exemplified by Maillardet\\'s automaton created in the early 1800s. Markov chains have long been used to model natural languages since their development by Russian mathematician Andrey Markov in the early 20th century. Markov published his first paper on the topic in 1906, and analyzed the pattern of vowels and consonants in the novel Eugeny Onegin using Markov chains. Once a Markov chain is learned on a text corpus, it can then be used as a probabilistic text generator.\\n\\n\\n=== Academic artificial intelligence ===\\nThe academic discipline of artificial intelligence was established at a research workshop held at Dartmouth College in 1956 and has experienced several waves of advancement and optimism in the decades since. Artificial Intelligence research began in the 1950s with works like Computing Machinery and Intelligence (1950) and the 1956 Dartmouth Summer Research Project on AI. Since the 1950s, artists and researchers have used artificial intelligence to create artistic works. By the early 1970s, Harold Cohen was creating and exhibiting generative AI works created by AARON, the computer program Cohen created to generate paintings.\\nThe terms generative AI planning or generative planning were used in the 1980s and 1990s to refer to AI planning systems, especially computer-aided process planning, used to generate sequences of actions to reach a specified goal. Generative AI planning systems used symbolic AI methods such as state space search and constraint satisfaction and were a \"relatively mature\" technology by the early 1990s. They were used to generate crisis action plans for military use, process plans for manufacturing and decision plans such as in prototype autonomous spacecraft.\\n\\n\\n=== Generative neural nets (2014-2019) ===\\n\\nSince its inception, the field of machine learning used both discriminative models and generative models, to mode'),\n",
       " Document(metadata={'title': 'Whisper (speech recognition system)', 'source': 'https://en.wikipedia.org/wiki/Whisper_(speech_recognition_system)'}, page_content='Whisper is a machine learning model for speech recognition and transcription, created by OpenAI and first released as open-source software in September 2022.\\nIt is capable of transcribing speech in English and several other languages, and is also capable of translating several non-English languages into English. OpenAI claims that the combination of different training data used in its development has led to improved recognition of accents, background noise and jargon compared to previous approaches.\\nWhisper is a weakly-supervised deep learning acoustic model, made using an encoder-decoder transformer architecture.\\nWhisper Large V2 was released on December 8, 2022. Whisper Large V3 was released in November 2023, on the OpenAI Dev Day.\\n\\n\\n== Background ==\\nSpeech recognition has had a long history in research; the first approaches made use of statistical methods, such as dynamic time warping, and later hidden Markov models. At around the 2010s, deep neural network approaches became more common for speech recognition models, which were enabled by the availability of large datasets (\"big data\") and increased computational performance. Early approaches to deep learning in speech recognition included convolutional neural networks, which were limited due to their inability to capture sequential data, which later led to developments of Seq2seq approaches, which include recurrent neural networks which made use of long short-term memory.\\nTransformers, introduced in 2017 by Google, displaced many prior state-of-the art approaches to many problems in machine learning, and started becoming the core neural architecture in fields such as language modeling and computer vision; weakly-supervised approaches to training acoustic models were recognized in the early 2020s as promising for speech recognition approaches using deep neural networks.\\nAccording to a NYT report, in 2021 OpenAI believed they exhausted sources of higher-quality data to train their large language models and decided to complement scraped web text with transcriptions of YouTube videos and podcasts, and developed Whisper to solve this task.\\n\\n\\n== Architecture ==\\n\\nThe Whisper architecture is based on an encoder-decoder transformer.\\nInput audio is resampled to 16,000 Hz and converting to an 80-channel log-magnitude Mel spectrogram using 25 ms windows with a 10 ms stride. The spectrogram is then normalized to a [-1, 1] range with near-zero mean.\\nThe encoder takes this Mel spectrogram as input and processes it. It first passes through two convolutional layers. Sinusoidal positional embeddings are added. It is then processed by a series of Transformer encoder blocks (with pre-activation residual connections). The encoder\\'s output is layer normalized.\\nThe decoder is a standard Transformer decoder. It has the same width and Transformer blocks as the encoder. It uses learned positional embeddings and tied input-output token representations (using the same weight matrix for both the input and output embeddings). It uses a byte-pair encoding tokenizer, of the same kind as used in GPT-2. English-only models use the GPT-2 vocabulary, while multilingual models employ a re-trained multilingual vocabulary with the same number of words.\\nSpecial tokens are used to allow the decoder to perform multiple tasks:\\n\\nTokens that denote language (one unique token per language).\\nTokens that specify task (<|transcribe|> or <|translate|>).\\nTokens that specify if no timestamps are present (<|notimestamps|>). If the token is not present, then the decoder predicts timestamps relative to the segment, and quantized to 20 ms intervals.\\n<|nospeech|> for voice activity detection.\\n<|startoftranscript|>, and <|endoftranscript|> . Any text that appears before <|startoftranscript|> is not generated by the decoder, but given to the decoder as context. Loss is only computed over non-contextual parts of the sequence, i.e. tokens between these two special tokens.\\n\\n\\n== Data ==\\nThe training dataset consists of 680,000 hours o'),\n",
       " Document(metadata={'title': 'Artificial general intelligence', 'source': 'https://en.wikipedia.org/wiki/Artificial_general_intelligence'}, page_content='Artificial general intelligence (AGI) is a type of artificial intelligence (AI) that matches or surpasses human cognitive capabilities across a wide range of cognitive tasks. This contrasts with narrow AI, which is limited to specific tasks. Artificial superintelligence (ASI), on the other hand, refers to AGI that greatly exceeds human cognitive capabilities. AGI is considered one of the definitions of strong AI.\\nCreating AGI is a primary goal of AI research and of companies such as OpenAI and Meta. A 2020 survey identified 72 active AGI research and development projects across 37 countries.\\nThe timeline for achieving AGI remains a subject of ongoing debate among researchers and experts. As of 2023, some argue that it may be possible in years or decades; others maintain it might take a century or longer; a minority believe it may never be achieved; and another minority claims that it is already here. Notable AI researcher Geoffrey Hinton has expressed concerns about the rapid progress towards AGI, suggesting it could be achieved sooner than many expect.\\nThere is debate on the exact definition of AGI and regarding whether modern large language models (LLMs) such as GPT-4 are early forms of AGI. AGI is a common topic in science fiction and futures studies.\\nContention exists over whether AGI represents an existential risk. Many experts on AI have stated that mitigating the risk of human extinction posed by AGI should be a global priority. Others find the development of AGI to be too remote to present such a risk.\\n\\n\\n== Terminology ==\\nAGI is also known as strong AI, full AI, human-level AI, human-level intelligent AI, or general intelligent action.\\nSome academic sources reserve the term \"strong AI\" for computer programs that experience sentience or consciousness. In contrast, weak AI (or narrow AI) is able to solve one specific problem but lacks general cognitive abilities. Some academic sources use \"weak AI\" to refer more broadly to any programs that neither experience consciousness nor have a mind in the same sense as humans.\\nRelated concepts include artificial superintelligence and transformative AI. An artificial superintelligence (ASI) is a hypothetical type of AGI that is much more generally intelligent than humans, while the notion of transformative AI relates to AI having a large impact on society, for example, similar to the agricultural or industrial revolution.\\nA framework for classifying AGI in levels was proposed in 2023 by Google DeepMind researchers. They define five levels of AGI: emerging, competent, expert, virtuoso, and superhuman. For example, a competent AGI is defined as an AI that outperforms 50% of skilled adults in a wide range of non-physical tasks, and a superhuman AGI (i.e. an artificial superintelligence) is similarly defined but with a threshold of 100%. They consider large language models like ChatGPT or LLaMA 2 to be instances of emerging AGI.\\n\\n\\n== Characteristics ==\\n\\nVarious popular definitions of intelligence have been proposed. One of the leading proposals is the Turing test. However, there are other well-known definitions, and some researchers disagree with the more popular approaches. \\n\\n\\n=== Intelligence traits ===\\nHowever, researchers generally hold that intelligence is required to do all of the following:\\n\\nreason, use strategy, solve puzzles, and make judgments under uncertainty\\nrepresent knowledge, including common sense knowledge\\nplan\\nlearn\\ncommunicate in natural language\\nif necessary, integrate these skills in completion of any given goal\\nMany interdisciplinary approaches (e.g. cognitive science, computational intelligence, and decision making) consider additional traits such as imagination (the ability to form novel mental images and concepts) and autonomy.\\nComputer-based systems that exhibit many of these capabilities exist (e.g. see computational creativity, automated reasoning, decision support system, robot, evolutionary computation, intelligent agent). There is debate about whether mo'),\n",
       " Document(metadata={'title': 'Anthropic', 'source': 'https://en.wikipedia.org/wiki/Anthropic'}, page_content='Anthropic PBC is a U.S.-based artificial intelligence (AI) public-benefit startup founded in 2021. It researches and develops AI to \"study their safety properties at the technological frontier\" and use this research to deploy safe, reliable models for the public. Anthropic has developed a family of large language models (LLMs) named Claude as a competitor to OpenAI\\'s ChatGPT and Google\\'s Gemini.\\nAnthropic was founded by former members of OpenAI, including siblings Daniela Amodei and Dario Amodei. In September 2023, Amazon announced an investment of up to $4 billion, followed by a $2 billion commitment from Google in the following month.\\n\\n\\n== History ==\\n\\nAnthropic was founded in 2021 by seven former employees of OpenAI, including siblings Daniela Amodei and Dario Amodei, the latter of whom served as OpenAI\\'s Vice President of Research.\\nIn April of 2022, Anthropic announced it had received $580 million in funding, including a $500 million investment from FTX under the leadership of Sam Bankman-Fried.\\nIn the summer of 2022, Anthropic finished training the first version of Claude but did not release it, mentioning the need for further internal safety testing and the desire to avoid initiating a potentially hazardous race to develop increasingly powerful AI systems.  \\nIn February 2023, Anthropic was sued by Texas-based Anthrop LLC for the use of its registered trademark \"Anthropic A.I.\" On September 25, 2023, Amazon announced a partnership with Anthropic, with Amazon becoming a minority stakeholder by initially investing $1.25 billion, and planning a total investment of $4 billion. As part of the deal, Anthropic would use Amazon Web Services (AWS) as its primary cloud provider and make its AI models available to AWS customers. The next month, Google invested $500 million in Anthropic, and committed to an additional $1.5 billion over time. \\nIn March 2024, Amazon maxed out its potential investment from the agreement made in the prior year by investing another US $2.75 billion into Anthropic, completing its $4 billion investment.\\nIn November 2024, Amazon announced a new investment of $4 billion in Anthropic (bringing its total investment to $8 billion), including an agreement to increase the use of Amazon\\'s AI chips for training and running Anthropic\\'s large language models.\\nIn 2024, Anthropic attracted several notable employees from OpenAI, including Jan Leike, John Schulman, and Durk Kingma.\\n\\n\\n== Participants ==\\n\\n\\n=== Key employees ===\\nDario Amodei: Co-Founder and Chief Executive Officer\\nDaniela Amodei: Co-Founder and President\\nJason Clinton: Chief Information Security Officer\\nJared Kaplan: Co-Founder and Chief Science Officer\\nBen Mann: Co-Founder and Member of Technical Staff\\nJack Clark: Co-Founder and Head of Policy\\nMike Krieger: Chief Product Officer\\nJan Leike: ex-OpenAI alignment researcher\\n\\n\\n=== Board of Directors ===\\nDario Amodei: Chief Executive Officer\\nDaniela Amodei: representative of common shareholders\\nYasmin Razavi: representative of Series C shareholders\\nJay Kreps: appointed by Long-Term Benefit Trust, CEO of Confluent\\n\\n\\n=== Investors ===\\nAmazon.com – $8B\\nGoogle – $2B\\nMenlo Ventures – $750M\\nWisdom Ventures\\nRipple Impact Investments\\nFactorial Funds\\n\\n\\n== Motives ==\\nAccording to Anthropic, the company\\'s goal is to research the safety and reliability of artificial intelligence systems. The Amodei siblings were among those who left OpenAI due to directional differences. Anthropic incorporated itself as a Delaware public-benefit corporation (PBC), which requires the company to maintain a balance between private and public interests.\\nAnthropic is a corporate \"Long-Term Benefit Trust\", a company-derived entity that requires the company\\'s directors to align the company\\'s priorities with the public benefit rather than profit in \"extreme\" instances of \"catastrophic risk\". As of September 19, 2023, members of the Trust included Jason Matheny (CEO and President of the RAND Corporation), Kanika Bahl (CEO and President of Evidence Act'),\n",
       " Document(metadata={'title': 'AI boom', 'source': 'https://en.wikipedia.org/wiki/AI_boom'}, page_content='The AI boom is an ongoing period of rapid progress in the field of artificial intelligence (AI) that started in the late 2010s before gaining international prominence in the early 2020s. Examples include protein folding prediction led by Google DeepMind as well as large language models and generative AI applications developed by OpenAI. This period is sometimes referred to as an AI spring, to contrast it with previous AI winters. \\n\\n\\n== History ==\\n\\nIn 2012, a University of Toronto research team used artificial neural networks and deep learning techniques to lower the error rate below 25% for the first time during the ImageNet challenge for object recognition in computer vision. The event catalyzed the AI boom later that decade, when many alumni of the ImageNet challenge became leaders in the tech industry. In March 2016, AlphaGo beat Lee Sedol in a five-game match, marking the first time a computer Go program had beaten a 9-dan professional without handicap. This match led to significant increase in public interest in AI. The generative AI race began in earnest in 2016 or 2017 following the founding of OpenAI and earlier advances made in graphical processing units (GPUs), the amount and quality of training data, generative adversarial networks, diffusion models and transformer architectures. \\nIn 2018, the Artificial Intelligence Index, an initiative from Stanford University, reported a global explosion of commercial and research efforts in AI. Europe published the largest number of papers in the field that year, followed by China and North America. Technologies such as AlphaFold led to more accurate predictions of protein folding and improved the process of drug development. Economists and lawmakers began to discuss the potential impact of AI more frequently. By 2022, large language models (LLMs) saw increased usage in chatbot applications; text-to-image-models could generate images that appeared to be human-made; and speech synthesis software was able to replicate human speech efficiently.\\nAccording to metrics from 2017 to 2021, the United States outranks the rest of the world in terms of venture capital funding, the number of startups, and patents granted in AI. Scientists who have immigrated to the U.S. play an outsized role in the country\\'s development of AI technology. Many of them were educated in China, prompting debates about national security concerns amid worsening relations between the two countries.\\nExperts have framed AI development as a competition for economic and geopolitical advantage between the United States and China. In 2021, an analyst for the Council on Foreign Relations outlined ways that the U.S. could maintain its position amid progress made by China. In 2023, an analyst at the Center for Strategic and International Studies advocated for the U.S. to use its dominance in AI technology to drive its foreign policy instead of relying on trade agreements.\\n\\n\\n== Advances ==\\n\\n\\n=== Biomedical ===\\nThe AlphaFold 2 score of more than 90 in CASP\\'s global distance test (GDT) is considered a significant achievement in computational biology and great progress towards a decades-old grand challenge of biology. Nobel Prize winner and structural biologist Venki Ramakrishnan called the result \"a stunning advance on the protein folding problem\", adding that \"It has occurred decades before many people in the field would have predicted.\"\\nThe ability to predict protein structures accurately based on the constituent amino acid sequence is expected to accelerate drug discovery and enable a better understanding of diseases.\\n\\n\\n=== Images and videos ===\\n\\nText-to-image models captured widespread public attention when OpenAI announced DALL-E, a transformer system, in January 2021. A successor capable of generating complex and realistic images, DALL-E 2, was unveiled in April 2022. An alternative text-to-image model, Midjourney, was released in July 2022. Another alternative, open-source model Stable Diffusion, released in August 2022'),\n",
       " Document(metadata={'title': 'GPT-4', 'source': 'https://en.wikipedia.org/wiki/GPT-4'}, page_content='Generative Pre-trained Transformer 4 (GPT-4) is a multimodal large language model trained and created by OpenAI and the fourth in its series of GPT foundation models. It was launched on March 14, 2023, and made publicly available via the paid chatbot product ChatGPT Plus, via OpenAI\\'s API, and via the free chatbot Microsoft Copilot.  As a transformer-based model, GPT-4 uses a paradigm where pre-training using both public data and \"data licensed from third-party providers\" is used to predict the next token. After this step, the model was then fine-tuned with reinforcement learning feedback from humans and AI for human alignment and policy compliance.:\\u200a2\\u200a\\nObservers reported that the iteration of ChatGPT using GPT-4 was an improvement on the previous iteration based on GPT-3.5, with the caveat that GPT-4 retains some of the problems with earlier revisions. GPT-4, equipped with vision capabilities (GPT-4V), is capable of taking images as input on ChatGPT. OpenAI has not revealed technical details and statistics about GPT-4, such as the precise size of the model.\\n\\n\\n== Background ==\\n \\n\\nOpenAI introduced the first GPT model (GPT-1) in 2018, publishing a paper called \"Improving Language Understanding by Generative Pre-Training.\" It was based on the transformer architecture and trained on a large corpus of books. The next year, they introduced GPT-2, a larger model that could generate coherent text. In 2020, they introduced GPT-3, a model with over 100 times as many parameters as GPT-2, that could perform various tasks with few examples. GPT-3 was further improved into GPT-3.5, which was used to create the chatbot product ChatGPT.\\nRumors claim that GPT-4 has 1.76 trillion parameters, which was first estimated by the speed it was running and by George Hotz.\\n\\n\\n== Capabilities ==\\nOpenAI stated that GPT-4 is \"more reliable, creative, and able to handle much more nuanced instructions than GPT-3.5.\" They produced two versions of GPT-4, with context windows of 8,192 and 32,768 tokens, a significant improvement over GPT-3.5 and GPT-3, which were limited to 4,096 and 2,049 tokens respectively. Some of the capabilities of GPT-4 were predicted by OpenAI before training it, although other capabilities remained hard to predict due to breaks in downstream scaling laws. Unlike its predecessors, GPT-4 is a multimodal model: it can take images as well as text as input; this gives it the ability to describe the humor in unusual images, summarize text from screenshots, and answer exam questions that contain diagrams. It can now interact with users through spoken words and respond to images, allowing for more natural conversations and the ability to provide suggestions or answers based on photo uploads.\\nTo gain further control over GPT-4, OpenAI introduced the \"system message\", a directive in natural language given to GPT-4 in order to specify its tone of voice and task. For example, the system message can instruct the model to \"be a Shakespearean pirate\", in which case it will respond in rhyming, Shakespearean prose, or request it to \"always write the output of [its] response in JSON\", in which case the model will do so, adding keys and values as it sees fit to match the structure of its reply. In the examples provided by OpenAI, GPT-4 refused to deviate from its system message despite requests to do otherwise by the user during the conversation.\\nWhen instructed to do so, GPT-4 can interact with external interfaces. For example, the model could be instructed to enclose a query within <search></search> tags to perform a web search, the result of which would be inserted into the model\\'s prompt to allow it to form a response. This allows the model to perform tasks beyond its normal text-prediction capabilities, such as using APIs, generating images, and accessing and summarizing webpages.\\nA 2023 article in Nature stated programmers have found GPT-4 useful for assisting in coding tasks (despite its propensity for error), such as finding errors in existing co'),\n",
       " Document(metadata={'title': 'Greg Brockman', 'source': 'https://en.wikipedia.org/wiki/Greg_Brockman'}, page_content=\"Gregory Brockman is an American entrepreneur, investor and software developer who is a co-founder and currently the president of OpenAI. He began his career at Stripe in 2010, upon leaving MIT, and became their CTO in 2013. He left Stripe in 2015 to co-found OpenAI, where he also assumed the role of CTO.\\n\\n\\n== Early life and education ==\\nBrockman was born in Thompson, North Dakota, and attended Red River High School, where he excelled in mathematics, chemistry, and computer science. He won a silver medal in the 2006 International Chemistry Olympiad and became the first finalist from North Dakota to participate in the Intel science talent search since 1973. In 2003, 2005, and 2007, he attended Canada/USA Mathcamp, a summer program for mathematically talented high-school students. In 2008, Brockman enrolled in Harvard University, but left only a year later, before briefly enrolling at the Massachusetts Institute of Technology.\\n\\n\\n== Career ==\\nIn 2010, he dropped out of MIT to join Stripe, Inc., a company founded by Patrick Collison, an MIT classmate, and his brother, John Collison. In 2013, he became Stripe's first-ever CTO, and grew the company from 5 to 205 employees. Brockman left Stripe in May 2015, and co-founded OpenAI in December 2015 with Sam Altman and Ilya Sutskever. The company was initially run from Brockman's living room.\\nBrockman helped create the OpenAI founding team, and led various prominent projects early on at OpenAI, including OpenAI Gym and OpenAI Five, a Dota 2 bot.\\nOn February 14, 2019, OpenAI announced that they had developed a new large language model called GPT-2, but kept it private due to their concern for its potential misuse. They finally released the model to a limited group of beta testers in May 2019.\\nOn March 14, 2023, in a live video demo, Brockman unveiled GPT-4, the fourth iteration in the GPT series, and the newest language model created by OpenAI.\\nOn November 17, 2023, along with the firing of Sam Altman from OpenAI, Greg Brockman was told he was being removed from the board, but was vital to the company and would remain in his role at the company, reporting to the CEO. He later in the day announced on X (formerly known as Twitter) he had quit the company.\\nOn November 20, 2023, Microsoft CEO Satya Nadella announced that Brockman and former OpenAI CEO Sam Altman would join Microsoft to lead a new advanced AI research team. The following day, after a deal was reached to reinstate Altman as CEO, Brockman returned to OpenAI. Brockman was on sabbatical since August 2024 and announced to join at the end of the year. In November 2024, he returned to the company after three months of absence.\\n\\n\\n== Personal life ==\\nIn November 2019, Brockman married his girlfriend, Anna.\\n\\n\\n== References ==\"),\n",
       " Document(metadata={'title': 'Microsoft Copilot', 'source': 'https://en.wikipedia.org/wiki/Microsoft_Copilot'}, page_content='Microsoft Copilot is a generative artificial intelligence chatbot developed by Microsoft. Based on the GPT-4 series of large language models, it was launched in 2023 as Microsoft\\'s primary replacement for the discontinued Cortana.\\nThe service was introduced in February 2023 under the name Bing Chat, as a built-in feature for Microsoft Bing and Microsoft Edge. Over the course of 2023, Microsoft began to unify the Copilot branding across its various chatbot products, cementing the \"copilot\" analogy. At its Build 2023 conference, Microsoft announced its plans to integrate Copilot into Windows 11, allowing users to access it directly through the taskbar. In January 2024, a dedicated Copilot key was announced for Windows keyboards.\\nCopilot utilizes the Microsoft Prometheus model, built upon OpenAI\\'s GPT-4 foundational large language model, which in turn has been fine-tuned using both supervised and reinforcement learning techniques. Copilot\\'s conversational interface style resembles that of ChatGPT. The chatbot is able to cite sources, create poems, generate songs, and use numerous languages and dialects.\\nMicrosoft operates Copilot on a freemium model. Users on its free tier can access most features, while priority access to newer features, including custom chatbot creation, is provided to paid subscribers under the \"Microsoft Copilot Pro\" paid subscription service. Several default chatbots are available in the free version of Microsoft Copilot, including the standard Copilot chatbot as well as Microsoft Designer, which is oriented towards using its Image Creator to generate images based on text prompts.\\n\\n\\n== Background ==\\nIn 2019, Microsoft partnered with OpenAI and began investing billions of dollars into the organization. Since then, OpenAI systems have run on an Azure-based supercomputing platform from Microsoft. In September 2020, Microsoft announced that it had licensed OpenAI\\'s GPT-3 exclusively. Others can still receive output from its public API, but Microsoft has exclusive access to the underlying model.\\nIn November 2022, OpenAI launched ChatGPT, a chatbot which was based on GPT-3.5. ChatGPT gained worldwide attention following its release, becoming a viral Internet sensation. On January 23, 2023, Microsoft announced a multi-year US$10 billion investment in OpenAI. On February 6, Google announced Bard (later rebranded as Gemini), a ChatGPT-like chatbot service, fearing that ChatGPT could threaten Google\\'s place as a go-to source for information. Multiple media outlets and financial analysts described Google as \"rushing\" Bard\\'s announcement to preempt rival Microsoft\\'s planned February 7 event unveiling Copilot, as well as to avoid playing \"catch-up\" to Microsoft.\\n\\n\\n== History ==\\n\\n\\n=== As Bing Chat ===\\n\\nOn February 7, 2023, Microsoft began rolling out a major overhaul to Bing, called the new Bing. A chatbot feature, at the time known as Bing Chat, had been developed by Microsoft and was released in Bing and Edge as part of this overhaul. According to Microsoft, one million people joined its waitlist within a span of 48 hours. Bing Chat was available only to users of Microsoft Edge and Bing mobile app, and Microsoft claimed that waitlisted users would be prioritized if they set Edge and Bing as their defaults and installed the Bing mobile app. \\nWhen Microsoft demonstrated Bing Chat to journalists, it produced several hallucinations, including when asked to summarize financial reports. The new Bing was criticized in February 2023 for being more argumentative than ChatGPT, sometimes to an unintentionally humorous extent. The chat interface proved vulnerable to prompt injection attacks with the bot revealing its hidden initial prompts and rules, including its internal codename \"Sydney\". Upon scrutiny by journalists, Bing Chat claimed it spied on Microsoft employees via laptop webcams and phones. It confessed to spying on, falling in love with, and then murdering one of its developers at Microsoft to The Verge reviews editor Na'),\n",
       " Document(metadata={'title': 'Gemini (chatbot)', 'source': 'https://en.wikipedia.org/wiki/Gemini_(chatbot)'}, page_content='Gemini, formerly known as Bard, is a generative artificial intelligence chatbot developed by Google. Based on the large language model (LLM) of the same name, it was launched in 2023 after being developed as a direct response to the rise of OpenAI\\'s ChatGPT. It was previously based on PaLM, and initially the LaMDA family of large language models.\\nLaMDA had been developed and announced in 2021, but it was not released to the public out of an abundance of caution. OpenAI\\'s launch of ChatGPT in November 2022 and its subsequent popularity caught Google executives off-guard, prompting a sweeping response in the ensuing months. After mobilizing its workforce, the company launched Bard in a limited capacity in March 2023 before expanding to other countries in May. Bard took center stage during the 2023 Google I/O keynote in May and was upgraded to the Gemini LLM in December. In February 2024, Bard and Duet AI, another artificial intelligence product from Google, were unified under the Gemini brand, coinciding with the launch of an Android app.\\n\\n\\n== Background ==\\n\\nOpenAI launched ChatGPT, a chatbot based on the GPT-3 family of large language models, in November 2022 and it gained worldwide attention, becoming a viral Internet sensation. Alarmed by ChatGPT\\'s potential threat to Google Search, Google executives issued a \"code red\" alert, reassigning several teams to assist in the company\\'s artificial intelligence (AI) efforts. Sundar Pichai, the CEO of Google and parent company Alphabet, was widely reported to have issued the alert, but Pichai later denied this to The New York Times. In a rare move, Google co-founders Larry Page and Sergey Brin, who had stepped down from their roles as co-CEOs of Alphabet in 2019, attended emergency meetings with company executives to discuss Google\\'s response to ChatGPT. Brin requested access to Google\\'s code in February 2023, for the first time in years.\\nThe company had unveiled LaMDA, a prototype LLM, in 2021 but not released it to the public. When asked by employees at an all-hands meeting whether LaMDA was a missed opportunity for Google to compete with ChatGPT, Pichai and Google AI chief Jeff Dean said that while the company had similar capabilities to ChatGPT, moving too quickly in that arena would represent a major \"reputational risk\" due to Google being substantially larger than OpenAI. In January 2023, Google Brain sister company DeepMind CEO Demis Hassabis hinted at plans for a ChatGPT rival, and Google employees were instructed to accelerate progress on a ChatGPT competitor, intensively testing \"Apprentice Bard\" and other chatbots. Pichai assured investors during Google\\'s quarterly earnings investor call in February that the company had plans to expand LaMDA\\'s availability and applications.\\n\\n\\n== History ==\\n\\n\\n=== Announcement ===\\nOn February 6, 2023, Google announced Bard, a generative artificial intelligence chatbot powered by LaMDA. Bard was first rolled out to a select group of 10,000 \"trusted testers\", before a wide release scheduled at the end of the month. The project was overseen by product lead Jack Krawczyk, who described the product as a \"collaborative AI service\" rather than a search engine, while Pichai detailed how Bard would be integrated into Google Search. Reuters calculated that adding ChatGPT-like features to Google Search could cost the company $6 billion in additional expenses by 2024, while research and consulting firm SemiAnalysis calculated that it would cost Google $3 billion. The technology was developed under the codename \"Atlas\", with the name \"Bard\" in reference to the Celtic term for a storyteller and chosen to \"reflect the creative nature of the algorithm underneath\".\\nMultiple media outlets and financial analysts described Google as \"rushing\" Bard\\'s announcement to preempt rival Microsoft\\'s planned February 7 event unveiling its partnership with OpenAI to integrate ChatGPT into its Bing search engine in the form of Bing Chat (later rebranded as Microsoft Copilot)'),\n",
       " Document(metadata={'title': 'XAI (company)', 'source': 'https://en.wikipedia.org/wiki/XAI_(company)'}, page_content='X.AI Corp., doing business as xAI, is an American startup company working in the area of artificial intelligence (AI). Founded by Elon Musk in March 2023, its stated goal is \"to understand the true nature of the universe\".\\n\\n\\n== History ==\\nxAI was founded by Musk in Nevada on March 9, 2023, and has since been headquartered in the San Francisco Bay Area in California.\\nIgor Babuschkin, formerly associated with Google\\'s DeepMind unit, was recruited by Musk to be Chief Engineer.\\nMusk officially announced the formation of xAI on July 12, 2023. He linked the date (7 + 12 + 23 = 42) to the book The Hitchhiker\\'s Guide to the Galaxy by Douglas Adams, in which a supercomputer calculates that the answer to the ultimate question of \"life, the universe, and everything\" is 42; and to the company\\'s mission \"to understand the universe\".\\nIn December 2023, in a U.S. Securities Exchange Commission (SEC) filing, xAI revealed that it had raised US$134.7 million in outside funding out of a total of up to $1 billion. Despite the filing, Musk later claimed via X that xAI was not seeking any funding.\\nIn May 2024, xAI was reported to be looking for $6 billion of funding. Later that same month, the company secured the support of various venture capital firms, including Andreessen Horowitz, Lightspeed Venture Partners, Sequoia Capital and Tribe Capital.\\nIn May 2024, Musk predicted that AI will make most jobs obsolete, requiring a high universal income.\\nIn June 2024, the Greater Memphis Chamber announced xAI is planning on building Colossus, the world\\'s largest supercomputer in Memphis, Tennessee. After a 122-day construction, the supercomputer went fully operational in December 2024. Local government in Memphis has voiced concerns regarding the increased usage of electricity, 150 megawatts of power at peak, and while the agreement with the city is being worked out, the company has deployed 14 VoltaGrid gas generators to temporarily enhance the power supply. Environmental advocates state that the gas-burning turbines emit large quantities of gases causing air pollution, and that xAI has been operating the turbines illegally without the necessary permits.\\nAs of August 2024, Musk diverted a large number of Nvidia chips which had been ordered by Tesla, Inc. to X and xAI.\\nOn December 23, 2024, xAI raised an additional $6 billion in a funding round supported by Fidelity, BlackRock, Sequoia Capital, among others, making its total funding over $12 billion.\\n\\n\\n== Products ==\\nWhile xAI\\'s stated goal is \"to understand the true nature of the universe\", one of its immediate objectives is to create an AI that is capable of advanced mathematical reasoning, something not found in current models as of 2023.\\nOn November 4, 2023, xAI unveiled Grok, an AI chatbot that is integrated with X. xAI stated that when the bot is out of beta, it will only be available to X\\'s Premium+ subscribers.\\nOn November 6, 2023, xAI released PromptIDE, an integrated development environment (IDE) designed for prompt engineering and interpretability research, offering tools like a Python code editor and rich analytics to empower users in exploring and refining prompts for large language models like Grok-1.\\nOn March 2024, Grok was made available to all X Premium subscribers; it was previously available only to Premium+ subscribers.\\nOn March 17, 2024, xAI released Grok-1 as open source.\\nOn March 29, 2024, Grok-1.5 was announced, with \"improved reasoning capabilities\" and a context length of 128,000 tokens.\\nOn April 12, 2024, Grok-1.5 Vision (Grok-1.5V) was announced. Grok-1.5V is able to process a wide variety of visual information, including documents, diagrams, graphs, screenshots, and photographs.\\nOn August 14, 2024, Grok-2 was made available to X Premium subscribers. It is the first Grok model with image generation capabilities.\\nOn October 21, 2024, xAI released an applications programming interface (API).\\nOn December 9, 2024, xAI released a text-to-image model named Aurora.\\n\\n\\n== Notes ==\\n\\n\\n== Re'),\n",
       " Document(metadata={'title': 'Jan Leike', 'source': 'https://en.wikipedia.org/wiki/Jan_Leike'}, page_content='Jan Leike (born 1986 or 1987) is an AI alignment researcher who has worked at DeepMind and OpenAI. He joined Anthropic in May 2024.\\n\\n\\n== Education ==\\nJan Leike obtained his undergraduate degree from the University of Freiburg in Germany. After earning a master\\'s degree in computer science, he pursued a PhD in machine learning at the Australian National University under the supervision of Marcus Hutter.\\n\\n\\n== Career ==\\nLeike made a six-month postdoctoral fellowship at the Future of Humanity Institute before joining DeepMind to focus on empirical AI safety research, where he collaborated with Shane Legg.\\n\\n\\n=== OpenAI ===\\nIn 2021, Leike joined OpenAI. In June 2023, he and Ilya Sutskever became the co-leaders of the newly introduced \"superalignment\" project, which aimed to determine how to align future artificial superintelligences within four years to ensure their safety. This project involved automating AI alignment research using relatively advanced AI systems. At the time, Sutskever was OpenAI\\'s Chief Scientist, and Leike was the Head of Alignment. Leike was featured in Time\\'s list of the 100 most influential personalities in AI, both in 2023 and in 2024. In May 2024, Leike announced his resignation from OpenAI, following the departure of Ilya Sutskever, Daniel Kokotajlo and several other AI safety employees from the company. Leike wrote that \"Over the past years, safety culture and processes have taken a backseat to shiny products\", and that he \"gradually lost trust\" in OpenAI\\'s leadership.\\nIn May 2024, Leike joined Anthropic, an AI company founded by former OpenAI employees.\\n\\n\\n== References ==\\n\\n\\n== External links ==\\nOfficial website\\nJan Leike at IMDb'),\n",
       " Document(metadata={'title': 'GPT-4o', 'source': 'https://en.wikipedia.org/wiki/GPT-4o'}, page_content='GPT-4o (\"o\" for \"omni\") is a multilingual, multimodal generative pre-trained transformer developed by OpenAI and released in May 2024. GPT-4o is free, but with a usage limit that is five times higher for ChatGPT Plus subscribers. It can process and generate text, images and audio. Its application programming interface (API) is twice as fast and half the price of its predecessor, GPT-4 Turbo.\\n\\n\\n== Background ==\\nMultiple versions of GPT-4o were originally secretly launched under different names on Large Model Systems Organization\\'s (LMSYS) Chatbot Arena as three different models. These three models were called gpt2-chatbot, im-a-good-gpt2-chatbot, and im-also-a-good-gpt2-chatbot. On 7 May 2024, Sam Altman tweeted \"im-a-good-gpt2-chatbot\", which was commonly interpreted as a confirmation that these were new OpenAI models being A/B tested.\\n\\n\\n== Capabilities ==\\nGPT-4o achieved state-of-the-art results in voice, multilingual, and vision benchmarks, setting new records in audio speech recognition and translation. GPT-4o scored 88.7 on the Massive Multitask Language Understanding (MMLU) benchmark compared to 86.5 by GPT-4. Unlike GPT-3.5 and GPT-4, which rely on other models to process sound, GPT-4o natively supports voice-to-voice. Sam Altman noted on 15 May 2024 that GPT-4o\\'s voice-to-voice capabilities were not yet integrated into ChatGPT, and that the old version was still being used.  This new mode, called Advanced Voice Mode, is currently in limited alpha release and is based on the 4o-audio-preview. On 1 October 2024, the Realtime API was introduced.\\nThe model supports over 50 languages, which OpenAI claims cover over 97% of speakers. Mira Murati demonstrated the model\\'s multilingual capability by speaking Italian to the model and having it translate between English and Italian during the live-streamed OpenAI demonstration event on 13 May 2024. In addition, the new tokenizer uses fewer tokens for certain languages, especially languages that are not based on the Latin alphabet, making it cheaper for those languages.\\nGPT-4o has knowledge up to October 2023, but can access the Internet if up-to-date information is needed. It has a context length of 128k tokens with an output token limit capped to 4,096, and after a later update (gpt-4o-2024-08-06) to 16,384.\\nAs of May 2024, it is the leading model in the LMSYS Elo Arena Benchmarks by the University of California, Berkeley.\\n\\n\\n=== Corporate customization ===\\nIn August 2024, OpenAI introduced a new feature allowing corporate customers to customize GPT-4o using proprietary company data. This customization, known as fine-tuning, enables businesses to adapt GPT-4o to specific tasks or industries, enhancing its utility in areas like customer service and specialized knowledge domains. Previously, fine-tuning was available only on the less powerful model GPT-4o mini.\\nThe fine-tuning process requires customers to upload their data to OpenAI\\'s servers, with the training typically taking one to two hours. Initially, the customization will be limited to text-based data. OpenAI\\'s focus with this rollout is to reduce the complexity and effort required for businesses to tailor AI solutions to their needs, potentially increasing the adoption and effectiveness of AI in corporate environments.\\n\\n\\n== GPT-4o mini ==\\nOn July 18, 2024, OpenAI released a smaller and cheaper version, GPT-4o mini.\\nAccording to OpenAI, its low cost is expected to be particularly useful for companies, startups, and developers that seek to integrate it into their services, which often make a high number of API calls. Its API costs $0.15 per million input tokens and $0.6 per million output tokens, compared to $5 and $15, respectively, for GPT-4o. It is also significantly more capable and 60% cheaper than GPT-3.5 Turbo, which it replaced on the ChatGPT interface. The price after fine-tuning doubles: $0.3 per million input tokens and $1.2 per million output tokens.\\nGPT-4o mini is the default model for users not logged in who use'),\n",
       " Document(metadata={'title': 'Generative pre-trained transformer', 'source': 'https://en.wikipedia.org/wiki/Generative_pre-trained_transformer'}, page_content='A generative pre-trained transformer (GPT) is a type of large language model (LLM) and a prominent framework for generative artificial intelligence. It is an artificial neural network that is used in natural language processing by machines. It is based on the transformer deep learning architecture, pre-trained on large data sets of unlabeled text, and able to generate novel human-like content. As of 2023, most LLMs had these characteristics and are sometimes referred to broadly as GPTs.\\nThe first GPT was introduced in 2018 by OpenAI. OpenAI has released significant GPT foundation models that have been sequentially numbered, to comprise its \"GPT-n\" series. Each of these was significantly more capable than the previous, due to increased size (number of trainable parameters) and training. The most recent of these, GPT-4o, was released in May 2024. Such models have been the basis for their more task-specific GPT systems, including models fine-tuned for instruction following—which in turn power the ChatGPT chatbot service.\\nThe term \"GPT\" is also used in the names and descriptions of such models developed by others. For example, other GPT foundation models include a series of models created by EleutherAI, and seven models created by Cerebras in 2023. Companies in different industries have developed task-specific GPTs in their respective fields, such as Salesforce\\'s \"EinsteinGPT\" (for CRM) and Bloomberg\\'s \"BloombergGPT\" (for finance).\\n\\n\\n== History ==\\n\\n\\n=== Initial developments ===\\nGenerative pretraining (GP) was a long-established concept in machine learning applications. It was originally used as a form of semi-supervised learning, as the model is trained first on an unlabelled dataset (pretraining step) by learning to generate datapoints in the dataset, and then it is trained to classify a labelled dataset. \\nThere were mainly 3 types of early GP. The hidden Markov models learn a generative model of sequences for downstream applications. For example, in speech recognition, a trained HMM infers the most likely hidden sequence for a speech signal, and the hidden sequence is taken as the phonemes of the speech signal. These were developed in the 1970s and became widely applied in speech recognition in the 1980s.\\nThe compressors learn to compress data such as images and textual sequences, and the compressed data serves as a good representation for downstream applications such as facial recognition. The autoencoders similarly learn a latent representation of data for later downstream applications such as speech recognition. The connection between autoencoders and algorithmic compressors was noted in 1993.\\n\\nDuring the 2010s, the problem of machine translation was solved by recurrent neural networks, with attention mechanism added. This was optimized into the transformer architecture, published by Google researchers in Attention Is All You Need (2017). That development led to the emergence of large language models such as BERT (2018) which was a pre-trained transformer (PT) but not designed to be generative (BERT was an \"encoder-only\" model). Also in 2018, OpenAI published Improving Language Understanding by Generative Pre-Training, which introduced GPT-1, the first in its GPT series.\\nPreviously in 2017, some of the authors who would later work on GPT-1 worked on generative pre-training of language with LSTM, which resulted in a model that could represent text with vectors that could easily be fine-tuned for downstream applications.\\nPrior to transformer-based architectures, the best-performing neural NLP (natural language processing) models commonly employed supervised learning from large amounts of manually-labeled data. The reliance on supervised learning limited their use on datasets that were not well-annotated, and also made it prohibitively expensive and time-consuming to train extremely large language models.\\nThe semi-supervised approach OpenAI employed to make a large-scale generative system—and was first to do with a transformer m'),\n",
       " Document(metadata={'title': 'Suchir Balaji', 'source': 'https://en.wikipedia.org/wiki/Suchir_Balaji'}, page_content='Suchir Balaji (1998 – November 26, 2024) was an artificial intelligence researcher and former employee of OpenAI, where he worked from 2020 until 2024. He gained attention for his whistleblowing activities related to artificial intelligence ethics and the inner workings of OpenAI. \\nBalaji was found dead in his home on November 26, 2024. San Francisco authorities determined the death was a suicide, though Balaji\\'s parents have disputed the verdict.\\n\\n\\n== Early life and education ==\\nBalaji was born in an Indian-American household and was raised in Cupertino, California. He attended Monta Vista High School and was a finalist for the 2015-16 season of the United States of America Computing Olympiad. He graduated from the University of California, Berkeley in 2021, receiving a Bachelor of Arts with a major in computer science. He was the 31st position in the ACM International Collegiate Programming Contest 2018 World Finals. He was first place in both the 2017 Pacific Northwest Regional and Berkeley Programming Contests.\\n\\n\\n== Career ==\\nSuchir Balaji spent four years as an artificial intelligence researcher at OpenAI. Among other projects, he was involved in gathering and organizing the internet data used to train GPT-4, a language model used by the company\\'s online chatbot, ChatGPT. He left the company in August 2024 after becoming disillusioned with its business practices, which he publicly denounced, alleging the company violated the United States copyright law in order to develop ChatGPT. Furthermore, he charged that ChatGPT would render many people and firms commercially unviable by utilizing their content to make improvements to OpenAI\\'s artificial intelligence systems. In an October 2024 story published by the New York Times, Balaji expressed these concerns regarding OpenAI, and was quoted saying \"If you believe what I believe, you have to just leave the company.\"\\nOpenAI was involved in legal matters relating to data-sharing, as North American news publishers and professional writers alike filed lawsuits against the company, alleging illegal use of their articles for software training. The company argued that the software was \"grounded in fair use and related international copyright principles that are fair for creators and support innovation\", and defended its business practices by contending that its software models are \"trained on publicly available data.\"\\nAfter leaving OpenAI, Balaji said he had been working on \"personal projects\", including plans to create a nonprofit centered on machine learning and neurosciences.\\nOn October 23rd 2024, about a month prior to his death, Balaji posted an essay on his personal website titled \"When does generative AI qualify for fair use?\". In the essay, he mathematically analyzes outputs of large language models such as ChatGPT and argues that they do not satisfy the four factors outlined in the United States\\' Fair Use law. He concluded, \"None of the four factors seem to weigh in favor of ChatGPT being a fair use of its training data. That being said, none of the arguments here are fundamentally specific to ChatGPT either, and similar arguments could be made for many generative AI products in a wide variety of domains\".\\n\\n\\n== Death ==\\nOn November 26, 2024, police said they found Balaji dead in his apartment in San Francisco, when they arrived there after being requested to conduct a well-being check. He was 26 years old. A spokesperson for his former employer, OpenAI,  said he was \"devastated\" by the news of Balaji\\'s death. \\nThe police ruled that there was \"no evidence of foul play\" found during the initial investigation, and the San Francisco medical examiner\\'s office confirmed the cause of death as suicide. His death has prompted public and media interest, particularly given his whistleblowing claims. \\nBalaji\\'s parents say that the circumstances surrounding Balaji\\'s death are unclear, and that they have hired an independent investigator to determine the cause of death. They also say they h'),\n",
       " Document(metadata={'title': 'Perplexity AI', 'source': 'https://en.wikipedia.org/wiki/Perplexity_AI'}, page_content='Perplexity AI is a conversational search engine that uses large language models (LLMs) to answer queries using sources from the web and cites links within the text response. Its developer, Perplexity AI, Inc., is based in San Francisco, California.\\n\\n\\n== History ==\\n\\nPerplexity was founded in 2022 by Aravind Srinivas, Andy Konwinski, Denis Yarats and Johnny Ho, engineers with backgrounds in back-end systems, artificial intelligence (AI) and machine learning:\\n\\nSrinivas, the CEO, worked at OpenAI as an AI researcher.\\nKonwinski was among the founding team at Databricks.\\nYarats, the CTO, was an AI research scientist at Meta.\\nHo, the CSO, worked as an engineer at Quora, then as a quantitative trader on Wall Street.\\n\\n\\n== Services ==\\nPerplexity works on a freemium model. It also has an enterprise version of its product.\\n\\n\\n=== Free plan ===\\nThe free model uses the company\\'s standalone LLM based on GPT-3.5 with browsing.\\nIt uses the context of the user queries to provide a personalized search result. Perplexity summarizes the search results and produces a text with inline citations.\\nPerplexity also enables users to use Pages to generate customizable webpage and research presentations based on user prompts.\\n\\n\\n=== Perplexity Pro ===\\nProvides access to an API\\nSearch both internal files and web content\\nAsks the user clarifying questions to refine queries\\nEnables users to upload and analyze local files, including images\\nHas access to GPT-4, Claude 3.5, Grok-2, Llama 3 and in-house Perplexity LLMs\\nAble to generate images using AI via Playground v3, DALL-E, Stable Diffusion and FLUX 1\\n\\n\\n=== Shopping hub ===\\nOn 18 November 2024, Perplexity launched its shopping hub to attract users, backed by Amazon and leading AI chipmaker Nvidia. This will give users product cards which will show relevant items in response to asked questions about shopping.\\n\\n\\n=== Internal Knowledge Search ===\\nInternal Knowledge Search enables Pro and Enterprise Pro users to search across web content and internal documents simultaneously. Users can upload and search through Excel, Word, PDF, and other common file formats. Enterprise Pro users have a limit of 500 files for upload and indexing.\\n\\n\\n=== Finance ===\\nIn October 2024, introduced new finance-related features, including looking up stock prices and company earnings data. The tool provides real-time stock quotes and price tracking, industry peer comparisons and basic financial analysis tools. The platform sources its financial data from Financial Modeling Prep (FMP) to ensure accuracy.\\n\\n\\n=== Spaces ===\\nPerplexity Spaces was released in October 2024 as an AI-powered collaboration hub. The platform allows users to create customized knowledge spaces that combine web searches with personal file integration. Users can upload up to 50 different documents, with a 25MB size limit per file.\\n\\n\\n== As a business ==\\nAs of 2024, Perplexity has raised $165 million in funding, valuing the company at over $1 billion. \\nAs of December 2024, Perplexity closed a $500 million round of funding that elevates its valuation to $9 billion.\\nIn July 2024, Perplexity announced the launch of a new publishers\\' program to share ad revenue with partners.\\nPerplexity AI plans to introduce ads on its search platform by Q4 of 2024.\\n\\n\\n=== Notable investors ===\\nJeff Bezos\\nNvidia\\nDatabricks\\nBessemer Venture Partners\\nSusan Wojcicki\\nJeff Dean\\nYann LeCun\\nAndrej Karpathy\\nNat Friedman\\nGarry Tan\\n\\n\\n== Controversies ==\\n\\n\\n=== Forbes ===\\nIn June 2024, Forbes publicly criticized Perplexity for use of their content.\\nAccording to Forbes, Perplexity published a story which was largely copied from a proprietary Forbes article, without mentioning or prominently citing Forbes. \\nIn response, Srinivas said that the feature had some \"rough edges\" and accepted feedback, but maintained that Perplexity only \"aggregates\" rather than plagiarizes information.\\n\\n\\n=== Wired ===\\nIn June 2024, separate investigations by the magazine Wired and web developer Robb Knight found that Perplexity doe'),\n",
       " Document(metadata={'source': '/Apple_MacOS_English.pdf'}, page_content='ENGLISH\\nAPPLE INC.\\nSOFTWARE LICENSE AGREEMENT FOR macOS Sequoia\\nFor use on Apple-branded Systems\\nPLEASE READ THIS SOFTWARE LICENSE AGREEMENT (“LICENSE”) CAREFULLY BEFORE \\nUSING THE APPLE SOFTWARE.  BY USING THE APPLE SOFTWARE, YOU ARE AGREEING TO BE \\nBOUND BY THE TERMS OF THIS LICENSE.  IF YOU DO NOT AGREE TO THE TERMS OF THIS \\nLICENSE, DO NOT INSTALL AND/OR USE THE APPLE SOFTWARE AND, IF PRESENTED WITH \\nTHE OPTION TO “AGREE” OR “DISAGREE” TO THE TERMS, CLICK “DISAGREE”. IF YOU \\nACQUIRED THE APPLE SOFTWARE AS PART OF AN APPLE HARDWARE PURCHASE AND IF YOU \\nDO NOT AGREE TO THE TERMS OF THIS LICENSE, YOU MAY RETURN THE ENTIRE APPLE \\nHARDWARE/SOFTWARE PACKAGE WITHIN THE RETURN PERIOD TO THE APPLE STORE OR \\nAUTHORIZED DISTRIBUTOR WHERE YOU OBTAINED IT FOR A REFUND, SUBJECT TO APPLE’S \\nRETURN POLICY FOUND AT https://www.apple.com/legal/sales-support/. YOU MUST RETURN \\nTHE ENTIRE HARDWARE/SOFTWARE PACKAGE IN ORDER TO OBTAIN A REFUND.\\nIMPORTANT NOTE: To the extent that this software may be used to reproduce, modify, publish or \\ndistribute materials, it is licensed to you only for reproduction, modiﬁcation, publication and \\ndistribution of non-copyrighted materials, materials in which you own the copyright, or materials \\nyou are authorized or legally permitted to reproduce, modify, publish or distribute. If you are \\nuncertain about your right to copy, modify, publish or distribute any material, you should contact \\nyour legal advisor.\\n1. General.\\nA. The Apple software (including Boot ROM code), any third party software, documentation, interfaces, \\ncontent, fonts and any data accompanying this License (“Original Apple Software”), as may be updated \\nor replaced by feature enhancements, software updates, security responses, system ﬁles, or system \\nrestore software provided by Apple for your Apple-branded computer or supported peripheral device \\n(“Apple Software Changes”), whether preinstalled on Apple-branded hardware, on internal storage, on \\nremovable media, on disk, in read only memory, on any other media or in any other form (the Original \\nApple Software and Apple Software Changes are collectively referred to as the “Apple Software”), are \\nlicensed, not sold, to you by Apple Inc. (“Apple”) for use only under the terms of this License. Apple and/\\nor Apple’s licensors retain ownership of the Apple Software itself and reserve all rights not expressly \\ngranted to you. You agree that the terms of this License will apply to any Apple-branded application \\nsoftware product that may be preinstalled on your Apple-branded hardware, unless such product is \\naccompanied by a separate license, in which case you agree that the terms of that license will govern \\nyour use of that product.\\nB. Apple, at its discretion, may make available future Apple Software Changes. Apple Software \\nChanges, if any, may not necessarily include all existing software features or new features that Apple \\nreleases for newer or other models of Apple-branded computers. The terms of this License will govern \\nany Apple Software Changes provided by Apple, unless such Apple Software Changes are accompanied \\nby a separate license, in which case the terms of that license will govern.\\nC. Title and intellectual property rights in and to any content displayed by or accessed through the Apple \\nSoftware belongs to the respective content owner. Such content may be protected by copyright or other \\nintellectual property laws and treaties, and may be subject to terms of use of the third party providing \\nsuch content. Except as otherwise provided herein, this License does not grant you any rights to use \\nsuch content nor does it guarantee that such content will continue to be available to you. You are '),\n",
       " Document(metadata={'source': '/Apple_MacOS_English.pdf'}, page_content='prohibited from republishing, retransmitting or reproducing any images accessed through News or Maps \\nas a stand-alone ﬁle.\\n2. Permitted License Uses and Restrictions.\\nA. Preinstalled and Single-Copy Apple Software License. Subject to the terms and conditions of this \\nLicense, unless you obtained the Apple Software from the Mac App Store, through an automatic \\ndownload or under a volume license, maintenance or other written agreement from Apple, you are \\ngranted a limited, non-exclusive license to install, use and run one (1) copy of the Apple Software on a \\nsingle Apple-branded computer at any one time. For example, these single-copy license terms apply to \\nyou if you obtained the Apple Software preinstalled on Apple-branded hardware.\\nB. Mac App Store License. If you obtained a license for the Apple Software from the Mac App Store or \\nthrough an automatic download, then subject to the terms and conditions of this License and as \\npermitted by the Services and Content Usage Rules set forth in the Apple Media Services Terms and \\nConditions (https://www.apple.com/legal/internet-services/itunes/) (“Usage Rules”), you are granted a \\nlimited, non-transferable, non-exclusive license:\\n(i) to download, install, use and run for personal, non-commercial use, one (1) copy of the Apple \\nSoftware directly on each Apple-branded computer running macOS Sonoma, macOS Ventura, \\nmacOS Monterey, macOS Big Sur, macOS Catalina, macOS Mojave, or macOS High Sierra  \\n(“Mac Computer”) that you own or control;\\n(ii) If you are a commercial enterprise or educational institution, to download, install, use and run \\none (1) copy of the Apple Software for use either: (a) by a single individual on each of the Mac \\nComputer(s) that you own or control, or (b) by multiple individuals on a single shared Mac \\nComputer that you own or control. For example, a single employee may use the Apple Software \\non both the employee’s desktop Mac Computer and laptop Mac Computer, or multiple students \\nmay serially use the Apple Software on a single Mac Computer located at a resource center or \\nlibrary; and \\n(iii) to install, use and run up to two (2) additional copies or instances of the Apple Software, or \\nany prior macOS or OS X operating system software or subsequent release of the Apple \\nSoftware, within virtual operating system environments on each Apple-branded computer you \\nown or control that is already running the Apple Software, for purposes of: (a) software \\ndevelopment; (b) testing during software development; (c) using macOS Server; or (d) personal, \\nnon-commercial use.\\nExcept as expressly permitted in Section 3, the grant set forth in Section 2B(iii) above does not permit \\nyou to use the virtualized copies or instances of the Apple Software in connection with service bureau, \\ntime-sharing, terminal sharing, relay service or other similar types of services. Except as expressly \\npermitted in this Section 2B, you may not use the Apple Software to run any Apple operating system \\nsoftware, including iOS, iPadOS, watchOS or tvOS, in virtual operating system environments on Mac \\nComputer(s).\\nC. Volume or Maintenance License. If you obtained the Apple Software under a volume or maintenance \\nlicense program with Apple, the terms of your volume or maintenance license will determine the number \\nof copies of the Apple Software you are permitted to download, install, use and run on Apple-branded \\ncomputers you own or control.  Except as agreed to in writing by Apple, all other terms and conditions \\nof this License shall apply to your use of the Apple Software obtained under a volume or maintenance \\nlicense.\\nD. System Requirements; Apple Account. Please note that the Apple Software is supported on only '),\n",
       " Document(metadata={'source': '/Apple_MacOS_English.pdf'}, page_content='Apple-branded hardware that meets speciﬁed system requirements as indicated by Apple. In addition, \\nuse of and access to certain features of the Apple Software and certain Services (as deﬁned in Section \\n6) may require you to apply for a unique user name and password combination, known as an Apple \\nAccount.\\nE. Fonts. Subject to the terms and conditions of this License, you may use the fonts included with the \\nApple Software to display and print content while running the Apple Software; however, you may only \\nembed fonts in content if that is permitted by the embedding restrictions accompanying the font in \\nquestion. These embedding restrictions can be found in the Font Book/Preview/Show Font Info panel.\\nF. Voices; Live Captions. Subject to the terms and conditions of this License, you may: (i) use the system \\nvoices included in the Apple Software (“System Voices”) (1) while running the Apple Software and (2) to \\ncreate your own original content and projects for your personal, non-commercial use; (ii) use the Live \\nCaptions automatically generated on-device in real time by the Apple Software (“Live Captions”), \\nwhether generated during a FaceTime call or otherwise, only for your personal, non-commercial use; and \\n(iii) use the Personal Voice feature to create a voice on device that sounds like you (“Personal Voice”) \\nusing your own\\xa0personal voice for your personal, non-commercial use. No other creation or use of the \\nSystem Voices, Live Captions or Personal Voice is permitted by this License, including but not limited to \\nthe use, reproduction, display, performance, recording, publishing or redistribution of any of the System \\nVoices, Live Captions or Personal Voice in a proﬁt, non-proﬁt, public sharing or commercial context.\\nG. Photos App Features and Support. The Photos application of the Apple Software (“Photos App”) may \\nnot support some video and photo formats. Use of some features of the Photos App will depend on the \\nfeatures of your camera. Synchronizing photos with the Photos App and any Apple or third party \\nservices may result in loss of data. The slideshow graphics, music and themes included with the Photos \\nApp are only for personal, non-commercial use in slideshows you create using the Photos App. You may \\nnot use, extract or distribute, commercially or otherwise, on a standalone basis, any photographs, \\nimages, graphics, artwork, audio, video or similar assets (“Digital Materials”) contained within, or \\nprovided as a part of, the Photos App, or otherwise use the Digital Materials outside the context of its \\nintended use as part of the Photos App.\\nH. Content Caching Features.\\n1.\\xa0 To the extent that Apple and/or its aﬃliates make particular software and/or content available for \\ncaching (e.g., applicable content from the Mac App Store) (“Apple Eligible Content”), certain features of \\nthe Apple Software (the “Content Caching Features”) may automatically download and locally cache \\nsuch Apple Eligible Content on your Apple-branded computer that is running the Apple Software (for \\npurposes of this Section, such Apple-branded computer is referred to as the “Caching Enabled Mac”).\\xa0 \\nBy using the Content Caching Features of the Apple Software, you agree that Apple may \\ndownload and cache such Apple Eligible Content on your Caching Enabled Mac.\\xa0 You can turn oﬀ \\nthe Content Caching Features of the Apple Software at any time by going to Sharing under System \\nSettings on your Caching Enabled Mac.\\n2.\\xa0 The Content Caching Features of the Apple Software are for use only on a Caching Enabled Mac you \\nown or control and solely for purposes of expediting the delivery of such Apple Eligible Content to \\nauthorized end users within your home, company or organization.\\xa0 You understand that such users may \\nneed to separately authenticate with Apple prior to receiving the Apple Eligible Content and that the \\nexpedited delivery of Apple Eligible Content through the use of your Caching Enabled Mac will not \\nmodify the terms under which you or your end users receive such Apple Eligible Content.\\xa0\\n3.\\xa0 You acknowledge and agree that all use of the Apple Eligible Content is subject to the applicable \\nlicense terms that govern the type of Apple Eligible Content being cached. \\xa0These terms may include, \\nbut are not limited to, the Apple Media Services Terms and Conditions, the iCloud Terms and '),\n",
       " Document(metadata={'source': '/Apple_MacOS_English.pdf'}, page_content='Conditions, the iTunes U Instructor Agreement, the iTunes U Software License Agreement, and/or the \\napplicable licensing terms that accompanied the software being downloaded, unless the download was \\naccompanied by its own separate license agreement in which case the latter would apply.\\xa0\\xa0A list of \\nApple Software License Agreements (SLAs) may be found here: https://www.apple.com/legal/sla/.\\xa0 You \\nacknowledge and agree that the use of the Content Caching Features and storage of Apple Eligible \\nContent on your Caching Enabled Mac does not transfer to you any rights beyond those granted to you \\nin the applicable license terms for the Apple Eligible Content and shall not constitute a grant, waiver, or \\nother limitation of any rights of Apple or any other copyright owners in any Apple Eligible Content.\\n4.\\xa0 You are not authorized to deploy your Caching Enabled Mac with the Content Caching Features \\nenabled on a network you do not own or control (or which you are not legally authorized to use for such \\npurposes), or to permit access to such Apple Eligible Content from end users outside of your home, \\ncompany or organization.\\xa0 You agree to only use the Content Caching Features for your own personal, \\nnon-commercial use or for internal use within your company or organization, and only as expressly \\npermitted herein.\\xa0 You may not provide a service to third parties that integrates with or leverages \\nservices or information provided by the Content Caching Features or uses the Content Caching Features \\nin any way.\\n5.\\xa0 By enabling the Content Caching Features of the Apple Software, you agree that Apple may store, \\nmonitor, and secure the Apple Eligible Content on your Caching Enabled Mac, and may collect and use \\ntechnical information about your Caching Enabled Mac and related networks, including but not limited \\nto, hardware identiﬁers and IP addresses, for such purposes.\\xa0 You agree not to disable, disrupt, hack, \\ncircumvent, or otherwise interfere with Apple’s veriﬁcation, storage or authentication mechanisms, digital \\nsigning, digital rights management, or other security mechanisms implemented in or by the Apple \\nSoftware, services, the Apple Eligible Content, or other Apple software or technology, or to enable \\nothers to do so. \\xa0\\n6.\\xa0 Apple reserves the right to stop making Apple Eligible Content available for caching on your Caching \\nEnabled Mac (e.g., some content that you may have previously cached may not be available for \\nsubsequent caching) and to remove any cached Apple Eligible Content from your Caching Enabled \\nMacs at any time in its sole discretion, and Apple shall have no liability to you in such event.\\xa0 You \\nunderstand that such caching of Apple Eligible Content may not be available in all countries or regions.\\xa0 \\nYou may remove the cached Apple Eligible Content and disable the Content Caching Features at any \\ntime.\\nI. Remote Desktop Connections.\\xa0 Subject to the terms and conditions of this License, when remotely \\nconnecting from another computer or electronic device (each a “Device”) to an Apple-branded computer \\nthat is running the Apple Software (for purposes of this Section, such Apple-branded computer is \\nreferred to as the “Home Mac”), whether through the Screen Sharing feature or through any other \\nmeans:\\n(i) only one (1) Device may remotely connect at any one time, whether directly or indirectly, to \\ncontrol the graphical desktop session of the Apple Software that is running and being displayed \\non the Home Mac; and\\n(ii) a reasonable number of Devices may remotely connect at the same time for the sole purpose \\nof simultaneously observing the same graphical desktop session of the Apple Software that is \\nrunning and being displayed on the Home Mac, as long as they do not control the Apple \\nSoftware in any way; but\\n(iii) only one (1) Apple-branded Device may remotely connect at any one time, whether directly or \\nindirectly, to control a separate graphical desktop session of the Apple Software that is diﬀerent '),\n",
       " Document(metadata={'source': '/Apple_MacOS_English.pdf'}, page_content='from the one running and being displayed on the Home Mac, and such connection may only be \\nmade through the Screen Sharing feature of the Apple Software.\\nExcept as expressly permitted in this Section 2I or Section 3, or except as otherwise licensed by Apple, \\nyou agree not to use the Apple Software, or any of its functionality, in connection with service bureau, \\ntime-sharing, terminal sharing or other similar types of services, whether such services are being \\nprovided within your own organization or to third parties.\\nJ. Other Use Restrictions. The grants set forth in this License do not permit you to, and you agree not to, \\ninstall, use or run the Apple Software on any non-Apple-branded computer, or to enable others to do so. \\nYou agree not to remove, obscure, or alter any proprietary notices (including trademark and copyright \\nnotices) that may be aﬃxed to or contained within the Apple Software. Except as otherwise permitted by \\nthe terms of this License or otherwise licensed by Apple: (i) only one user may use the Apple Software at \\na time, and (ii) you may not make the Apple Software available over a network where it could be run or \\nused by multiple computers at the same time. Except as expressly permitted in Section 3, you may not \\nrent, lease, lend, sell, redistribute or sublicense the Apple Software.\\nK. Backup Copy. You may make one copy of the Apple Software (excluding the Boot ROM code and \\nother Apple ﬁrmware that is embedded or otherwise contained in Apple-branded hardware) in machine-\\nreadable form for backup purposes only; provided that the backup copy must include all copyright or \\nother proprietary notices contained on the original. Apple Boot ROM code and ﬁrmware is provided only \\nfor use on Apple-branded hardware and you may not copy, modify or redistribute the Apple Boot ROM \\ncode or ﬁrmware, or any portions thereof.\\nL. Migration of Existing Software. If you use Setup/Migration Assistant to transfer software from one \\nApple-branded computer to another Apple-branded computer, please remember that continued use of \\nthe original copy of the software may be prohibited once a copy has been transferred to another \\ncomputer, unless you already have a licensed copy of such software on both computers. You should \\ncheck the relevant software license agreements for applicable terms and conditions. Third party \\nsoftware and services may not be compatible with this Apple Software and installation of this Apple \\nSoftware may aﬀect the availability and usability of such third party software or services.\\nM. Open Source. Certain components of the Apple Software, and third party open source programs \\nincluded with the Apple Software, have been or may be made available by Apple on its Open Source \\nweb site (https://www.opensource.apple.com/) (collectively the “Open-Sourced Components”). You may \\nmodify or replace only these Open-Sourced Components; provided that: (i) the resultant modiﬁed Apple \\nSoftware is used, in place of the unmodiﬁed Apple Software, on Apple-branded computers you own or \\ncontrol, as long as each such Apple computer has a properly licensed copy of the Apple Software on it; \\nand (ii) you otherwise comply with the terms of this License and any applicable licensing terms \\ngoverning use of the Open-Sourced Components. Apple is not obligated to provide any updates, \\nmaintenance, warranty, technical or other support, or services for the resultant modiﬁed Apple Software. \\nYou expressly acknowledge that if failure or damage to Apple hardware results from modiﬁcation of the \\nOpen-Sourced Components of the Apple Software, such failure or damage is excluded from the terms of \\nthe Apple hardware warranty.\\nN. No Reverse Engineering. You may not, and you agree not to or enable others to, copy (except as \\nexpressly permitted by this License or by the Usage Rules if they are applicable to you), decompile, \\nreverse engineer, disassemble, attempt to derive the source code of, decrypt, modify, or create \\nderivative works of the Apple Software or any services provided by the Apple Software or any part \\nthereof (except as and only to the extent any foregoing restriction is prohibited by applicable law or by \\nlicensing terms governing use of Open-Sourced Components that may be included with the Apple \\nSoftware).'),\n",
       " Document(metadata={'source': '/Apple_MacOS_English.pdf'}, page_content='O. Compliance with Laws. You agree to use the Apple Software and the Services (as deﬁned in Section \\n6 below) in compliance with all applicable laws, including local laws of the country or region in which you \\nreside or in which you download or use the Apple Software and Services. Features of the Apple \\nSoftware and the Services may not be available in all languages or regions, and some features may vary \\nby region. An Internet connection is required for some features of the Apple Software and Services.\\nP. Third Party Software. Apple has provided as part of the Apple Software package, and may provide as \\nan upgrade, update or supplement to the Apple Software, access to certain third party software or \\nservices as a convenience. To the extent that the Apple Software contains or provides access to any \\nthird party software or services, Apple has no express or implied obligation to provide any technical or \\nother support for such software or services. Please contact the appropriate software vendor, \\nmanufacturer or service provider directly for technical support and customer service related to its \\nsoftware, service and/or products.\\nQ. Apple Software Changes. The Apple Software will periodically check with Apple for Apple Software \\nChanges. If a change is available, the change may automatically download and install onto your \\ncomputer and, if applicable, your peripheral devices. By using the Apple Software, you agree that \\nApple may download and install automatic Apple Software Changes onto your computer and your \\nperipheral devices. You can turn oﬀ automatic operating system updates and Rapid Security \\nResponses at any time by adjusting the settings found within System Settings > Software Update > \\nAdvanced.\\nR. System Characters. Subject to the terms and conditions of this License, you may use the Memoji \\ncharacters included in or created with the Apple Software (“System Characters”): (i) while running the \\nApple Software; and (ii) to create your own original content and projects for your personal, non-\\ncommercial use. No other use of the System Characters is permitted by this License, including but not \\nlimited to the use, reproduction, display, performance, recording, publishing or redistribution of any of \\nthe System Characters in a proﬁt, non-proﬁt, public sharing or commercial context.\\n3. Leasing for Permitted Developer Services.\\nA. Leasing.  You may lease or sublease a validly licensed version of the Apple Software in its entirety to \\nan individual or organization (each, a “Lessee”) provided that all of the following conditions are met: \\n(i) the leased Apple Software must be used for the sole purpose of providing Permitted \\nDeveloper Services and each Lessee must review and agree to be bound by the terms of this \\nLicense;\\n(ii) each lease period must be for a minimum period of twenty-four (24) consecutive hours; \\n(iii) during the lease period, the End User Lessee must have sole and exclusive use and control \\nof the Apple Software and the Apple-branded hardware on which it is installed, except that you, \\nas the party leasing the Apple Software (“Lessor”), may provide administrative support for the \\nApple Software; and \\n(iv) prior to using the Apple Software, the End User Lessee must review and agree to be bound \\nby the terms applicable to any software preinstalled on the Apple Software, including, but not \\nlimited to Apple’s Xcode developer software and any other Apple or third-party software.\\nFor purposes of this Section 3: (A) End User Lessee means a Lessee who is the end user ultimately \\nusing the leased Apple Software solely for Permitted Developer Services; and (B) Permitted Developer \\nServices means continuous integration services, including but not limited to software development, \\nbuilding software from source, automated testing during software development, and running necessary '),\n",
       " Document(metadata={'source': '/Apple_MacOS_English.pdf'}, page_content='developer tools to support such activities. Each Lessor must provide Apple with advance notice prior to \\nleasing or subleasing the Apple Software pursuant to this Section 3 by contacting Apple Developer \\nRelations (https://developer.apple.com/contact/macos-license/).\\nB. Subleasing.  A Lessee may further sublease the Apple Software pursuant to this Section 3 provided \\nthat such Lessee complies with all of the terms of this Section 3. A Lessee subleasing the Apple \\nSoftware (who shall also be considered a Lessor under this Section 3) must fully relinquish exclusive use \\nand control of the Apple Software and the Apple-branded hardware on which it is installed to its Lessee \\nduring the lease period.\\nC. Enforcement. As a Lessor, you shall be responsible for: (i) ensuring that each Lessee complies with \\nthe requirements of this Section 3; (ii) ensuring each Lessee agrees to all applicable license terms; and \\n(iii) assisting Apple in enforcing compliance therewith. If a Lessee breaches this License or other \\napplicable Apple license terms, their rights to use the Apple Software shall automatically terminate and \\nyou agree to immediately terminate such Lessee’s use of the Apple Software upon your discovery of \\nsuch breach or upon written notice from Apple of such breach.  \\nD. Virtualization.  For each copy of the Apple Software subject to a lease under this Section 3, either a \\nLessor or a Lessee (but not both) may install, use and run additional copies or instances of the Apple \\nSoftware within virtual operating system environments in accordance with Section 2B(iii), provided that a \\nLessor may only virtualize a single instance or copy of the Apple Software as a provisioning tool for the \\npurpose of providing a Lessee with access to and use of the Apple Software pursuant to this Section 3. \\nE. System Conﬁguration. If you are a Lessee, you acknowledge that the Apple Software may have been \\npreviously conﬁgured by the Lessor who is leasing the Apple Software to you, including the selection of \\nsettings for Analytics, Location Services, and other security, privacy and data collection-related features. \\nYou acknowledge and agree that Apple is not responsible for the conﬁguration of the Apple Software by \\nthe Lessor.\\n4. Transfer.\\nA. If you obtained the Apple Software preinstalled on Apple-branded hardware or if you obtained your \\nlicense to the Apple Software from the Mac App Store or through a software update, you may make a \\none-time permanent transfer of all of your license rights to the Apple Software (in its original form as \\nprovided by Apple) to another party, provided that: (i) the Apple Software is transferred together with \\nyour Apple-branded hardware; (ii) the transfer must include all of the Apple Software, including all its \\ncomponent parts and this License; (iii) you do not retain any copies of the Apple Software, full or partial, \\nincluding copies stored on a computer or other storage device; and (iv) the party receiving the Apple \\nSoftware reads and agrees to accept the terms and conditions of this License. For purposes of this \\nLicense, if Apple provides an update (e.g., version 10.14 to 10.14.1) to the Apple Software, the update is \\nconsidered part of the Apple Software and may not be transferred separately from the pre-update \\nversion of the Apple Software.\\nB. You may not transfer any Apple Software that has been modiﬁed or replaced under Section 2M \\nabove. All components of the Apple Software are provided as part of a bundle and may not be \\nseparated from the bundle and distributed as standalone applications. Note that the Apple Software \\nprovided with a particular Apple-branded hardware product might not run on other models of Apple-\\nbranded hardware.\\nC. Any copy of the Apple Software that may be provided by Apple for promotional, evaluation, \\ndiagnostic or restorative purposes may be used only for such purposes and may not be resold or \\ntransferred.'),\n",
       " Document(metadata={'source': '/Apple_MacOS_English.pdf'}, page_content='D. Apple may assign all or any portion of this License or any rights or obligations hereunder to any Apple \\naﬃliate or subsidiary at any time without notice.\\n5. Consent to Use of Data. Certain features like Analytics, Location Services, Siri, Dictation and \\nSpotlight may require information from your computer to provide their respective functions. When you \\nturn on or use these features, details\\xa0will be provided\\xa0regarding what information is sent to Apple and \\nhow the information may be used.\\xa0 You can learn more by visiting https://www.apple.com/privacy/. At all \\ntimes your information will be treated in accordance with Apple’s Privacy Policy, which can be viewed at: \\nhttps://www.apple.com/legal/privacy/.\\n6. Apple Software, Services, and Third Party Materials.\\nA. General. The Apple Software may enable access to Apple’s iTunes Store, Mac App Store, Apple \\nBooks, Game Center, iCloud, Maps, News and other Apple and third party services and web sites \\n(collectively and individually, “Services”). Use of these Services requires Internet access and use of \\ncertain Services may require an Apple Account, may require you to accept additional terms and may be \\nsubject to additional fees. By using this software in connection with an Apple Account, or other Apple \\nServices, you agree to the applicable terms of service, such as the latest Apple Media Services Terms \\nand Conditions for the country or region in which you access such Services, which you may access and \\nreview at https://www.apple.com/legal/internet-services/itunes/.\\nB. If you sign up for iCloud, certain iCloud features like “iCloud Drive”, “Shared Albums” and “Find My” \\nmay be accessed directly from the Apple Software. You acknowledge and agree that your use of iCloud \\nand these features is subject to the latest terms and conditions of the iCloud service, which you may \\naccess and review at: https://www.apple.com/legal/internet-services/icloud/.\\nC. News App Content. Your use of content accessed through the News application is limited solely to \\npersonal, noncommercial use, does not transfer any ownership interest to you in the content, and \\nspeciﬁcally excludes, without limitation, any commercial or promotional use rights in such content.  \\nD. Maps. The maps service and features of the Apple Software (“Maps”), including map data coverage, \\nmay vary by region. When you turn on or use Maps, details will be provided regarding what information \\nis sent to Apple and how the information may be used. You acknowledge and agree that your use of \\nMaps is subject to the latest terms and conditions of the Maps service, which you may access and \\nreview on the sidebar of the Maps application.\\nE. You understand that by using any of the Apple Software and Services, you may encounter content \\nthat may be deemed oﬀensive, indecent, or objectionable, which content may or may not be identiﬁed \\nas having explicit language, and that the results of any search or entering of a particular URL may \\nautomatically and unintentionally generate links or references to objectionable material. Nevertheless, \\nyou agree to use the Apple Software and Services at your sole risk and that Apple, its aﬃliates, agents, \\nprincipals, or licensors shall have no liability to you for content that may be found to be oﬀensive, \\nindecent, or objectionable.\\nF. Certain Apple Software and Services may display, include or make available content, data, \\ninformation, applications or materials from third parties (“Third Party Materials”) or provide links to \\ncertain third party web sites. By using the Apple Software and Services, you acknowledge and agree \\nthat Apple is not responsible for examining or evaluating the content, accuracy, completeness, \\ntimeliness, validity, copyright compliance, legality, decency, quality or any other aspect of such Third \\nParty Materials or web sites. Apple, its oﬃcers, aﬃliates and subsidiaries do not warrant or endorse and \\ndo not assume and will not have any liability or responsibility to you or any other person for any third-\\nparty Services, Third Party Materials or web sites, or for any other materials, products, or services of \\nthird parties. Third Party Materials and links to other web sites are provided solely as a convenience to '),\n",
       " Document(metadata={'source': '/Apple_MacOS_English.pdf'}, page_content='you.\\nG. Neither Apple nor any of its content providers guarantees the availability, accuracy, completeness, \\nreliability, or timeliness of stock information, location data, health information or any other data displayed \\nby any Apple Software and Services. Financial information displayed by any Apple Software and \\nServices is for general informational purposes only and should not be relied upon as investment advice. \\nBefore executing any securities transaction based upon information obtained through the Apple \\nSoftware and Services, you should consult with a ﬁnancial or securities professional who is legally \\nqualiﬁed to give ﬁnancial or securities advice in your country or region. You should evaluate the outputs \\ngenerated by artiﬁcial intelligence for accuracy and appropriateness for your use case, including using \\nhuman review as appropriate, before using or sharing the outputs from the Apple Software and Services. \\nLocation data provided by any Apple Software and Services, including the Apple Maps service, is \\nprovided for basic navigational and/or planning purposes only and is not intended to be relied upon in \\nsituations where precise location information is needed or where erroneous, inaccurate, time-delayed or \\nincomplete location data may lead to death, personal injury, or property or environmental damage. You \\nagree that the results you receive from the Maps service may vary from actual road or terrain conditions \\ndue to factors that can aﬀect the accuracy of the Maps data, such as, but not limited to, weather, road \\nand traﬃc conditions, and geopolitical events. For your safety, always pay attention to\\xa0posted road signs \\nand current road conditions. Follow safe driving practices and traﬃc regulations, and note that cycling \\nand walking directions may not include designated pathways.\\nH. To the extent that you transmit any content through the use of the Apple Software and Services, you \\nrepresent that you own all rights in, or have authorization or are otherwise legally permitted to transmit, \\nsuch content and that such content does not violate any terms of service applicable to the Apple \\nSoftware and Services. You agree that the Apple Software and Services contain proprietary content, \\ninformation and material that is owned by Apple, the site owner or their licensors, and is protected by \\napplicable intellectual property and other laws, including but not limited to copyright. You agree that you \\nwill not use such proprietary content, information or materials other than for permitted use of the Apple \\nSoftware and Services or in any manner that is inconsistent with the terms of this License or that \\ninfringes any intellectual property rights of a third party or Apple. No portion of the Apple Software and \\nServices may be reproduced in any form or by any means. You agree not to modify, rent, lease, loan, \\nsell, distribute, or create derivative works based on the Apple Software and Services, in any manner, and \\nyou shall not exploit the Apple Software and Services in any unauthorized way whatsoever, including but \\nnot limited to, using the Apple Software and Services to transmit any computer viruses, worms, trojan \\nhorses or other malware, or by trespass or burdening network capacity. You further agree not to use the \\nApple Software and Services in any manner to spam, defraud, harass, abuse, stalk, threaten, defame or \\notherwise infringe or violate the rights of any other party, and that Apple is not in any way responsible for \\nany such use by you, nor for any harassing, threatening, defamatory, oﬀensive, infringing or illegal \\nmessages or transmissions that you may receive as a result of using any of the Apple Software and \\nServices.\\nI. Certain Apple Software, Services, and Third Party Materials may not be available in all languages or in \\nall countries or regions. Apple makes no representation that such Apple Software, Services, and Third \\nParty Materials are appropriate or available for use in any particular location. To the extent you choose \\nto use or access such Apple Software, Services, or Third Party Materials, you do so at your own initiative \\nand are responsible for compliance with any applicable laws, including but not limited to applicable local \\nlaws and privacy and data collection laws. Apple and its licensors reserve the right to change, suspend, \\nremove, or disable access to any Apple Software and Services at any time without notice. In no event \\nwill Apple be liable for the removal of or disabling of access to any such Apple Software and Services. \\nApple may also impose limits on the use of or access to certain Apple Software and Services, in any \\ncase and without notice or liability.'),\n",
       " Document(metadata={'source': '/Apple_MacOS_English.pdf'}, page_content='7. Termination. This License is eﬀective until terminated. Your rights under this License will terminate \\nautomatically or otherwise cease to be eﬀective without notice from Apple if you fail to comply with any \\nterm(s) of this License. Upon the termination of this License, you shall cease all use of the Apple \\nSoftware and destroy all copies, full or partial, of the Apple Software. Sections 5, 6, 7, 8, 9, 11, 12 and \\n13 of this License shall survive any such termination.\\n8. Disclaimer of Warranties.\\nA. If you are a customer who is a consumer (someone who uses the Apple Software outside of your \\ntrade, business or profession), you may have legal rights in your country of residence which would \\nprohibit the following limitations from applying to you, and where prohibited they will not apply to you. \\nTo ﬁnd out more about rights, you should contact a local consumer advice organization. For consumers \\nin Australia, nothing in this License aﬀects, or is intended to aﬀect, your statutory rights under the \\nAustralian Consumer Law (including the consumer guarantees).\\nB. YOU EXPRESSLY ACKNOWLEDGE AND AGREE THAT, TO THE EXTENT PERMITTED BY \\nAPPLICABLE LAW, USE OF THE APPLE SOFTWARE AND ANY SERVICES PERFORMED BY OR \\nACCESSED THROUGH THE APPLE SOFTWARE IS AT YOUR SOLE RISK AND THAT THE ENTIRE RISK \\nAS TO SATISFACTORY QUALITY, PERFORMANCE, ACCURACY AND EFFORT IS WITH YOU.\\nC. TO THE MAXIMUM EXTENT PERMITTED BY APPLICABLE LAW, THE APPLE SOFTWARE AND \\nSERVICES ARE PROVIDED “AS IS” AND “AS AVAILABLE”, WITH ALL FAULTS AND WITHOUT \\nWARRANTY OF ANY KIND, AND APPLE AND APPLE’S LICENSORS (COLLECTIVELY REFERRED TO \\nAS “APPLE” FOR THE PURPOSES OF SECTIONS 8 AND 9) HEREBY DISCLAIM ALL WARRANTIES \\nAND CONDITIONS WITH RESPECT TO THE APPLE SOFTWARE AND SERVICES, EITHER EXPRESS, \\nIMPLIED OR STATUTORY, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES AND/OR \\nCONDITIONS OF MERCHANTABILITY, SATISFACTORY QUALITY, FITNESS FOR A PARTICULAR \\nPURPOSE, ACCURACY, QUIET ENJOYMENT, AND NON-INFRINGEMENT OF THIRD PARTY RIGHTS.\\nD. APPLE DOES NOT WARRANT AGAINST INTERFERENCE WITH YOUR ENJOYMENT OF THE APPLE \\nSOFTWARE AND SERVICES, THAT THE FUNCTIONS CONTAINED IN, OR SERVICES PERFORMED OR \\nPROVIDED BY, THE APPLE SOFTWARE WILL MEET YOUR REQUIREMENTS, THAT THE OPERATION \\nOF THE APPLE SOFTWARE AND SERVICES WILL BE UNINTERRUPTED OR ERROR-FREE, THAT ANY \\nSERVICES WILL CONTINUE TO BE MADE AVAILABLE, THAT DEFECTS IN THE APPLE SOFTWARE OR \\nSERVICES WILL BE CORRECTED, OR THAT THE APPLE SOFTWARE OR SERVICES WILL BE \\nCOMPATIBLE OR WORK WITH ANY THIRD PARTY SOFTWARE, APPLICATIONS OR THIRD PARTY \\nSERVICES. INSTALLATION OF THIS APPLE SOFTWARE MAY AFFECT THE AVAILABILITY AND \\nUSABILITY OF THIRD PARTY SOFTWARE, APPLICATIONS OR THIRD PARTY SERVICES, AS WELL AS \\nAPPLE PRODUCTS AND SERVICES.\\nE. YOU FURTHER ACKNOWLEDGE THAT THE APPLE SOFTWARE AND SERVICES ARE NOT \\nINTENDED OR SUITABLE FOR USE IN SITUATIONS OR ENVIRONMENTS WHERE THE FAILURE OR \\nTIME DELAYS OF, OR ERRORS OR INACCURACIES IN THE CONTENT, DATA OR INFORMATION \\nPROVIDED BY THE APPLE SOFTWARE OR SERVICES COULD LEAD TO DEATH, PERSONAL INJURY, \\nOR SEVERE PHYSICAL OR ENVIRONMENTAL DAMAGE, INCLUDING WITHOUT LIMITATION THE \\nOPERATION OF NUCLEAR FACILITIES, AIRCRAFT NAVIGATION OR COMMUNICATION SYSTEMS, \\nAIR TRAFFIC CONTROL, LIFE SUPPORT OR WEAPONS SYSTEMS.\\nF. NO ORAL OR WRITTEN INFORMATION OR ADVICE GIVEN BY APPLE OR AN APPLE AUTHORIZED \\nREPRESENTATIVE SHALL CREATE A WARRANTY. SHOULD THE APPLE SOFTWARE OR SERVICES \\nPROVE DEFECTIVE, YOU ASSUME THE ENTIRE COST OF ALL NECESSARY SERVICING, REPAIR OR \\nCORRECTION.  SOME JURISDICTIONS DO NOT ALLOW THE EXCLUSION OF IMPLIED WARRANTIES \\nOR LIMITATIONS ON APPLICABLE STATUTORY RIGHTS OF A CONSUMER, SO THE ABOVE '),\n",
       " Document(metadata={'source': '/Apple_MacOS_English.pdf'}, page_content='EXCLUSION AND LIMITATIONS MAY NOT APPLY TO YOU.\\n9. Limitation of Liability. EXCEPT AS OTHERWISE PROVIDED IN THIS LICENSE, TO THE EXTENT \\nNOT PROHIBITED BY APPLICABLE LAW, IN NO EVENT SHALL APPLE, ITS AFFILIATES, AGENTS OR \\nPRINCIPALS BE LIABLE FOR PERSONAL INJURY, OR ANY INCIDENTAL, SPECIAL, INDIRECT OR \\nCONSEQUENTIAL DAMAGES WHATSOEVER, INCLUDING, WITHOUT LIMITATION, DAMAGES FOR \\nLOSS OF PROFITS, CORRUPTION OR LOSS OF DATA, FAILURE TO TRANSMIT OR RECEIVE ANY \\nDATA OR INFORMATION (INCLUDING WITHOUT LIMITATION COURSE INSTRUCTIONS, \\nASSIGNMENTS AND MATERIALS), BUSINESS INTERRUPTION OR ANY OTHER COMMERCIAL \\nDAMAGES OR LOSSES, ARISING OUT OF OR RELATED TO YOUR USE OR INABILITY TO USE THE \\nAPPLE SOFTWARE OR SERVICES OR ANY THIRD PARTY SOFTWARE, APPLICATIONS OR SERVICES \\nIN CONJUNCTION WITH THE APPLE SOFTWARE OR SERVICES, HOWEVER CAUSED, REGARDLESS \\nOF THE THEORY OF LIABILITY (CONTRACT, TORT OR OTHERWISE) AND EVEN IF APPLE HAS BEEN \\nADVISED OF THE POSSIBILITY OF SUCH DAMAGES. SOME JURISDICTIONS DO NOT ALLOW THE \\nEXCLUSION OR LIMITATION OF LIABILITY FOR PERSONAL INJURY, OR OF INCIDENTAL OR \\nCONSEQUENTIAL DAMAGES, SO THIS LIMITATION MAY NOT APPLY TO YOU. In no event shall \\nApple’s total liability to you for all damages (other than as may be required by applicable law in cases \\ninvolving personal injury) exceed the amount of two hundred and ﬁfty dollars (U.S.$250.00).  The \\nforegoing limitations will apply even if the above stated remedy fails of its essential purpose. \\nNOTWITHSTANDING ANY OTHER TERMS IN THIS LICENSE, IF THE CONSUMER CONTRACT ACT OF \\nJAPAN APPLIES, TERMS WHICH LIMIT APPLE’S LIABILITY FOR DAMAGES ARISING FROM BREACH \\nOF THE CONTRACT OR TORT COMMITTED BY APPLE SHALL NOT APPLY IF SUCH DAMAGE IS DUE \\nTO APPLE’S INTENTIONAL MISCONDUCT OR GROSS NEGLIGENCE.\\n10. Export Control. You may not use or otherwise export or reexport the Apple Software except as \\nauthorized by United States law and the laws of the jurisdiction(s) in which the Apple Software was \\nobtained. In particular, but without limitation, the Apple Software may not be exported or re-exported (a) \\ninto any U.S. embargoed countries or (b) to anyone on the U.S. Treasury Department’s list of Specially \\nDesignated Nationals or the U.S. Department of Commerce Denied Person’s List or Entity List or any \\nother restricted party lists. By using the Apple Software, you represent and warrant that you are not \\nlocated in any such country or on any such list. You also agree that you will not use the Apple Software \\nfor any purposes prohibited by United States law, including, without limitation, the development, design, \\nmanufacture or production of missiles, nuclear, chemical or biological weapons.\\n11. Government End Users. The Apple Software and related documentation are “Commercial \\nProducts”, as that term is deﬁned at 48 C.F.R. §2.101, consisting of “Commercial Computer Software” \\nand “Commercial Computer Software Documentation”, as such terms are used in 48 C.F.R. §12.212 or \\n48 C.F.R. §227.7202, as applicable. Consistent with 48 C.F.R. §12.212 or 48 C.F.R. §227.7202-1 \\nthrough 227.7202-4, as applicable, the Commercial Computer Software and Commercial Computer \\nSoftware Documentation are being licensed to U.S. Government end users (a) only as Commercial \\nProducts and (b) with only those rights as are granted to all other end users pursuant to the terms and \\nconditions herein.  Unpublished-rights reserved under the copyright laws of the United States.\\n12. Controlling Law and Severability. This License will be governed by and construed in accordance \\nwith the laws of the State of California, excluding its conﬂict of law principles. This License shall not be \\ngoverned by the United Nations Convention on Contracts for the International Sale of Goods, the \\napplication of which is expressly excluded. If you are a consumer based in the United Kingdom, this \\nLicense will be governed by the laws of the jurisdiction of your residence. If for any reason a court of \\ncompetent jurisdiction ﬁnds any provision, or portion thereof, to be unenforceable, the remainder of this \\nLicense shall continue in full force and eﬀect.\\n13. Complete Agreement; Governing Language. This License constitutes the entire agreement '),\n",
       " Document(metadata={'source': '/Apple_MacOS_English.pdf'}, page_content='between you and Apple relating to the Apple Software and supersedes all prior or contemporaneous \\nunderstandings regarding such subject matter.  No amendment to or modiﬁcation of this License will be \\nbinding unless in writing and signed by Apple. Any translation of this License is done for local \\nrequirements and in the event of a dispute between the English and any non-English versions, the \\nEnglish version of this License shall govern, to the extent not prohibited by local law in your jurisdiction.\\n14. Third Party Acknowledgements.\\nA. Portions of the Apple Software may utilize or include third party software and other copyrighted \\nmaterial. Acknowledgements, licensing terms and disclaimers for such material are contained in the \\nelectronic documentation for the Apple Software, and your use of such material is governed by their \\nrespective terms. Use of the Google Safe Browsing Service is subject to the Google Terms of Service \\n(https://www.google.com/intl/en/policies/terms/) and to Google’s Privacy Policy (https://\\nwww.google.com/intl/en/policies/privacy/).\\nB. Certain software libraries and other third party software included with the Apple Software are free \\nsoftware and licensed under the terms of the GNU General Public License (GPL) or the GNU Library/\\nLesser General Public License (LGPL), as the case may be. You may obtain a complete machine-\\nreadable copy of the source code for such free software under the terms of the GPL or LGPL, as the \\ncase may be, without charge except for the cost of media, shipping, and handling, upon written request \\nto Apple at opensource@apple.com. The GPL/LGPL software is distributed in the hope that it will be \\nuseful, but WITHOUT ANY WARRANTY, without even the implied warranty of MERCHANTABILITY or \\nFITNESS FOR A PARTICULAR PURPOSE.  A copy of the GPL and LGPL is included with the Apple \\nSoftware.\\nC. Use of MPEG-4. This product is licensed under the MPEG-4 Systems Patent Portfolio License for \\nencoding in compliance with the MPEG-4 Systems Standard, except that an additional license and \\npayment of royalties are necessary for encoding in connection with (i) data stored or replicated in \\nphysical media which is paid for on a title by title basis and/or (ii) data which is paid for on a title by title \\nbasis and is transmitted to an end user for permanent storage and/or use. Such additional license may \\nbe obtained from MPEG LA, LLC. See https://www.mpegla.com for additional details.\\nThis product is licensed under the MPEG-4 Visual Patent Portfolio License for the personal and non-\\ncommercial use of a consumer for (i) encoding video in compliance with the MPEG-4 Visual Standard \\n(“MPEG-4 Video”) and/or (ii) decoding MPEG-4 video that was encoded by a consumer engaged in a \\npersonal and non-commercial activity and/or was obtained from a video provider licensed by MPEG LA \\nto provide MPEG-4 video. No license is granted or shall be implied for any other use. Additional \\ninformation including that relating to promotional, internal and commercial uses and licensing may be \\nobtained from MPEG LA, LLC. See https://www.mpegla.com.\\nD. H.264/AVC Notice.  To the extent that the Apple Software contains AVC encoding and/or decoding \\nfunctionality, commercial use of H.264/AVC requires additional licensing and the following provision \\napplies: THE AVC FUNCTIONALITY IN THIS PRODUCT IS LICENSED HEREIN ONLY FOR THE \\nPERSONAL AND NON-COMMERCIAL USE OF A CONSUMER TO (i) ENCODE VIDEO IN COMPLIANCE \\nWITH THE AVC STANDARD (“AVC VIDEO”) AND/OR (ii) DECODE AVC VIDEO THAT WAS ENCODED BY \\nA CONSUMER ENGAGED IN A PERSONAL AND NON-COMMERCIAL ACTIVITY AND/OR AVC VIDEO \\nTHAT WAS OBTAINED FROM A VIDEO PROVIDER LICENSED TO PROVIDE AVC VIDEO. \\nINFORMATION REGARDING OTHER USES AND LICENSES MAY BE OBTAINED FROM MPEG LA \\nL.L.C. SEE HTTPS://WWW.MPEGLA.COM.\\nE. AMR Notice. The Adaptive Multi-Rate (“AMR”) encoding and decoding functionality in this product is \\nnot licensed to perform cellular voice calls, or for use in any telephony products built on the QuickTime \\narchitecture for the Windows platform. The AMR encoding and decoding functionality in this product is '),\n",
       " Document(metadata={'source': '/Apple_MacOS_English.pdf'}, page_content='also not licensed for use in a cellular communications infrastructure including:  base stations, base \\nstation controllers/radio network controllers, switching centers, and gateways to and from the public \\nswitched network.\\nF. FAA Notice. Aircraft Situation Display and National Airspace System Status Information data \\n(collectively “Flight Data”) displayed through the Apple Software is generated by the Federal Aviation \\nAdministration. You agree not to redistribute Flight Data without the prior written consent of the FAA. \\nThe FAA and Apple disclaim all warranties, expressed or implied (including the implied warranties of \\nmerchantability and ﬁtness for a particular purpose), regarding the use and accuracy of the Flight Data. \\nYou agree that the FAA and Apple shall not be liable, either collectively or individually, for any loss, \\ndamage, claim, liability, expense, or penalty, or for any indirect, special, secondary, incidental, or \\nconsequential damages deriving from the use of the Flight Data. The Apple Software is not sponsored or \\nendorsed by the FAA. The FAA is not responsible for technical or system problems, and you should not \\ncontact the FAA regarding such problems or regarding operational traﬃc ﬂow issues.\\nG. Use of Adobe Color Proﬁles. You may use the Adobe Color Proﬁle software included with the Apple \\nSoftware pursuant to this License, but Adobe is under no obligation to provide any support for the Color \\nProﬁles hereunder, including upgrades or future versions of the Proﬁles or other items. In addition to the \\nprovisions of Sections 8 and 9 above, IN NO EVENT WILL ADOBE BE LIABLE TO YOU FOR ANY \\nDAMAGES, CLAIMS OR COSTS WHATSOEVER.  The Adobe Color Proﬁle software distributed with the \\nApple Software is also available for download from Adobe at https://www.adobe.com.\\nH. Gracenote® End User License Agreement. This application or device contains software from \\nGracenote, Inc. of Emeryville, California (“Gracenote”).\\xa0 The software from Gracenote (the “Gracenote \\nSoftware”) enables this application to perform disc and/or ﬁle identiﬁcation and obtain music-related \\ninformation, including name, artist, track, and title information (“Gracenote Data”) from online servers or \\nembedded databases (collectively, “Gracenote Servers”) and to perform other functions. You may use \\nGracenote Data only by means of the intended End-User functions of this application or device.\\xa0\\nYou agree that you will use Gracenote Data, the Gracenote Software, and Gracenote Servers for your \\nown personal non-commercial use only.\\xa0 You agree not to assign, copy, transfer or transmit the \\nGracenote Software or any Gracenote Data to any third party. YOU AGREE NOT TO USE OR EXPLOIT \\nGRACENOTE DATA, THE GRACENOTE SOFTWARE, OR GRACENOTE SERVERS, EXCEPT AS \\nEXPRESSLY PERMITTED HEREIN.\\xa0\\nYou agree that your non-exclusive license to use the Gracenote Data, the Gracenote Software, and \\nGracenote Servers will terminate if you violate these restrictions.\\xa0 If your license terminates, you agree to \\ncease any and all use of the Gracenote Data, the Gracenote Software, and Gracenote Servers. \\nGracenote reserves all rights in Gracenote Data, the Gracenote Software, and the Gracenote Servers, \\nincluding all ownership rights.\\xa0 Under no circumstances will Gracenote become liable for any payment to \\nyou for any information that you provide.\\xa0 You agree that Gracenote, Inc. may enforce its rights under \\nthis Agreement against you directly in its own name.\\xa0\\nThe Gracenote service uses a unique identiﬁer to track queries for statistical purposes.\\xa0 The purpose of a \\nrandomly assigned numeric identiﬁer is to allow the Gracenote service to count queries without knowing \\nanything about who you are.\\xa0 For more information, see the web page for the Gracenote Privacy Policy \\nfor the Gracenote service.\\xa0\\nThe Gracenote Software and each item of Gracenote Data are licensed to you “AS IS.” Gracenote \\nmakes no representations or warranties, express or implied, regarding the accuracy of any Gracenote \\nData from in the Gracenote Servers.\\xa0 Gracenote reserves the right to delete data from the Gracenote \\nServers or to change data categories for any cause that Gracenote deems suﬃcient.\\xa0 No warranty is '),\n",
       " Document(metadata={'source': '/Apple_MacOS_English.pdf'}, page_content='made that the Gracenote Software or Gracenote Servers are error-free or that functioning of Gracenote \\nSoftware or Gracenote Servers will be uninterrupted. Gracenote is not obligated to provide you with new \\nenhanced or additional data types or categories that Gracenote may provide in the future and is free to \\ndiscontinue its services at any time. GRACENOTE DISCLAIMS ALL WARRANTIES EXPRESS OR \\nIMPLIED, INCLUDING, BUT NOT LIMITED TO, IMPLIED WARRANTIES OF MERCHANTABILITY, \\nFITNESS FOR A PARTICULAR PURPOSE, TITLE, AND NON-INFRINGEMENT.\\xa0\\n GRACENOTE DOES \\nNOT WARRANT THE RESULTS THAT WILL BE OBTAINED BY YOUR USE OF THE GRACENOTE \\nSOFTWARE OR ANY GRACENOTE SERVER. IN NO CASE WILL GRACENOTE BE LIABLE FOR ANY \\nCONSEQUENTIAL OR INCIDENTAL DAMAGES OR FOR ANY LOST PROFITS OR LOST REVENUES.\\xa0\\n15. Yahoo Search Service Restrictions. The Yahoo Search Service available through Safari is licensed \\nfor use only in the following countries and regions: Argentina, Aruba, Australia, Austria, Barbados, \\nBelgium, Bermuda, Brazil, Bulgaria, Canada, Cayman Islands, Chile, China mainland, Hong Kong, \\nTaiwan, Colombia, Cyprus, Czech Republic, Denmark, Dominican Republic, Ecuador, El Salvador, \\nFinland, France, Germany, Greece, Grenada, Guatemala, Hungary, Iceland, India, Indonesia, Ireland, \\nItaly, Jamaica, Japan, Latvia, Lithuania, Luxembourg, Malaysia, Malta, Mexico, Netherlands, New \\nZealand, Nicaragua, Norway, Panama, Peru, Philippines, Poland, Portugal, Puerto Rico, Romania, \\nSingapore, Slovakia, Slovenia, South Korea, Spain, St. Lucia, St. Vincent, Sweden, Switzerland, \\nThailand, The Bahamas, Trinidad and Tobago, Turkey, UK, Uruguay, US and Venezuela.\\nEA1885\\n07/11/2024\\n————————————\\nApple Pay & Wallet Terms and Conditions\\nThese Apple Pay & Wallet Terms and Conditions (the “Apple Pay & Wallet Terms”) supplement the Apple \\nSoftware License Agreement for macOS (the “License”); both the terms of the License and these Apple \\nPay & Wallet Terms govern your use of the Apple Pay feature (“Apple Pay”) and Apple Wallet (“Wallet”), \\nwhich shall be deemed a “Service” under the License.\\xa0 Capitalized terms used in these Apple Pay & \\nWallet Terms have the meanings set forth in the License. Your acceptance of these Apple Pay & Wallet \\nTerms constitutes your acceptance of the respective Apple Pay & Wallet Terms on all of your Apple \\ndevices that support Apple Pay and Wallet.\\nApple Pay and support of your Apple Pay Cards (as deﬁned below) in Apple Wallet (“Wallet”) are \\nprovided in the United States by Apple Payments Services LLC (“Apple Payments Services”) and outside \\nof the United States by other Apple aﬃ\\nliates. Wallet is otherwise provided by Apple and other Apple \\naﬃliates. Apple, Apple Payments Services, and the respective Apple aﬃliates are each a party to these \\nApple Pay Terms with respect to the Services it provides.\\n1. Overview\\nApple Pay\\nApple Pay allows you to make contactless payments using credit, debit and prepaid cards, as well as \\nthird party installment payment services, within select websites.\\xa0 Apple Pay and certain features of Apple \\nPay may only be available in select regions, with select card issuers, payment networks, merchants, and \\nother third parties.\\nWallet\\nWallet allows you to store virtual representations of credit, debit, and prepaid cards to be used with \\nApple Pay (collectively, “Supported Cards”). Supported Cards may only be available in select regions '),\n",
       " Document(metadata={'source': '/Apple_MacOS_English.pdf'}, page_content='and with select partners and Supported Cards may change from time to time.\\n2. Eligibility\\nTo set up and use Apple Pay and Wallet, you must have (i) a supported Mac Computer running a version \\nof operating software that supports the Services (latest version recommended and sometimes required), \\n(ii) an Apple Account associated with an iCloud account that is in good standing with Apple, and (iii) \\nInternet access or cellular data access (fees may apply). Subject to certain exceptions, Supported Cards \\nare only available to individuals aged 13 years or older, and may be subject to additional age-based \\nrestrictions imposed by iCloud or the relevant issuer, merchant, or other third party.\\n3. Use of the Services\\nApple Pay is intended for your personal use and you may only provision your own Supported Cards.\\xa0 If \\nyou are provisioning a supported corporate card, you represent that you are doing so with the \\nauthorization of your employer and you are authorized to bind your employer to these Apple Pay & \\nWallet Terms and all transactions eﬀected by use of Apple Pay and Wallet. \\nYou agree not to use Apple Pay for illegal or fraudulent purposes, or any other purposes that are \\nprohibited by the License and these Apple Pay Terms.\\xa0 You further agree to use Apple Pay in \\naccordance with applicable laws and regulations.\\xa0 You agree not to interfere with or disrupt Apple Pay \\n(including accessing the Service through any automated means), or any servers or networks connected \\nto the Service, or any policies, requirements or regulations of networks connected to the Service \\n(including any unauthorized access to, use or monitoring of data or traﬃc thereon).\\nYou can use Apple Pay on your supported Mac Computer to initiate payments on websites.\\xa0You can \\ncomplete your purchase by authorizing the transaction using your Mac with Touch ID or password, \\nsupported iPhone, or supported Apple Watch. If you complete your purchase on your Mac Computer, \\nyou must have a Supported Card that is associated with an active iCloud account. If you complete your \\npurchase using iPhone or Apple Watch (collectively, the “Supported Devices”), you must be signed in \\nwith the same Apple Account as on your Mac Computer. \\nWhen using Apple Pay on a desktop web browser other than Safari, you must have a Supported Card \\nthat has been provisioned to a supported iPhone or iPad. In addition to the terms set forth in these \\nApple Pay Terms, your use of Apple Pay to initiate payments on websites is subject to the Apple Pay & \\nWallet Terms and Conditions applicable to your Supported Devices, the terms of which are hereby \\nincorporated by reference and which can be accessed by going to:  Settings > General > About > Legal \\n> License from your iPhone, or About > Legal > License from the Watch app on a paired iPhone.  \\nSupported Devices may change from time to time.\\n4. Apple’s Relationship With You\\nFor purposes of this Section 3 only, “Apple” refers to, collectively, Apple Inc., Apple Payments Services \\nLLC, and the respective Apple aﬃliates. Your use of Apple Pay will be governed by these Apple Pay \\nTerms, as well as by the terms of the cardholder agreement you have in place with the relevant issuer, \\nmerchant, or other third party responsible for your Supported Card.\\nApple is not a ﬁnancial institution. Supported Cards are not issued or serviced by Apple, and Apple does \\nnot process payments, or have any other control over payments, chargebacks, returns, refunds, \\nrewards, value, discounts, orders, order fulﬁlment, or other commerce activity that may arise out of your \\nuse of Apple Pay or Wallet.\\xa0 \\nInstallment payments on Apple Pay are supported and managed by third party partners. If you choose to '),\n",
       " Document(metadata={'source': '/Apple_MacOS_English.pdf'}, page_content='pay in installments or to purchase and pay later with a third party when using Apple Pay, you are \\ncontracting with the third party and eligibility determinations and repayment of your purchase obligation \\nare governed by that third party’s terms of service, which are presented to you during checkout. Apple is \\nnot a party to any agreement between you and that third party, and expressly disclaims all liability with \\nrespect to such installment payment services and agreements.\\nThe terms of cardholder agreements you may have in place with your card issuer will continue to govern \\nyour use of your Supported Cards and their use in connection with Apple Pay.  Similarly, your purchase \\nof any goods or services using the Apple Pay feature will be subject to the merchant’s terms and \\nconditions.  \\nNothing in the License or these Apple Pay & Wallet Terms modiﬁes the terms of any cardholder or \\nmerchant agreement, or other terms and conditions applicable to the use of the features of Apple Pay, \\nand such terms will govern your use of the applicable feature of Apple Pay and their virtual \\nrepresentation on your supported Mac Computer.\\nYou agree that Apple is not a party to your cardholder or merchant agreements, nor is Apple responsible \\nfor the (a) content, accuracy or availability of any Supported Cards, commerce activities, transactions, \\npurchases, orders, order fulﬁlment, or receipts while using Apple Pay or Wallet; (b) issuance of credit or \\nassessing eligibility for credit; (c) activities of issuers, merchants, or other third parties related to your use \\nof Apple Pay or Wallet; (d) decisions made by an issuer, merchant, or other third party in connection with \\na Supported Card; (e) accrual or redemption of rewards or stored value in connection with your \\nSupported Cards; or (f) funding or reloading of prepaid Supported Cards.\\xa0 For all disputes or questions \\nabout Supported Cards or associated commerce activity, please contact your issuer or the applicable \\nmerchant. For questions regarding Apple Pay or Wallet, please contact (800) APL-CARE (800-275-2273).\\n5. Privacy\\nWhen using Apple Pay to initiate a payment transaction to be completed on a Supported Device, Apple \\nPay will transfer payment information in an encrypted format between your Mac Computer and your \\nSupported Device to complete your transaction. When using Apple Pay to make a payment transaction \\non a MacBook Pro with built-in Touch ID, your payment information will be provided in encrypted format \\nto the website as part of that transaction. When adding a card to Apple Pay on a MacBook Pro with \\nbuilt-in Touch ID, information about your device, such as whether certain device settings are enabled \\nand device use patterns (e.g. percent time device is in motion, approximate number of calls per week), \\nwill be sent to Apple to determine your eligibility and to prevent fraud. You can ﬁnd detailed information \\non the personal information collected, used or shared as part of your use of Apple Pay by reading the \\nrelevant service speciﬁc privacy notices, including About Apple Pay and Privacy, which can be accessed \\non your iOS device or Mac Computer, or within the Watch app on a paired iOS device, or by visiting \\nhttps://www.apple.com/legal/privacy.  By using Apple Pay and Wallet, you agree and consent to Apple’s \\nand its aﬃliates’ and agents’ transmission, collection, maintenance, processing, and use of all of the \\nforegoing information, to provide these Services.\\n6. Security\\nApple Pay and Wallet store virtual representations of your Supported Cards and should be protected as \\nyou would protect your physical wallet, or credit, debit, prepaid and other cards. You are solely \\nresponsible for maintaining the security of your Mac Computer, Supported Devices, your Apple Account, \\nyour Touch ID information, the passcode(s) to your device(s), and any other authentication credentials \\nused in connection with the Services (collectively, your “Credentials”). If you authorize or allow anyone \\nelse to use your Mac Computer or Supported Devices (e.g., by providing your device passcode to a third \\nparty or otherwise providing any of your Credentials to a third party), the person may be able to make '),\n",
       " Document(metadata={'source': '/Apple_MacOS_English.pdf'}, page_content='payments, use value or make other transactions with your Supported Cards in Wallet. In such event, you \\nwill be responsible for all payments and transactions made by that person.\\nIf you make unauthorized modiﬁcations to your Mac Computer or Supported Devices, such as by \\ndisabling hardware or software controls (sometimes referred to as “jailbreaking”), your Mac Computer \\nand/or Supported Devices may no longer be eligible to access or use the Services. You acknowledge \\nthat the use of a modiﬁed device in connection with the Services is expressly prohibited, constitutes a \\nviolation of these Apple Pay & Wallet Terms, and is grounds for us to deny or limit your access to the \\nServices.\\nIf your device is lost or stolen and you have Find My enabled, you can use Find My or https://icloud.com \\nto attempt to suspend the ability to pay with the virtual Supported Cards on your Mac Computer by \\nputting it into Lost Mode. You can also erase your device, which will attempt to suspend the ability to \\npay with the virtual Supported Cards on the device. If you have set up a recurring payment transaction \\nusing Apple Pay, you may need to contact the relevant issuer or merchant to cancel the recurring \\npayment.  You should also contact the card issuer of your Supported Cards in order to prevent \\nunauthorized access to your virtual Supported Cards.\\nIf you report or Apple or its aﬃliates suspect fraudulent or abusive activity, you agree to cooperate with \\nApple and its aﬃliates in any investigation and to use any fraud prevention measures we prescribe.\\n7. Limitation of Liability\\nIN ADDITION TO THE DISCLAIMERS OF WARRANTIES AND LIMITATION OF LIABILITY SET \\nFORTH IN THE LICENSE, APPLE, APPLE PAYMENTS SERVICES, AND THE RESPECTIVE APPLE \\nAFFILIATES DO NOT ASSUME ANY LIABILITY FOR PURCHASES, PAYMENTS, TRANSACTIONS, \\nORDERS, ORDER FULFILMENT, RECEIPT INFORMATION, OR OTHER COMMERCE ACTIVITY \\nMADE USING APPLE PAY OR WALLET, AND YOU AGREE TO LOOK SOLELY TO AGREEMENTS \\nYOU MAY HAVE WITH YOUR CARD ISSUER, PAYMENT NETWORK, OR MERCHANT TO RESOLVE \\nANY QUESTIONS OR DISPUTES RELATING TO YOUR SUPPORTED CARDS AND ASSOCIATED \\nCOMMERCE ACTIVITIES.\\n———————————————\\nNOTICES FROM APPLE, APPLE PAYMENTS SERVICES, AND THE RESPECTIVE APPLE \\nAFFILIATES \\nIf Apple, Apple Payments Services, or an Apple aﬃliate needs to contact you about the Services, you \\nconsent to receive the notices by email. You agree that any such notices that we send you electronically \\nwill satisfy any legal communication requirements.')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12a944c3-8050-4c25-9ab1-37aa14ac8311",
   "metadata": {},
   "source": [
    "# Perform Article Summaries as Relationship Extraction Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2528c928-2f38-42ed-a509-1ed4109885bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l7/wzp39ssn7gggk5cszv0vycdc0000gn/T/ipykernel_98203/4268091352.py:22: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
      "  map_chain = LLMChain(llm=llm, prompt=map_prompt)\n",
      "/var/folders/l7/wzp39ssn7gggk5cszv0vycdc0000gn/T/ipykernel_98203/4268091352.py:40: LangChainDeprecationWarning: This class is deprecated. Use the `create_stuff_documents_chain` constructor instead. See migration guide here: https://python.langchain.com/docs/versions/migrating_chains/stuff_docs_chain/\n",
      "  combine_documents_chain = StuffDocumentsChain(\n",
      "/var/folders/l7/wzp39ssn7gggk5cszv0vycdc0000gn/T/ipykernel_98203/4268091352.py:46: LangChainDeprecationWarning: This class is deprecated. Please see the migration guide here for a recommended replacement: https://python.langchain.com/docs/versions/migrating_chains/map_reduce_chain/\n",
      "  reduce_documents_chain = ReduceDocumentsChain(\n",
      "/var/folders/l7/wzp39ssn7gggk5cszv0vycdc0000gn/T/ipykernel_98203/4268091352.py:56: LangChainDeprecationWarning: This class is deprecated. Please see the migration guide here for a recommended replacement: https://python.langchain.com/docs/versions/migrating_chains/map_reduce_chain/\n",
      "  map_reduce_chain = MapReduceDocumentsChain(\n",
      "/var/folders/l7/wzp39ssn7gggk5cszv0vycdc0000gn/T/ipykernel_98203/4268091352.py:73: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  summarization_results = map_reduce_chain.run(split_docs)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 73\u001b[0m\n\u001b[1;32m     70\u001b[0m split_docs \u001b[38;5;241m=\u001b[39m text_splitter\u001b[38;5;241m.\u001b[39msplit_documents(all_data)\n\u001b[1;32m     72\u001b[0m \u001b[38;5;66;03m# Run the MapReduce Chain\u001b[39;00m\n\u001b[0;32m---> 73\u001b[0m summarization_results \u001b[38;5;241m=\u001b[39m \u001b[43mmap_reduce_chain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_docs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/langchain_core/_api/deprecation.py:182\u001b[0m, in \u001b[0;36mdeprecated.<locals>.deprecate.<locals>.warning_emitting_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    180\u001b[0m     warned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    181\u001b[0m     emit_warning()\n\u001b[0;32m--> 182\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/langchain/chains/base.py:606\u001b[0m, in \u001b[0;36mChain.run\u001b[0;34m(self, callbacks, tags, metadata, *args, **kwargs)\u001b[0m\n\u001b[1;32m    604\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    605\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`run` supports only one positional argument.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 606\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m)\u001b[49m[\n\u001b[1;32m    607\u001b[0m         _output_key\n\u001b[1;32m    608\u001b[0m     ]\n\u001b[1;32m    610\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m args:\n\u001b[1;32m    611\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m(kwargs, callbacks\u001b[38;5;241m=\u001b[39mcallbacks, tags\u001b[38;5;241m=\u001b[39mtags, metadata\u001b[38;5;241m=\u001b[39mmetadata)[\n\u001b[1;32m    612\u001b[0m         _output_key\n\u001b[1;32m    613\u001b[0m     ]\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/langchain_core/_api/deprecation.py:182\u001b[0m, in \u001b[0;36mdeprecated.<locals>.deprecate.<locals>.warning_emitting_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    180\u001b[0m     warned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    181\u001b[0m     emit_warning()\n\u001b[0;32m--> 182\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/langchain/chains/base.py:389\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[1;32m    357\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Execute the chain.\u001b[39;00m\n\u001b[1;32m    358\u001b[0m \n\u001b[1;32m    359\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    380\u001b[0m \u001b[38;5;124;03m        `Chain.output_keys`.\u001b[39;00m\n\u001b[1;32m    381\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    382\u001b[0m config \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    383\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m: callbacks,\n\u001b[1;32m    384\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtags\u001b[39m\u001b[38;5;124m\"\u001b[39m: tags,\n\u001b[1;32m    385\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m: metadata,\n\u001b[1;32m    386\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: run_name,\n\u001b[1;32m    387\u001b[0m }\n\u001b[0;32m--> 389\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    390\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    391\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mRunnableConfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43mk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    392\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_only_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_only_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    393\u001b[0m \u001b[43m    \u001b[49m\u001b[43minclude_run_info\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minclude_run_info\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    394\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/langchain/chains/base.py:170\u001b[0m, in \u001b[0;36mChain.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    169\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n\u001b[0;32m--> 170\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    171\u001b[0m run_manager\u001b[38;5;241m.\u001b[39mon_chain_end(outputs)\n\u001b[1;32m    173\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m include_run_info:\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/langchain/chains/base.py:160\u001b[0m, in \u001b[0;36mChain.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    158\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_inputs(inputs)\n\u001b[1;32m    159\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 160\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    161\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    162\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(inputs)\n\u001b[1;32m    163\u001b[0m     )\n\u001b[1;32m    165\u001b[0m     final_outputs: Dict[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprep_outputs(\n\u001b[1;32m    166\u001b[0m         inputs, outputs, return_only_outputs\n\u001b[1;32m    167\u001b[0m     )\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/langchain/chains/combine_documents/base.py:138\u001b[0m, in \u001b[0;36mBaseCombineDocumentsChain._call\u001b[0;34m(self, inputs, run_manager)\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;66;03m# Other keys are assumed to be needed for LLM prediction\u001b[39;00m\n\u001b[1;32m    137\u001b[0m other_keys \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m inputs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_key}\n\u001b[0;32m--> 138\u001b[0m output, extra_return_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcombine_docs\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    139\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdocs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_run_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_child\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mother_keys\u001b[49m\n\u001b[1;32m    140\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    141\u001b[0m extra_return_dict[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_key] \u001b[38;5;241m=\u001b[39m output\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m extra_return_dict\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/langchain/chains/combine_documents/map_reduce.py:240\u001b[0m, in \u001b[0;36mMapReduceDocumentsChain.combine_docs\u001b[0;34m(self, docs, token_max, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcombine_docs\u001b[39m(\n\u001b[1;32m    229\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    230\u001b[0m     docs: List[Document],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    233\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    234\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mdict\u001b[39m]:\n\u001b[1;32m    235\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Combine documents in a map reduce manner.\u001b[39;00m\n\u001b[1;32m    236\u001b[0m \n\u001b[1;32m    237\u001b[0m \u001b[38;5;124;03m    Combine by mapping first chain over all documents, then reducing the results.\u001b[39;00m\n\u001b[1;32m    238\u001b[0m \u001b[38;5;124;03m    This reducing can be done recursively if needed (if there are many documents).\u001b[39;00m\n\u001b[1;32m    239\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 240\u001b[0m     map_results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllm_chain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    241\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# FYI - this is parallelized and so it is fast.\u001b[39;49;00m\n\u001b[1;32m    242\u001b[0m \u001b[43m        \u001b[49m\u001b[43m[\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdocument_variable_name\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43md\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpage_content\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m}\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43md\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdocs\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    243\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    244\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    245\u001b[0m     question_result_key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm_chain\u001b[38;5;241m.\u001b[39moutput_key\n\u001b[1;32m    246\u001b[0m     result_docs \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    247\u001b[0m         Document(page_content\u001b[38;5;241m=\u001b[39mr[question_result_key], metadata\u001b[38;5;241m=\u001b[39mdocs[i]\u001b[38;5;241m.\u001b[39mmetadata)\n\u001b[1;32m    248\u001b[0m         \u001b[38;5;66;03m# This uses metadata from the docs, and the textual results from `results`\u001b[39;00m\n\u001b[1;32m    249\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m i, r \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(map_results)\n\u001b[1;32m    250\u001b[0m     ]\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/langchain/chains/llm.py:251\u001b[0m, in \u001b[0;36mLLMChain.apply\u001b[0;34m(self, input_list, callbacks)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    250\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n\u001b[0;32m--> 251\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    252\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_outputs(response)\n\u001b[1;32m    253\u001b[0m run_manager\u001b[38;5;241m.\u001b[39mon_chain_end({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutputs\u001b[39m\u001b[38;5;124m\"\u001b[39m: outputs})\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/langchain/chains/llm.py:248\u001b[0m, in \u001b[0;36mLLMChain.apply\u001b[0;34m(self, input_list, callbacks)\u001b[0m\n\u001b[1;32m    242\u001b[0m run_manager \u001b[38;5;241m=\u001b[39m callback_manager\u001b[38;5;241m.\u001b[39mon_chain_start(\n\u001b[1;32m    243\u001b[0m     \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    244\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_list\u001b[39m\u001b[38;5;124m\"\u001b[39m: input_list},\n\u001b[1;32m    245\u001b[0m     name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_name(),\n\u001b[1;32m    246\u001b[0m )\n\u001b[1;32m    247\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 248\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    250\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/langchain/chains/llm.py:138\u001b[0m, in \u001b[0;36mLLMChain.generate\u001b[0;34m(self, input_list, run_manager)\u001b[0m\n\u001b[1;32m    136\u001b[0m callbacks \u001b[38;5;241m=\u001b[39m run_manager\u001b[38;5;241m.\u001b[39mget_child() \u001b[38;5;28;01mif\u001b[39;00m run_manager \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm, BaseLanguageModel):\n\u001b[0;32m--> 138\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    139\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    140\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    142\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllm_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    143\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    145\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm\u001b[38;5;241m.\u001b[39mbind(stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm_kwargs)\u001b[38;5;241m.\u001b[39mbatch(\n\u001b[1;32m    146\u001b[0m         cast(List, prompts), {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m: callbacks}\n\u001b[1;32m    147\u001b[0m     )\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/langchain_core/language_models/chat_models.py:786\u001b[0m, in \u001b[0;36mBaseChatModel.generate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    778\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_prompt\u001b[39m(\n\u001b[1;32m    779\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    780\u001b[0m     prompts: \u001b[38;5;28mlist\u001b[39m[PromptValue],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    783\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    784\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[1;32m    785\u001b[0m     prompt_messages \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[0;32m--> 786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_messages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/langchain_core/language_models/chat_models.py:643\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    641\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n\u001b[1;32m    642\u001b[0m             run_managers[i]\u001b[38;5;241m.\u001b[39mon_llm_error(e, response\u001b[38;5;241m=\u001b[39mLLMResult(generations\u001b[38;5;241m=\u001b[39m[]))\n\u001b[0;32m--> 643\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    644\u001b[0m flattened_outputs \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    645\u001b[0m     LLMResult(generations\u001b[38;5;241m=\u001b[39m[res\u001b[38;5;241m.\u001b[39mgenerations], llm_output\u001b[38;5;241m=\u001b[39mres\u001b[38;5;241m.\u001b[39mllm_output)  \u001b[38;5;66;03m# type: ignore[list-item]\u001b[39;00m\n\u001b[1;32m    646\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results\n\u001b[1;32m    647\u001b[0m ]\n\u001b[1;32m    648\u001b[0m llm_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_combine_llm_outputs([res\u001b[38;5;241m.\u001b[39mllm_output \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results])\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/langchain_core/language_models/chat_models.py:633\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    630\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(messages):\n\u001b[1;32m    631\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    632\u001b[0m         results\u001b[38;5;241m.\u001b[39mappend(\n\u001b[0;32m--> 633\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    634\u001b[0m \u001b[43m                \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    635\u001b[0m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    636\u001b[0m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    637\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    638\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    639\u001b[0m         )\n\u001b[1;32m    640\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    641\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/langchain_core/language_models/chat_models.py:851\u001b[0m, in \u001b[0;36mBaseChatModel._generate_with_cache\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    849\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    850\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39msignature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 851\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    852\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    853\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    854\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    855\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(messages, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/langchain_community/chat_models/openai.py:476\u001b[0m, in \u001b[0;36mChatOpenAI._generate\u001b[0;34m(self, messages, stop, run_manager, stream, **kwargs)\u001b[0m\n\u001b[1;32m    470\u001b[0m message_dicts, params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_message_dicts(messages, stop)\n\u001b[1;32m    471\u001b[0m params \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    472\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams,\n\u001b[1;32m    473\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m: stream} \u001b[38;5;28;01mif\u001b[39;00m stream \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m {}),\n\u001b[1;32m    474\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    475\u001b[0m }\n\u001b[0;32m--> 476\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompletion_with_retry\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    477\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmessage_dicts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\n\u001b[1;32m    478\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    479\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_chat_result(response)\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/langchain_community/chat_models/openai.py:387\u001b[0m, in \u001b[0;36mChatOpenAI.completion_with_retry\u001b[0;34m(self, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    385\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Use tenacity to retry the completion call.\"\"\"\u001b[39;00m\n\u001b[1;32m    386\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_openai_v1():\n\u001b[0;32m--> 387\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    389\u001b[0m retry_decorator \u001b[38;5;241m=\u001b[39m _create_retry_decorator(\u001b[38;5;28mself\u001b[39m, run_manager\u001b[38;5;241m=\u001b[39mrun_manager)\n\u001b[1;32m    391\u001b[0m \u001b[38;5;129m@retry_decorator\u001b[39m\n\u001b[1;32m    392\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_completion_with_retry\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/openai/_utils/_utils.py:279\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    277\u001b[0m             msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    278\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[0;32m--> 279\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/openai/resources/chat/completions.py:859\u001b[0m, in \u001b[0;36mCompletions.create\u001b[0;34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, reasoning_effort, response_format, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    817\u001b[0m \u001b[38;5;129m@required_args\u001b[39m([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m], [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    818\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[1;32m    819\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    856\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m NOT_GIVEN,\n\u001b[1;32m    857\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatCompletion \u001b[38;5;241m|\u001b[39m Stream[ChatCompletionChunk]:\n\u001b[1;32m    858\u001b[0m     validate_response_format(response_format)\n\u001b[0;32m--> 859\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    860\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/chat/completions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    861\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    862\u001b[0m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m    863\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    864\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    865\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43maudio\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43maudio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    866\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfrequency_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    867\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunction_call\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    868\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunctions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    869\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogit_bias\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    870\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    871\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_completion_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_completion_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    872\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    873\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetadata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    874\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodalities\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodalities\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    875\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mn\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    876\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparallel_tool_calls\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    877\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprediction\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    878\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpresence_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mreasoning_effort\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mreasoning_effort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    880\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresponse_format\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    881\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseed\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    882\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mservice_tier\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_tier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    883\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstop\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    884\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstore\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    885\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    886\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    887\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtemperature\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    888\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtool_choice\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    889\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtools\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    890\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_logprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    891\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_p\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    892\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    893\u001b[0m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    894\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCompletionCreateParams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    895\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    896\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    897\u001b[0m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[1;32m    898\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    899\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    900\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    901\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    902\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/openai/_base_client.py:1283\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1269\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[1;32m   1270\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1271\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1278\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1279\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[1;32m   1280\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[1;32m   1281\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[1;32m   1282\u001b[0m     )\n\u001b[0;32m-> 1283\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/openai/_base_client.py:960\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    957\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    958\u001b[0m     retries_taken \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m--> 960\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    961\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    962\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    963\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    964\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    965\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries_taken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    966\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/openai/_base_client.py:996\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[1;32m    993\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSending HTTP Request: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, request\u001b[38;5;241m.\u001b[39mmethod, request\u001b[38;5;241m.\u001b[39murl)\n\u001b[1;32m    995\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 996\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    997\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    998\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_should_stream_response_body\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    999\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1000\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1001\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m httpx\u001b[38;5;241m.\u001b[39mTimeoutException \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m   1002\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEncountered httpx.TimeoutException\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/httpx/_client.py:914\u001b[0m, in \u001b[0;36mClient.send\u001b[0;34m(self, request, stream, auth, follow_redirects)\u001b[0m\n\u001b[1;32m    910\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_timeout(request)\n\u001b[1;32m    912\u001b[0m auth \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_request_auth(request, auth)\n\u001b[0;32m--> 914\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_handling_auth\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    915\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    916\u001b[0m \u001b[43m    \u001b[49m\u001b[43mauth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    917\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    918\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhistory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    919\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    920\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    921\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/httpx/_client.py:942\u001b[0m, in \u001b[0;36mClient._send_handling_auth\u001b[0;34m(self, request, auth, follow_redirects, history)\u001b[0m\n\u001b[1;32m    939\u001b[0m request \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(auth_flow)\n\u001b[1;32m    941\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 942\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_handling_redirects\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    943\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    944\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    945\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhistory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhistory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    946\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    947\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    948\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/httpx/_client.py:979\u001b[0m, in \u001b[0;36mClient._send_handling_redirects\u001b[0;34m(self, request, follow_redirects, history)\u001b[0m\n\u001b[1;32m    976\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_event_hooks[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequest\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m    977\u001b[0m     hook(request)\n\u001b[0;32m--> 979\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_single_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    980\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    981\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_event_hooks[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/httpx/_client.py:1014\u001b[0m, in \u001b[0;36mClient._send_single_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m   1009\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m   1010\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttempted to send an async request with a sync Client instance.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1011\u001b[0m     )\n\u001b[1;32m   1013\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request\u001b[38;5;241m=\u001b[39mrequest):\n\u001b[0;32m-> 1014\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mtransport\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1016\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response\u001b[38;5;241m.\u001b[39mstream, SyncByteStream)\n\u001b[1;32m   1018\u001b[0m response\u001b[38;5;241m.\u001b[39mrequest \u001b[38;5;241m=\u001b[39m request\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/httpx/_transports/default.py:250\u001b[0m, in \u001b[0;36mHTTPTransport.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    237\u001b[0m req \u001b[38;5;241m=\u001b[39m httpcore\u001b[38;5;241m.\u001b[39mRequest(\n\u001b[1;32m    238\u001b[0m     method\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mmethod,\n\u001b[1;32m    239\u001b[0m     url\u001b[38;5;241m=\u001b[39mhttpcore\u001b[38;5;241m.\u001b[39mURL(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    247\u001b[0m     extensions\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mextensions,\n\u001b[1;32m    248\u001b[0m )\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[0;32m--> 250\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_pool\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp\u001b[38;5;241m.\u001b[39mstream, typing\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m    254\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Response(\n\u001b[1;32m    255\u001b[0m     status_code\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mstatus,\n\u001b[1;32m    256\u001b[0m     headers\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[1;32m    257\u001b[0m     stream\u001b[38;5;241m=\u001b[39mResponseStream(resp\u001b[38;5;241m.\u001b[39mstream),\n\u001b[1;32m    258\u001b[0m     extensions\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mextensions,\n\u001b[1;32m    259\u001b[0m )\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/httpcore/_sync/connection_pool.py:256\u001b[0m, in \u001b[0;36mConnectionPool.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    253\u001b[0m         closing \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_assign_requests_to_connections()\n\u001b[1;32m    255\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close_connections(closing)\n\u001b[0;32m--> 256\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m    258\u001b[0m \u001b[38;5;66;03m# Return the response. Note that in this case we still have to manage\u001b[39;00m\n\u001b[1;32m    259\u001b[0m \u001b[38;5;66;03m# the point at which the response is closed.\u001b[39;00m\n\u001b[1;32m    260\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response\u001b[38;5;241m.\u001b[39mstream, typing\u001b[38;5;241m.\u001b[39mIterable)\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/httpcore/_sync/connection_pool.py:236\u001b[0m, in \u001b[0;36mConnectionPool.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    232\u001b[0m connection \u001b[38;5;241m=\u001b[39m pool_request\u001b[38;5;241m.\u001b[39mwait_for_connection(timeout\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m    234\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    235\u001b[0m     \u001b[38;5;66;03m# Send the request on the assigned connection.\u001b[39;00m\n\u001b[0;32m--> 236\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    237\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpool_request\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\n\u001b[1;32m    238\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    239\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ConnectionNotAvailable:\n\u001b[1;32m    240\u001b[0m     \u001b[38;5;66;03m# In some cases a connection may initially be available to\u001b[39;00m\n\u001b[1;32m    241\u001b[0m     \u001b[38;5;66;03m# handle a request, but then become unavailable.\u001b[39;00m\n\u001b[1;32m    242\u001b[0m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m    243\u001b[0m     \u001b[38;5;66;03m# In this case we clear the connection and try again.\u001b[39;00m\n\u001b[1;32m    244\u001b[0m     pool_request\u001b[38;5;241m.\u001b[39mclear_connection()\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/httpcore/_sync/connection.py:103\u001b[0m, in \u001b[0;36mHTTPConnection.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connect_failed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    101\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[0;32m--> 103\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_connection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/httpcore/_sync/http11.py:136\u001b[0m, in \u001b[0;36mHTTP11Connection.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    134\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m Trace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse_closed\u001b[39m\u001b[38;5;124m\"\u001b[39m, logger, request) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[1;32m    135\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_response_closed()\n\u001b[0;32m--> 136\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/httpcore/_sync/http11.py:106\u001b[0m, in \u001b[0;36mHTTP11Connection.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Trace(\n\u001b[1;32m     98\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreceive_response_headers\u001b[39m\u001b[38;5;124m\"\u001b[39m, logger, request, kwargs\n\u001b[1;32m     99\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[1;32m    100\u001b[0m     (\n\u001b[1;32m    101\u001b[0m         http_version,\n\u001b[1;32m    102\u001b[0m         status,\n\u001b[1;32m    103\u001b[0m         reason_phrase,\n\u001b[1;32m    104\u001b[0m         headers,\n\u001b[1;32m    105\u001b[0m         trailing_data,\n\u001b[0;32m--> 106\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_receive_response_headers\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    107\u001b[0m     trace\u001b[38;5;241m.\u001b[39mreturn_value \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    108\u001b[0m         http_version,\n\u001b[1;32m    109\u001b[0m         status,\n\u001b[1;32m    110\u001b[0m         reason_phrase,\n\u001b[1;32m    111\u001b[0m         headers,\n\u001b[1;32m    112\u001b[0m     )\n\u001b[1;32m    114\u001b[0m network_stream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_network_stream\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/httpcore/_sync/http11.py:177\u001b[0m, in \u001b[0;36mHTTP11Connection._receive_response_headers\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    174\u001b[0m timeout \u001b[38;5;241m=\u001b[39m timeouts\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mread\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 177\u001b[0m     event \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_receive_event\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(event, h11\u001b[38;5;241m.\u001b[39mResponse):\n\u001b[1;32m    179\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/httpcore/_sync/http11.py:217\u001b[0m, in \u001b[0;36mHTTP11Connection._receive_event\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    214\u001b[0m     event \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_h11_state\u001b[38;5;241m.\u001b[39mnext_event()\n\u001b[1;32m    216\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m event \u001b[38;5;129;01mis\u001b[39;00m h11\u001b[38;5;241m.\u001b[39mNEED_DATA:\n\u001b[0;32m--> 217\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_network_stream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    218\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mREAD_NUM_BYTES\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[1;32m    219\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    221\u001b[0m     \u001b[38;5;66;03m# If we feed this case through h11 we'll raise an exception like:\u001b[39;00m\n\u001b[1;32m    222\u001b[0m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m    223\u001b[0m     \u001b[38;5;66;03m#     httpcore.RemoteProtocolError: can't handle event type\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    227\u001b[0m     \u001b[38;5;66;03m# perspective. Instead we handle this case distinctly and treat\u001b[39;00m\n\u001b[1;32m    228\u001b[0m     \u001b[38;5;66;03m# it as a ConnectError.\u001b[39;00m\n\u001b[1;32m    229\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;241m==\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_h11_state\u001b[38;5;241m.\u001b[39mtheir_state \u001b[38;5;241m==\u001b[39m h11\u001b[38;5;241m.\u001b[39mSEND_RESPONSE:\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/httpcore/_backends/sync.py:128\u001b[0m, in \u001b[0;36mSyncStream.read\u001b[0;34m(self, max_bytes, timeout)\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m map_exceptions(exc_map):\n\u001b[1;32m    127\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sock\u001b[38;5;241m.\u001b[39msettimeout(timeout)\n\u001b[0;32m--> 128\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_bytes\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/Cellar/python@3.9/3.9.21/Frameworks/Python.framework/Versions/3.9/lib/python3.9/ssl.py:1260\u001b[0m, in \u001b[0;36mSSLSocket.recv\u001b[0;34m(self, buflen, flags)\u001b[0m\n\u001b[1;32m   1256\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1257\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1258\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m   1259\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[0;32m-> 1260\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuflen\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1261\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1262\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv(buflen, flags)\n",
      "File \u001b[0;32m/usr/local/Cellar/python@3.9/3.9.21/Frameworks/Python.framework/Versions/3.9/lib/python3.9/ssl.py:1135\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1133\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m, buffer)\n\u001b[1;32m   1134\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1135\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1136\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m SSLError \u001b[38;5;28;01mas\u001b[39;00m x:\n\u001b[1;32m   1137\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m SSL_ERROR_EOF \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msuppress_ragged_eofs:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from langchain.chains.combine_documents.stuff import StuffDocumentsChain\n",
    "from langchain.chains.llm import LLMChain\n",
    "from langchain.chains import MapReduceDocumentsChain, ReduceDocumentsChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "# Initialize the text splitter\n",
    "rtext_splitter = RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=150)\n",
    "\n",
    "# Initialize LLM\n",
    "llm = ChatOpenAI(temperature=0, model_name=\"gpt-4\")\n",
    "\n",
    "# Define the map prompt template\n",
    "map_template = \"\"\"The following is a set of documents\n",
    "{all_data}\n",
    "Based on this list of docs, please perform concise summaries while extracting essential relationships for relationships analysis later, please do include dates of actions or events, which are very important for timeline analysis later. Example: \"Sam gets fired by the OpenAI board on 11/17/2023 or (Nov. 17th, Friday)\", which showcases not only the relationship between Sam and OpenAI, but also when it happens.\n",
    "Helpful Answer:\"\"\"\n",
    "map_prompt = PromptTemplate.from_template(map_template)\n",
    "\n",
    "# Define the map_chain\n",
    "map_chain = LLMChain(llm=llm, prompt=map_prompt)\n",
    "\n",
    "all_data = news_articles_data + documents\n",
    "# Extract text from each document\n",
    "# all_text_data = [doc.page_content for doc in all_data]\n",
    "\n",
    "# Reduce\n",
    "reduce_template = \"\"\"The following is set of summaries:\n",
    "{all_data}\n",
    "Take these and distill it into concise summaries of the articles while containing important relationships and events (including the timeline). Example: \"Sam gets fired by the OpenAI board on 11/17/2023 or (Nov. 17th, Friday)\", which showcases not only the relationship between Sam and OpenAI, but also when it happens.\n",
    "Helpful Answer:\"\"\"\n",
    "reduce_prompt = PromptTemplate.from_template(reduce_template)\n",
    "\n",
    "# ChatPromptTemplate(input_variables=['all_data'], messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['all_data'], template='The following is a set of documents:\\n{all_data}\\nBased on this list of docs, please identify the main themes \\nHelpful Answer:'))])\n",
    "\n",
    "# Run chain\n",
    "reduce_chain = LLMChain(llm=llm, prompt=reduce_prompt)\n",
    "\n",
    "combine_documents_chain = StuffDocumentsChain(\n",
    "    llm_chain=reduce_chain,\n",
    "    document_variable_name=\"all_data\"  # This should match the variable name in reduce_prompt\n",
    ")\n",
    "\n",
    "# Combines and iteravely reduces the mapped documents\n",
    "reduce_documents_chain = ReduceDocumentsChain(\n",
    "    # This is final chain that is called.\n",
    "    combine_documents_chain=combine_documents_chain,\n",
    "    # If documents exceed context for `StuffDocumentsChain`\n",
    "    collapse_documents_chain=combine_documents_chain,\n",
    "    # The maximum number of tokens to group documents into.\n",
    "    token_max=4000,\n",
    ")\n",
    "\n",
    "# Combining documents by mapping a chain over them, then combining results\n",
    "map_reduce_chain = MapReduceDocumentsChain(\n",
    "    # Map chain\n",
    "    llm_chain=map_chain,\n",
    "    # Reduce chain\n",
    "    reduce_documents_chain=reduce_documents_chain,\n",
    "    # The variable name in the llm_chain to put the documents in\n",
    "    document_variable_name=\"all_data\",\n",
    "    # Return the results of the map steps in the output\n",
    "    return_intermediate_steps=False,\n",
    ")\n",
    "\n",
    "text_splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=1000, chunk_overlap=0\n",
    ")\n",
    "split_docs = text_splitter.split_documents(all_data)\n",
    "\n",
    "# Run the MapReduce Chain\n",
    "summarization_results = map_reduce_chain.run(split_docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "5816cb1b-0a49-47fa-be60-102252980584",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"1. Sam Altman was fired as CEO of OpenAI on November 17, 2023, leading to a power struggle within the company. Over 730 employees threatened to quit and join Altman at Microsoft unless the board resigned and reappointed Altman and co-founder Greg Brockman. Despite initial talks of reinstatement, the board later confirmed that Altman would not be returning. Altman and Brockman subsequently joined Microsoft to head a new advanced AI research unit.\\n\\n2. Following Altman's departure, the OpenAI board underwent a reshuffle, with Mira Murati appointed as interim CEO, only to be later replaced by Emmett Shear, the former CEO of Twitch. The board's actions were criticized for lack of transparency and communication, with the exact reasons for Altman's removal remaining unclear.\\n\\n3. The OpenAI board, composed of Ilya Sutskever, Adam D’Angelo, Helen Toner, and Tasha McCauley, faced calls for resignation. Potential replacements included Bret Taylor and Will Hurd. The board's composition was criticized for lack of diversity and deep knowledge about responsible use of AI.\\n\\n4. Altman announced on November 20, 2023, that he would not be returning as CEO of OpenAI and would instead join Microsoft to lead a new AI research team. Nearly 500 of OpenAI’s roughly 770 employees threatened to resign unless the startup’s board resigned and reappointed Altman.\\n\\n5. Microsoft has been acquiring top executives and AI engineering talent from OpenAI, a generative AI company in which Microsoft holds a minority stake worth several billion dollars. Microsoft's leadership, particularly CEO Satya Nadella, has been projecting a 'business as usual' message during these upheavals at OpenAI.\\n\\n6. Microsoft successfully navigated through U.K. and EU competition authorities to merge with Activision by restructuring the deal and agreeing to certain conditions. However, its market power in cloud computing and potential influence over OpenAI is raising concerns among competition regulators.\\n\\n7. Emmett Shear, the ex-CEO of Twitch, was appointed as the interim CEO of OpenAI after Sam Altman was fired and replaced by CTO Mira Murati. Shear plans to hire an independent investigator to look into the events leading up to his appointment and reform the management and leadership team at OpenAI.\\n\\n8. OpenAI, an American AI research organization, was founded in December 2015. Microsoft invested $1 billion in OpenAI Global LLC in 2019 and $10 billion in 2023. On November 17, 2023, Sam Altman was removed as CEO and Greg Brockman was removed as chairman of OpenAI. Both returned four days later after negotiations with the board.\\n\\n9. Mira Murati, born in Albania in 1988, is a technology executive who has worked at Tesla, Leap Motion, and OpenAI. She served as the CTO of OpenAI from 2018, leading projects like ChatGPT, Dall-E, and Codex. She briefly served as interim CEO of OpenAI in November 2023 after Sam Altman was removed, but was replaced by Emmett Shear after three days.\\n\\n10. Sam Altman, born in 1985, is a technology entrepreneur who co-founded Loopt and Hydrazine Capital, and served as a partner and president at Y Combinator. He was the CEO of OpenAI from 2019 until his removal in November 2023, but was reinstated five days later. He also co-founded Tools For Humanity and raised $1 billion for OpenAI from Microsoft.\""
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summarization_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a686f1a8-a3a7-4e4d-a297-3493927ff3f5",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'summarization_results' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Store summarization_results to a text file for future use\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# Timeline will further be added into the summaries\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msummary.txt\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[0;32m----> 4\u001b[0m     file\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;28mstr\u001b[39m(\u001b[43msummarization_results\u001b[49m))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'summarization_results' is not defined"
     ]
    }
   ],
   "source": [
    "# Store summarization_results to a text file for future use\n",
    "# Timeline will further be added into the summaries\n",
    "with open('summary.txt', 'w') as file:\n",
    "    file.write(str(summarization_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd4a39aa-5c78-49ed-a789-6514f33af1b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mDEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting openai\n",
      "  Using cached openai-1.59.7-py3-none-any.whl.metadata (27 kB)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.9/site-packages (from openai) (4.8.0)\n",
      "Collecting distro<2,>=1.7.0 (from openai)\n",
      "  Downloading distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.9/site-packages (from openai) (0.28.1)\n",
      "Collecting jiter<1,>=0.4.0 (from openai)\n",
      "  Downloading jiter-0.8.2-cp39-cp39-macosx_10_12_x86_64.whl.metadata (5.2 kB)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.9/site-packages (from openai) (2.10.5)\n",
      "Requirement already satisfied: sniffio in /usr/local/lib/python3.9/site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.9/site-packages (from openai) (4.64.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.9/site-packages (from openai) (4.12.2)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /usr/local/lib/python3.9/site-packages (from anyio<5,>=3.5.0->openai) (1.2.2)\n",
      "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.9/site-packages (from anyio<5,>=3.5.0->openai) (3.4)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.9/site-packages (from httpx<1,>=0.23.0->openai) (2022.12.7)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.9/site-packages (from httpx<1,>=0.23.0->openai) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.9/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.9/site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.9/site-packages (from pydantic<3,>=1.9.0->openai) (2.27.2)\n",
      "Using cached openai-1.59.7-py3-none-any.whl (454 kB)\n",
      "Downloading distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Downloading jiter-0.8.2-cp39-cp39-macosx_10_12_x86_64.whl (304 kB)\n",
      "\u001b[33mWARNING: Error parsing dependencies of bleach: Expected matching RIGHT_PARENTHESIS for LEFT_PARENTHESIS, after version specifier\n",
      "    tinycss2 (>=1.1.0<1.2) ; extra == 'css'\n",
      "             ~~~~~~~~^\u001b[0m\u001b[33m\n",
      "\u001b[0mInstalling collected packages: jiter, distro, openai\n",
      "\u001b[33m  DEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m  DEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m  DEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mDEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\u001b[33m\n",
      "\u001b[0mSuccessfully installed distro-1.9.0 jiter-0.8.2 openai-1.59.7\n"
     ]
    }
   ],
   "source": [
    "# !pip install openai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f8b2d01-5d61-41f5-a126-99194c2be22a",
   "metadata": {},
   "source": [
    "# Entity and Relationship"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56098473-d023-4452-a439-4cf62556b9e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[38;5;1m✘ OPENAI_API_KEY env variable was not found. Set it and try again.\u001b[0m\n",
      "\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "1",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/amber/Library/Python/3.9/lib/python/site-packages/IPython/core/interactiveshell.py:3450: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import spacy\n",
    "from collections import Counter\n",
    "from pathlib import Path\n",
    "from wasabi import msg\n",
    "from spacy_llm.util import assemble\n",
    "\n",
    "# traditional spacy NER (Named Recognition Library)\n",
    "def split_document_sent(text):\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    doc = nlp(text)\n",
    "    return [sent.text.strip() for sent in doc.sents] # referencial\n",
    "\n",
    "# spacy-llm relationship extraction\n",
    "def process_text(nlp, text, verbose=False):\n",
    "    doc = nlp(text)\n",
    "    if verbose:\n",
    "        msg.text(f\"Text: {doc.text}\")\n",
    "        msg.text(f\"Entities: {[(ent.text, ent.label_) for ent in doc.ents]}\")\n",
    "        msg.text(\"Relations:\")\n",
    "        for r in doc._.rel:\n",
    "            msg.text(f\"  - {doc.ents[r.dep]} [{r.relation}] {doc.ents[r.dest]}\")\n",
    "    return doc\n",
    "\n",
    "def run_pipeline(config_path, examples_path=None, verbose=False):\n",
    "    if not os.getenv(\"OPENAI_API_KEY\"):\n",
    "        msg.fail(\"OPENAI_API_KEY env variable was not found. Set it and try again.\", exits=1)\n",
    "\n",
    "    nlp = assemble(config_path, overrides={} if examples_path is None else {\"paths.examples\": str(examples_path)})\n",
    "\n",
    "    # Initialize counters and storage\n",
    "    processed_data = []\n",
    "    entity_counts = Counter()\n",
    "    relation_counts = Counter()\n",
    "\n",
    "    # Load your articles and news data here\n",
    "    # all_data = news_articles_data + documents\n",
    "\n",
    "    sents = split_document_sent(summarization_results)\n",
    "    for sent in sents:\n",
    "        doc = process_text(nlp, sent, verbose)\n",
    "        entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
    "        relations = [(doc.ents[r.dep].text, r.relation, doc.ents[r.dest].text) for r in doc._.rel]\n",
    "\n",
    "        # Store processed data\n",
    "        processed_data.append({'text': doc.text, 'entities': entities, 'relations': relations})\n",
    "\n",
    "        # Update counters\n",
    "        entity_counts.update([ent[1] for ent in entities])\n",
    "        relation_counts.update([rel[1] for rel in relations])\n",
    "\n",
    "    # Export to JSON\n",
    "    with open('processed_data.json', 'w') as f:\n",
    "        json.dump(processed_data, f)\n",
    "\n",
    "    # Display summary\n",
    "    msg.text(f\"Entity counts: {entity_counts}\")\n",
    "    msg.text(f\"Relation counts: {relation_counts}\")\n",
    "\n",
    "# Set your configuration paths and flags\n",
    "config_path = Path(\"zeroshot.cfg\")\n",
    "examples_path = None  # or None if not using few-shot\n",
    "verbose = True\n",
    "\n",
    "# Run the pipeline\n",
    "file = run_pipeline(config_path, None, verbose)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
