{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.9.21\n"
     ]
    }
   ],
   "source": [
    "from platform import python_version\n",
    "\n",
    "print(python_version())\n",
    "\n",
    "# %pip install langchain\n",
    "# %pip install -U langchain-community\n",
    "# %pip install pypdf\n",
    "# %pip install spacy\n",
    "# !python3.9 -m pip install spacy-llm\n",
    "# !python3.9 -m spacy download en_core_web_sm\n",
    "# %pip install scikit-learn\n",
    "# %pip install -U sentence-transformers\n",
    "# %pip install keybert\n",
    "# %pip install transformers sentence-transformers nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Apple-related documents: 17\n",
      "Number of Studio-related documents: 7\n"
     ]
    }
   ],
   "source": [
    "import langchain\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.docstore.document import Document\n",
    "import os\n",
    "\n",
    "# Directory containing your PDF files\n",
    "directory_path = 'TCs'\n",
    "\n",
    "# Initialize PyPDFLoader for each PDF in the directory\n",
    "loaders = [PyPDFLoader(os.path.join(directory_path, f)) for f in os.listdir(directory_path) if f.endswith('.pdf')]\n",
    "\n",
    "# Load documents from PDFs \n",
    "tc_docs = []\n",
    "for loader in loaders:\n",
    "    tc_docs.extend(loader.load())\n",
    "\n",
    "\n",
    "tc_data = [\n",
    "    Document(\n",
    "        page_content=doc.page_content, \n",
    "        metadata={\n",
    "            \"source\": doc.metadata['source'].removeprefix('TCs'),\n",
    "        }\n",
    "    )\n",
    "    for doc in tc_docs\n",
    "]\n",
    "\n",
    "# Separate documents based on their source metadata\n",
    "apple_docs = [doc for doc in tc_data if \"apple_macos_english\" in doc.metadata[\"source\"].lower()]\n",
    "rstudio_docs = [doc for doc in tc_data if \"rstudio_end\" in doc.metadata[\"source\"].lower()]\n",
    "\n",
    "print(f\"Number of Apple-related documents: {len(apple_docs)}\")\n",
    "print(f\"Number of Studio-related documents: {len(rstudio_docs)}\")\n",
    "\n",
    "page_contents_apple = [doc.page_content for doc in apple_docs]\n",
    "page_contents_rstudio = [doc.page_content for doc in rstudio_docs]\n",
    "\n",
    "combined_apple = \"\".join(page_contents_apple) \n",
    "combined_rstudio = \"\".join(page_contents_rstudio) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple 150\n",
      "Rstudio 112\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500, chunk_overlap=60\n",
    ")\n",
    "# split_docs = text_splitter.split_text(data)\n",
    "split_docs_apple = text_splitter.split_text(combined_apple)\n",
    "print(\"Apple\", len(split_docs_apple))\n",
    "\n",
    "split_docs_rstudio = text_splitter.split_text(combined_rstudio)\n",
    "print(\"Rstudio\", len(split_docs_rstudio))\n",
    "\n",
    "# function for functionality of pipeline\n",
    "def split_text(combined_text):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=500,\n",
    "        chunk_overlap=60\n",
    "    )\n",
    "    return text_splitter.split_text(combined_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entity & Relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/spacy_llm/pipeline/llm.py:143: UserWarning: Task supports sharding, but model does not provide context length. Data won't be sharded, prompt might exceed the model's context length. Set context length in your config. If you think spacy-llm should provide the context length for this model automatically, report this to https://github.com/explosion/spacy-llm/issues.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entity counts: Counter({'ORG': 263, 'GPE': 81, 'CARDINAL': 20, 'PERSON': 20,\n",
      "'ORDINAL': 17, 'LAW': 13, 'WORK_OF_ART': 10, 'PRODUCT': 1, 'DATE': 1, 'EVENT':\n",
      "1, 'TIME': 1})\n",
      "Relation counts: Counter({'is located in': 43, 'uses': 24, 'related_to': 23,\n",
      "'provides': 18, 'is a type of': 9, 'is_a': 7, 'supports': 6, 'includes': 6,\n",
      "'refers_to': 6, 'has access to': 5, 'affiliated_with': 4, 'governs': 3, 'runs':\n",
      "3, 'included_with': 3, 'associated_with': 3, 'obtained_from': 2, 'branded_by':\n",
      "2, 'is a quantity of': 2, 'is related to': 2, 'licensed_by': 2, 'affects': 2,\n",
      "'provides access to': 2, 'does not endorse': 2, 'governed by': 2, 'owns': 1,\n",
      "'subject to': 1, 'set forth in': 1, 'related to': 1, 'produces': 1,\n",
      "'quantifies': 1, 'ownership': 1, 'supported_on': 1, 'downloads': 1, 'delivers':\n",
      "1, 'has_license_agreement': 1, 'uses_feature': 1, 'stores_content': 1,\n",
      "'granted_rights': 1, 'has_rights_over': 1, 'is used on': 1, 'grants': 1,\n",
      "'not_permit': 1, 'transfers software to': 1, 'compatible_with': 1, 'offers': 1,\n",
      "'modifies': 1, 'modification of': 1, 'part of': 1, 'warranty for': 1, 'is part\n",
      "of': 1, 'checks_with': 1, 'owned_by': 1, 'leases': 1, 'installed_on': 1,\n",
      "'defines': 1, 'subleasing': 1, 'complies_with': 1, 'agrees_to': 1,\n",
      "'enforces_compliance': 1, 'used by': 1, 'notifies': 1, 'regulates': 1,\n",
      "'references': 1, 'responsibility': 1, 'licensed_from': 1, 'provided_by': 1,\n",
      "'transferred_with': 1, 'provides_update': 1, 'brand': 1, 'enables access to': 1,\n",
      "'owned by': 1, 'cease_use_of': 1, 'disclaims': 1, 'authorized representative\n",
      "of': 1, 'applies to': 1, 'commits': 1, 'licensed to': 1, 'relates_to': 1,\n",
      "'located_in': 1, 'licensed under': 1, 'encoded by': 1, 'licensed by': 1,\n",
      "'contains': 1, 'consent_required': 1, 'not liable': 1,\n",
      "'no_obligation_to_support': 1, 'distributed_with': 1, 'available_from': 1,\n",
      "'licensed_to': 1, 'is_feature_of': 1, 'is_type_of': 1, 'is provided by': 1,\n",
      "'provides_service': 1, 'in_good_standing_with': 1, 'requires': 1, 'applies_to':\n",
      "1, 'contracts_with': 1, 'disclaims liability': 1, 'affiliation': 1})\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import spacy\n",
    "from collections import Counter\n",
    "from pathlib import Path\n",
    "from wasabi import msg\n",
    "from spacy_llm.util import assemble\n",
    "\n",
    "import spacy\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "# traditional spacy NER\n",
    "# def split_document_sent(text):\n",
    "#     nlp = spacy.load(\"en_core_web_sm\")\n",
    "#     doc = nlp(text)\n",
    "#     return [sent.text.strip() for sent in doc.sents] # referencial\n",
    "\n",
    "def summarize_section(text):\n",
    "    # Load SpaCy model\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    doc = nlp(text)\n",
    "\n",
    "    # Extract sentences\n",
    "    sentences = [sent.text.strip() for sent in doc.sents if sent.text.strip()]\n",
    "\n",
    "    # Check if there are meaningful sentences\n",
    "    if not sentences:\n",
    "        return \"No summary available\"\n",
    "\n",
    "    try:\n",
    "        # Compute sentence embeddings using TF-IDF\n",
    "        vectorizer = TfidfVectorizer(stop_words='english')\n",
    "        sentence_vectors = vectorizer.fit_transform(sentences).toarray()\n",
    "\n",
    "        # Calculate average vector and similarity\n",
    "        avg_vector = np.mean(sentence_vectors, axis=0).reshape(1, -1)\n",
    "        similarities = cosine_similarity(avg_vector, sentence_vectors)\n",
    "\n",
    "        # Select the most relevant sentence\n",
    "        most_relevant_index = np.argmax(similarities)\n",
    "        return sentences[most_relevant_index]\n",
    "\n",
    "    except ValueError:  # Handle cases with stop-words-only or empty input\n",
    "        return \"No summary available\"\n",
    "\n",
    "# spacy-llm relationship extraction\n",
    "# def process_text(nlp, text, verbose=False):\n",
    "#     doc = nlp(text)\n",
    "#     if verbose:\n",
    "#         msg.text(f\"Text: {doc.text}\")\n",
    "#         msg.text(f\"Entities: {[(ent.text, ent.label_) for ent in doc.ents]}\")\n",
    "#         msg.text(\"Relations:\")\n",
    "#         for r in doc._.rel:\n",
    "#             msg.text(f\"  - {doc.ents[r.dep]} [{r.relation}] {doc.ents[r.dest]}\")\n",
    "#     return doc\n",
    "\n",
    "def run_pipeline(combined, config_path, filename, examples_path=None, verbose=False):\n",
    "    if not os.getenv(\"OPENAI_API_KEY\"):\n",
    "        msg.fail(\"OPENAI_API_KEY env variable was not found. Set it and try again.\", exits=1)\n",
    "\n",
    "    sections = split_text(combined)\n",
    "    nlp = assemble(config_path, overrides={} if examples_path is None else {\"paths.examples\": str(examples_path)})\n",
    "\n",
    "    # Initialize counters and storage\n",
    "    processed_data = []\n",
    "    entity_counts = Counter()\n",
    "    relation_counts = Counter()\n",
    "\n",
    "    for section in sections:\n",
    "        summary = summarize_section(section)\n",
    "        doc = nlp(summary)\n",
    "        entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
    "        relations = [(doc.ents[r.dep].text, r.relation, doc.ents[r.dest].text) for r in doc._.rel]\n",
    "\n",
    "        # Store processed data\n",
    "        processed_data.append({\n",
    "            'original_text': section,\n",
    "            'summary': summary,\n",
    "            'entities': entities,\n",
    "            'relations': relations})\n",
    "\n",
    "        # Update counters\n",
    "        entity_counts.update([ent[1] for ent in entities])\n",
    "        relation_counts.update([rel[1] for rel in relations])\n",
    "\n",
    "    # Export to JSON\n",
    "    with open(filename, 'w') as f:\n",
    "        json.dump(processed_data, f)\n",
    "\n",
    "    # Display summary\n",
    "    msg.text(f\"Entity counts: {entity_counts}\")\n",
    "    msg.text(f\"Relation counts: {relation_counts}\")\n",
    "\n",
    "# Set your configuration paths and flags\n",
    "config_path = Path(\"config.cfg\")\n",
    "examples_path = None  # or None if not using few-shot\n",
    "verbose = True\n",
    "\n",
    "# Run the pipeline for Apple\n",
    "file_apple = run_pipeline(combined_apple, config_path, 'apple_processed_data.json', None, verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/spacy_llm/pipeline/llm.py:143: UserWarning: Task supports sharding, but model does not provide context length. Data won't be sharded, prompt might exceed the model's context length. Set context length in your config. If you think spacy-llm should provide the context length for this model automatically, report this to https://github.com/explosion/spacy-llm/issues.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entity counts: Counter({'ORG': 107, 'CARDINAL': 48, 'LAW': 13, 'DATE': 12,\n",
      "'ORDINAL': 11, 'PERSON': 8, 'WORK_OF_ART': 6, 'PRODUCT': 5, 'GPE': 5, 'FAC': 4,\n",
      "'EVENT': 3, 'PERCENT': 1, 'MONEY': 1})\n",
      "Relation counts: Counter({'uses': 6, 'related_to': 6, 'is a type of': 6,\n",
      "'delivers': 5, 'is_part_of': 4, 'defines': 3, 'provides': 3, 'governed_by': 3,\n",
      "'includes': 2, 'is a version of': 2, 'in accordance with': 2, 'limits': 2,\n",
      "'has_term': 2, 'contains': 2, 'involves': 2, 'liability_to': 2, 'has_product':\n",
      "1, 'is_located_in': 1, 'offers_support': 1, 'authorized by': 1, 'offers': 1,\n",
      "'evaluates': 1, 'is associated with': 1, 'alternative': 1, 'purpose': 1,\n",
      "'grants': 1, 'backing-up': 1, 'serves': 1, 'terminate': 1, 'assign license': 1,\n",
      "'refers to': 1, 'is provided by': 1, 'terms_of': 1, 'licenses': 1, 'transmits\n",
      "data to': 1, 'provides activation mechanism in': 1, 'is part of': 1, 'creates':\n",
      "1, 'incorporates': 1, 'provides support for': 1, 'payment_due_time': 1,\n",
      "'disclose': 1, 'protects': 1, 'owns': 1, 'is_a': 1, 'warrants': 1,\n",
      "'varies_from': 1, 'agrees to defend': 1, 'against': 1, 'alleging infringement\n",
      "by': 1, 'arising under': 1, 'modification': 1, 'obligated_to_indemnify': 1,\n",
      "'not_in_accordance_with': 1, 'claims arise out of': 1, 'not in compliance with':\n",
      "1, 'breach of obligations': 1, 'breach of restrictions': 1, 'is related to': 1,\n",
      "'commences_on': 1, 'participates_in': 1, 'extends': 1, 'follows': 1,\n",
      "'cease_use_of': 1, 'certify_within': 1, 'time_limit_after': 1, 'applies to': 1,\n",
      "'is defined in': 1, 'cites': 1, 'supplements': 1, 'is available on': 1})\n"
     ]
    }
   ],
   "source": [
    "# Run the pipeline for Apple\n",
    "file_rstudio = run_pipeline(combined_rstudio, config_path, 'rstudio_processed_data.json', None, verbose)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Addition of description per chunk of text as well as simplification "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To the extent that this software may be used to reproduce, modify, publish or \n",
      "distribute materials, it is licensed to you only for reproduction, modiﬁcation, publication and \n",
      "distribution of non-copyrighted materials, materials in which you own the copyright, or materials \n",
      "you are authorized or legally permitted to reproduce, modify, publish or distribute.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'IMPORTANT NOTE: To the extent that this software may be used to reproduce, modify, publish or \\ndistribute materials, it is licensed to you only for reproduction, modiﬁcation, publication and \\ndistribution of non-copyrighted materials, materials in which you own the copyright, or materials \\nyou are authorized or legally permitted to reproduce, modify, publish or distribute. If you are \\nuncertain about your right to copy, modify, publish or distribute any material, you should contact'"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "def summarize_section(text):\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    doc = nlp(text)\n",
    "    sentences = [sent.text for sent in doc.sents]\n",
    "\n",
    "    vectorizer = TfidfVectorizer(stop_words='english')\n",
    "    # convert to array in order to perform cosine similarity\n",
    "    sentence_vectors = vectorizer.fit_transform(sentences).toarray()\n",
    "\n",
    "    avg_vector = np.mean(sentence_vectors, axis=0)\n",
    "    avg_vector = avg_vector.reshape(1, -1)\n",
    "    similarities = cosine_similarity(avg_vector, sentence_vectors)\n",
    "\n",
    "    # index with highest cosine similarity to the average vector\n",
    "    most_relevant_index = np.argmax(similarities)\n",
    "\n",
    "    return sentences[most_relevant_index]\n",
    "\n",
    "\n",
    "print(summarize_section(split_docs_apple[2]))\n",
    "split_docs_apple[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "By clicking on the “a ccept” button at t he e nd of this  document or by accessing, executing or otherwise using the \n",
      "Software, you acknowledge that you have read this Agreement, understand it and agree to be bound by its terms \n",
      "and conditions.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'By clicking on the “a ccept” button at t he e nd of this  document or by accessing, executing or otherwise using the \\nSoftware, you acknowledge that you have read this Agreement, understand it and agree to be bound by its terms \\nand conditions. If you are not willing to be bound by the terms of this Agreement, do not access or use the Software. \\n \\nIf you are using the Software in your capacity as employee or agent of a  company or organization, then any'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(summarize_section(split_docs_rstudio[2]))\n",
    "split_docs_rstudio[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simplification with Lexical Simplification with BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/meryjoy/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "BertForMaskedLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simplified Text: \n",
      "The user shall be deemed to have consent to the terms and conditions , referred to as the agreement. You agree to be bound by the contract under this contract.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForMaskedLM, AutoTokenizer\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import numpy as np\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Load models\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\", trust_remote_code=True)\n",
    "model = AutoModelForMaskedLM.from_pretrained(\"bert-base-uncased\", trust_remote_code=True)\n",
    "similarity_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "def mask_word(sentence, word):\n",
    "    \"\"\"Replace a word in the sentence with [MASK].\"\"\"\n",
    "    return sentence.replace(word, \"[MASK]\")\n",
    "\n",
    "def generate_candidates(masked_sentence, top_k=5):\n",
    "    \"\"\"Generate replacement candidates using MLM.\"\"\"\n",
    "    inputs = tokenizer(masked_sentence, return_tensors=\"pt\")\n",
    "    outputs = model(**inputs)\n",
    "    logits = outputs.logits\n",
    "\n",
    "    mask_token_index = (inputs.input_ids == tokenizer.mask_token_id).nonzero(as_tuple=True)[1]\n",
    "    mask_logits = logits[0, mask_token_index, :]\n",
    "    top_tokens = mask_logits.topk(top_k, dim=-1).indices[0].tolist()\n",
    "\n",
    "    candidates = [tokenizer.decode([token]).strip() for token in top_tokens]\n",
    "    return candidates\n",
    "\n",
    "def rank_candidates(sentence, word, candidates):\n",
    "    \"\"\"Rank candidates based on semantic similarity and simplicity.\"\"\"\n",
    "    original_embedding = similarity_model.encode(sentence)\n",
    "    rankings = []\n",
    "\n",
    "    for candidate in candidates:\n",
    "        replaced_sentence = sentence.replace(word, candidate)\n",
    "        candidate_embedding = similarity_model.encode(replaced_sentence)\n",
    "        similarity = util.cos_sim(original_embedding, candidate_embedding)[0][0].item()\n",
    "        rankings.append((candidate, similarity))\n",
    "\n",
    "    # Sort by similarity and word length (shorter words preferred for simplicity)\n",
    "    rankings.sort(key=lambda x: (-x[1], len(x[0])))\n",
    "    return [candidate for candidate, _ in rankings]\n",
    "\n",
    "def simplify_text(text, complexity_threshold=5):\n",
    "    \"\"\"Simplify text by replacing complex words.\"\"\"\n",
    "    simplified_sentences = []\n",
    "    sentences = sent_tokenize(text)\n",
    "\n",
    "    for sentence in sentences:\n",
    "        words = word_tokenize(sentence)\n",
    "        for word in words:\n",
    "            if len(word) > complexity_threshold:  # Detect complex words by length\n",
    "                masked_sentence = mask_word(sentence, word)\n",
    "                candidates = generate_candidates(masked_sentence)\n",
    "                ranked_candidates = rank_candidates(sentence, word, candidates)\n",
    "\n",
    "                if ranked_candidates:\n",
    "                    sentence = sentence.replace(word, ranked_candidates[0], 1)\n",
    "        simplified_sentences.append(sentence)\n",
    "\n",
    "    return \" \".join(simplified_sentences)\n",
    "\n",
    "# Example usage\n",
    "text = \"\"\"\n",
    "The user shall be deemed to have consented to the terms and conditions herein referred to as the agreement. \n",
    "You agree to be bound by the obligations under this contract.\n",
    "\"\"\"\n",
    "\n",
    "simplified_text = simplify_text(text)\n",
    "print(\"Simplified Text:\", simplified_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Still work ahead for this part of the project, might be for later, not enough time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Cypher Queries with eFLINT notation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_and_generate_queries(json_data, file_path):\n",
    "    nodes = {}\n",
    "    relationships = []\n",
    "\n",
    "    # Enhanced mapping function for entity types based on eFLINT elements\n",
    "    def map_eflint_type(entity_type, entity_name):\n",
    "        if 'http://' in entity_type or 'https://' in entity_name:\n",
    "            return 'FACT'\n",
    "        if entity_type in ['CARDINAL', 'ORDINAL']:\n",
    "            return 'NUMBER'\n",
    "        if entity_type == 'ORG':\n",
    "            if 'terms' in entity_name.lower() or 'conditions' in entity_name.lower() or 'agreement' in entity_name.lower():\n",
    "                return 'DUTY'\n",
    "            else:\n",
    "                return 'ACTOR'\n",
    "        if 'section' in entity_name.lower() or 'section' in entity_type.lower():\n",
    "            return 'SECTION'\n",
    "        mapping = {\n",
    "            'PERSON': 'ACTOR',\n",
    "            'EVENT': 'EVENT',\n",
    "            'LAW': 'DUTY',\n",
    "            'WORK_OF_ART': 'ACT',\n",
    "            'CONDITION': 'CONDITION',\n",
    "            'DATE': 'DATE',\n",
    "        }\n",
    "        return mapping.get(entity_type, 'FACT')\n",
    "\n",
    "    # Process summaries and associate them with entities\n",
    "    for item in json_data:\n",
    "        summary = item.get('summary', 'No summary available')\n",
    "        entities = item.get('entities', [])\n",
    "        relations = item.get('relations', [])\n",
    "\n",
    "        for entity in entities:\n",
    "            entity_name, entity_type = entity[:2]\n",
    "            mapped_type = map_eflint_type(entity_type, entity_name)\n",
    "            node_id = f\"{entity_name.replace(' ', '_')}_{mapped_type}\"\n",
    "            if node_id not in nodes:\n",
    "                nodes[node_id] = {\n",
    "                    'name': entity_name,\n",
    "                    'type': mapped_type,\n",
    "                    'descriptions': [summary],  # Start with the current summary\n",
    "                }\n",
    "            else:\n",
    "                nodes[node_id]['descriptions'].append(summary)  # Append additional summaries\n",
    "\n",
    "        # Add relationships if present\n",
    "        for relation in relations:\n",
    "            src_id = f\"{relation[0].replace(' ', '_')}_{map_eflint_type(entity_type, relation[0])}\"\n",
    "            tgt_id = f\"{relation[2].replace(' ', '_')}_{map_eflint_type(entity_type, relation[2])}\"\n",
    "            relationship_type = relation[1].replace(' ', '_').replace('-', '_')\n",
    "            relationships.append((src_id, relationship_type, tgt_id))\n",
    "\n",
    "    # Create Cypher queries for nodes\n",
    "    # node_queries = [\n",
    "    #     f\"\"\"\n",
    "    #     MERGE (n:{data['type']} {{name: '{data['name']}'}})\n",
    "    #     SET n.id = '{node_id}', n.descriptions = {json.dumps(data['descriptions'])}\n",
    "    #     \"\"\"\n",
    "    #     for node_id, data in nodes.items() if data['type'] != 'NUMBER'\n",
    "    # ]\n",
    "\n",
    "    # Create Cypher queries for nodes\n",
    "    node_queries = [\n",
    "        f\"\"\"\n",
    "        MERGE (n:{data['type']} {{name: '{data['name'].replace(\"'\", \"\")}'}})\n",
    "        SET n.id = '{node_id.replace(\"'\", \"\")}', \n",
    "        n.descriptions = {json.dumps([desc.replace(\"'\", \"\") for desc in data['descriptions']])}\n",
    "        \"\"\"\n",
    "        for node_id, data in nodes.items() if data['type'] != 'NUMBER'\n",
    "    ]\n",
    "  \n",
    "\n",
    "    # Create Cypher queries for relationships\n",
    "    # relationship_queries = [\n",
    "    #     f\"\"\"\n",
    "    #     MATCH (a), (b)\n",
    "    #     WHERE a.id = '{rel[0]}' AND b.id = '{rel[2]}'\n",
    "    #     MERGE (a)-[:{rel[1]}]->(b)\n",
    "    #     \"\"\"\n",
    "    #     for rel in relationships\n",
    "    # ]\n",
    "\n",
    "    relationship_queries = [\n",
    "        f\"\"\"\n",
    "        MATCH (a), (b)\n",
    "        WHERE a.id = '{rel[0].replace(\"'\", \"\")}' AND b.id = '{rel[2].replace(\"'\", \"\")}'\n",
    "        MERGE (a)-[:{rel[1].replace(\"'\", \"\")}]->(b)\n",
    "        \"\"\"\n",
    "        for rel in relationships\n",
    "    ]\n",
    "\n",
    "    queries = node_queries + relationship_queries\n",
    "\n",
    "    # Save queries to a text file\n",
    "    with open(file_path, 'w') as file:\n",
    "        for query in queries:\n",
    "            file.write(query.strip() + '\\n')\n",
    "\n",
    "    return queries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('apple_processed_data.json', 'r') as file:\n",
    "    json_data = json.load(file)\n",
    "\n",
    "queries_apple = classify_and_generate_queries(json_data, 'apple_cypher_queries.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('rstudio_processed_data.json', 'r') as file:\n",
    "#     json_data = json.load(file)\n",
    "\n",
    "# queries_rstudio = classify_and_generate_queries(json_data, 'rstudio_cypher_queries.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neo4j import GraphDatabase\n",
    "\n",
    "def execute_queries(queries, uri, user, password):\n",
    "    driver = GraphDatabase.driver(uri, auth=(user, password))\n",
    "    with driver.session() as session:\n",
    "        for query in queries:\n",
    "            session.run(query)\n",
    "    driver.close()\n",
    "\n",
    "# Example usage\n",
    "uri = \"neo4j://localhost:7687\"\n",
    "user = \"neo4j\"\n",
    "password = \"movies11\"\n",
    "\n",
    "#execute_queries(queries_apple, uri, user, password)\n",
    "execute_queries(queries_apple, uri, user, password)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other option: \n",
    "### Using Langchain for Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install langchain_openai\n",
    "# %pip install langchain_experimental\n",
    "# %pip install neo4j langchain-neo4j\n",
    "# %pip install yfiles_jupyter_graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.getenv(\"NEO4J_URI\"):\n",
    "    print(\"nope\")\n",
    "if not os.getenv(\"NEO4J_USERNAME\"):\n",
    "    print(\"nope\")\n",
    "if not os.getenv(\"NEO4J_PASSWORD\"):\n",
    "    print(\"nope\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import TokenTextSplitter\n",
    "text_splitter = TokenTextSplitter(chunk_size=512, chunk_overlap=24)\n",
    "tc_document = text_splitter.split_documents(tc_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_experimental.graph_transformers import LLMGraphTransformer\n",
    "\n",
    "llm=ChatOpenAI(temperature=0, model_name=\"gpt-4o-mini\")\n",
    "     \n",
    "llm_transformer = LLMGraphTransformer(llm=llm)\n",
    "\n",
    "graph_documents = llm_transformer.convert_to_graph_documents(tc_document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_neo4j import Neo4jGraph\n",
    "\n",
    "graph = Neo4jGraph()\n",
    "graph.add_graph_documents(\n",
    "    graph_documents,\n",
    "    baseEntityLabel=True,\n",
    "    include_source=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from yfiles_jupyter_graphs import GraphWidget\n",
    "from neo4j import GraphDatabase\n",
    "\n",
    "default_cypher = \"MATCH (s)-[r:!MENTIONS]->(t) RETURN s,r,t LIMIT 80\"\n",
    "\n",
    "def showGraph(cypher: str = default_cypher):\n",
    "    # create a neo4j session to run queries\n",
    "    driver = GraphDatabase.driver(\n",
    "        uri = os.environ[\"NEO4J_URI\"],\n",
    "        auth = (os.environ[\"NEO4J_USERNAME\"],\n",
    "                os.environ[\"NEO4J_PASSWORD\"]))\n",
    "    session = driver.session()\n",
    "    widget = GraphWidget(graph = session.run(cypher).graph())\n",
    "    widget.node_label_mapping = 'id'\n",
    "    display(widget)\n",
    "    return widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# showGraph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarization based RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains.llm import LLMChain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "# Neo4j Connection\n",
    "class KnowledgeGraph:\n",
    "    def __init__(self, uri, user, password):\n",
    "        from neo4j import GraphDatabase\n",
    "        self.driver = GraphDatabase.driver(uri, auth=(user, password))\n",
    "    \n",
    "    def close(self):\n",
    "        self.driver.close()\n",
    "    \n",
    "    def get_all_summaries(self):\n",
    "        with self.driver.session() as session:\n",
    "            query = \"\"\"\n",
    "            MATCH (n)\n",
    "            RETURN DISTINCT n.descriptions AS descriptions\n",
    "            \"\"\"\n",
    "            results = session.run(query)\n",
    "            summaries = []\n",
    "            for record in results:\n",
    "                descriptions = record[\"descriptions\"]\n",
    "                if descriptions:\n",
    "                    summaries.extend(descriptions)\n",
    "            return list(set(summaries))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary in Paragraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simplified Summarization Workflow\n",
    "def summarize_in_chunks(summaries, llm):\n",
    "    # summaries to single text\n",
    "    combined_text = \" \".join(summaries)\n",
    "\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=150)\n",
    "    split_texts = text_splitter.split_text(combined_text)\n",
    "\n",
    "    # summarize each split / chunk\n",
    "    chunk_prompt = PromptTemplate.from_template(\n",
    "        \"\"\"Summarize the following text while preserving key relationships, events, and timelines:\n",
    "        {text}\n",
    "        Helpful Answer:\"\"\"\n",
    "    )\n",
    "    llm_chain = LLMChain(llm=llm, prompt=chunk_prompt)\n",
    "\n",
    "    chunk_summaries = []\n",
    "    for chunk in split_texts:\n",
    "        summary = llm_chain.run({\"text\": chunk})\n",
    "        chunk_summaries.append(summary)\n",
    "\n",
    "    # combination into final summary\n",
    "    final_prompt = PromptTemplate.from_template(\n",
    "        \"\"\"Combine the following summaries into a cohesive final summary:\n",
    "        {text}\n",
    "        Helpful Answer:\"\"\"\n",
    "    )\n",
    "    final_chain = LLMChain(llm=llm, prompt=final_prompt)\n",
    "    final_summary = final_chain.run({\"text\": \" \".join(chunk_summaries)})\n",
    "\n",
    "    return final_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def clear_graph(uri, user, password):\n",
    "#     driver = GraphDatabase.driver(uri, auth=(user, password))\n",
    "#     with driver.session() as session:\n",
    "#         session.run(\"MATCH (n) DETACH DELETE n\")\n",
    "#     driver.close()\n",
    "\n",
    "# # Usage\n",
    "# clear_graph(\"neo4j://localhost:7687\", \"neo4j\", \"movies11\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary in bullet points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simplified Summarization Workflow\n",
    "def summarize_in_points(summaries, llm):\n",
    "    # summaries to single text\n",
    "    combined_text = \" \".join(summaries)\n",
    "\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=70)\n",
    "    split_texts = text_splitter.split_text(combined_text)\n",
    "\n",
    "    # summarize each split / chunk\n",
    "    chunk_prompt = PromptTemplate.from_template(\n",
    "        \"\"\"Summarize the following text into a structured format with bullet points and headings, ensuring that key relationships and events are preserved: \n",
    "        {text} \n",
    "        Helpful Answer:\"\"\"\n",
    "    )\n",
    "    llm_chain = LLMChain(llm=llm, prompt=chunk_prompt)\n",
    "\n",
    "    chunk_summaries = []\n",
    "    for chunk in split_texts:\n",
    "        summary = llm_chain.run({\"text\": chunk})\n",
    "        chunk_summaries.append(summary)\n",
    "\n",
    "    # combination into final summary\n",
    "    final_prompt = PromptTemplate.from_template(\n",
    "        \"\"\"Combine the following summaries into a structured format with bullet points and headings, while keeping the same structure, into a cohesive final summary: \n",
    "        {text} \n",
    "        Helpful Answer:\"\"\"\n",
    "    )\n",
    "    final_chain = LLMChain(llm=llm, prompt=final_prompt)\n",
    "    final_summary = final_chain.run({\"text\": \" \".join(chunk_summaries)})\n",
    "\n",
    "    return final_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Summary:\n",
      "The text outlines the terms and conditions for using Apple Software on a supported Mac Computer, including automatic downloads and installations of software changes. The number of copies that can be downloaded depends on the user's license with Apple. The software is licensed under the MPEG-4 Visual Patent Portfolio License for personal and non-commercial use. It also discusses the use of Gracenote software, licensed by MPEG LA, for personal and non-commercial activities, which enables applications to identify discs and files and obtain music-related information. The text also mentions the use of Setup/Migration Assistant for transferring software between Apple-branded computers. Users are advised to suspend the ability to pay with virtual Supported Cards on their Mac Computer by putting it into Lost Mode. If the license for the Gracenote software terminates, users must cease all use of the Gracenote Data, Software, and Servers. The text also mentions potential risks such as loss of profits, corruption or loss of data, and failure to transmit or receive any data. The text also mentions the Wallet feature, which allows users to store virtual representations of credit, debit, and prepaid cards. The countries mentioned could possibly be the locations where these conditions apply. The text outlines the terms of use for Apple Software, including the immediate termination of a Lessee's use upon discovery or notification of a breach. It states that there is no guarantee that the Gracenote Software or Servers will be error-free or uninterrupted. The software allows for real-time, on-device caption generation (\"Live Captions\") and the creation of a personal voice feature for non-commercial use. The leased Apple Software should be used solely for providing Permitted Developer Services, with each Lessee agreeing to the license terms. Each lease period must last for a minimum of 24 consecutive hours. The license agreement stipulates that each lease period must be at least 24 consecutive hours. During this time, the End User Lessee should have sole and exclusive use and control of the Apple Software and the Apple-branded hardware it is installed on. By using Apple Pay and Wallet, users consent to Apple's transmission, collection, maintenance, processing, and use of all related information to provide these services. The Apple Software is only supported on Apple-branded hardware that meets specific system requirements set by Apple. Users also agree to use Apple Pay in accordance with these terms. The text discusses the terms of a software license agreement for macOS, which also governs the use of Apple Pay and Apple Wallet. The user agrees not to alter any proprietary notices within the Apple Software. The AMR encoding and decoding functionality in the product is not licensed for use in cellular communications infrastructure. The agreement also mentions that if the Consumer Contract Act of Japan applies, terms limiting Apple's liability for damages from breach will be in effect. Japan has imposed terms on Apple that prevent the company from limiting its liability for damages resulting from contract breaches or torts, unless the damage is due to Apple's intentional misconduct or gross negligence. Apple does not guarantee the appropriateness or availability of its software, services, and third-party materials for use in any specific location. The use of a modified device with the services is prohibited and can result in restricted access to the services. Some Apple software and services may display or include third-party content or links to third-party websites. Apple requires advance notice from each lessor before the use of iCloud. Users must agree to use iCloud and its features according to the latest terms and conditions, which can be reviewed on Apple's website. Users must also use the Apple Software and Services in compliance with all applicable laws, including local laws of the country or region they reside in or where they download or use the software and services. Apple, Apple Payments Services, and their affiliates are all parties to the Apple Pay Terms with respect to the services they provide. Users are reminded that no oral or written information or advice given by Apple or an Apple authorized representative creates a warranty. The text states that Apple does not provide a warranty for its software and services, and does not guarantee the accuracy or availability of any data displayed by its software and services. It also states that Apple is not responsible for any third-party services or materials, and is not a party to any agreement between the user and a third party. The Gracenote service, used by Apple, tracks queries for statistical purposes. Upon termination of the license, the user must stop using the Apple software and services. The text states that Apple is not responsible for any agreements between the user and a third party, particularly regarding installment payment services. The user acknowledges that Apple Software and Services contain proprietary content owned by Apple or its licensors, protected by intellectual property laws, including copyright. The GPL and LGPL are included with the Apple Software. Neither the FAA nor Apple are liable for any loss, damage, claim, liability, expense, or penalty. When using Apple Pay, only one device can remotely connect at a time to control the Apple Software's graphical desktop session displayed on the Home Mac. The text describes the ability to control the graphical desktop session of Apple Software running on a Home Mac. It also allows a reasonable number of devices to remotely connect simultaneously to observe the same session, provided they do not control the software. Apple is not responsible for updates, maintenance, warranty, technical support, or services for the modified software. Users may need to authenticate with Apple to receive Apple Eligible Content. The delivery of this content through a Caching Enabled Mac does not change the terms of receiving it. The license does not permit unauthorized actions with the software. The text outlines the terms and conditions for using Apple Software. It states that the software cannot be installed, used, or run on non-Apple-branded computers. The software and its documentation are defined as \"Commercial Products\" and \"Commercial Computer Software\". Users are granted a limited, non-transferable, non-exclusive license to download, install, use, and run one copy of the software for personal, non-commercial use, subject to the Services and Content Usage Rules set forth in the Apple Media Services Terms and Conditions. If the software is governed by a separate license, the terms of that license will apply. The intellectual property rights of any content accessed through Apple Software belong to the respective content owner. Certain features of the Apple Software may automatically download and cache eligible content on an Apple-branded computer running the software. To use these features, users need the latest version of the operating software, an Apple Account associated with an iCloud account in good standing, and internet or cellular data access. Ownership of the Apple Software remains with Apple or its licensors, who reserve all rights not explicitly granted. The text outlines the terms of a license for Apple Software obtained from the Mac App Store or through a software update. The license allows for a one-time permanent transfer of all rights to the software to another party, under certain conditions: the software must be transferred with Apple-branded hardware, and the transfer must include all of the software. The software will periodically check with Apple for updates. For UK-based consumers, the license is governed by local laws. The license also prohibits actions such as reverse engineering, disassembling, decrypting, modifying, or creating derivative works of the software, unless such restrictions are prohibited by law. The text discusses licensing terms for Apple Software, particularly those that include Open-Sourced Components and AVC encoding/decoding functionality. Commercial use of H.264/AVC requires additional licensing from MPEG LA, LLC. Users can remove cached Apple Eligible Content and disable Content Caching Features. Some services may require an Apple Account and acceptance of additional terms. Users must also confirm they are not located in certain restricted countries. This license agreement between the user and Apple pertains to the Apple Software and supersedes all previous understandings. It details how user information is sent to Apple and its potential uses. The Apple Software allows access to various Apple and third-party services, including the iTunes Store, Mac App Store, Apple Books, Game Center, iCloud, Maps, News, and others. The license also covers the terms and conditions for remotely connecting from a device to an Apple-branded computer running the Apple Software, referred to as the \"Home Mac\". The text refers to a license agreement for Apple Software. The license allows users to install and run the software on each Apple-branded computer they own or control, running specific versions of macOS. Commercial enterprises or educational institutions are permitted to use one copy of the software. The license also allows users to use virtualized copies of the software in connection with certain services. However, the use of System Characters is restricted. The Gracenote Software and Gracenote Data are licensed \"AS IS\". Any changes to the agreement must be in writing and signed by Apple. Users are also required to use the Content Caching Features for personal use only. The text outlines the terms of a license agreement for non-commercial or internal use of Apple software. The user acknowledges that Apple is not responsible for the software's configuration. The license also covers any changes to the software provided by Apple. The software can be used by a consumer for personal and non-commercial activities, such as encoding and decoding AVC video. The user is allowed to make one copy of the software, excluding certain embedded firmware. The agreement does not limit any rights of Apple or other copyright owners. Apple allows users to backup their firmware embedded in Apple-branded hardware in machine-readable form, but the backup must include all copyright. Apple and its licensors can change, suspend, remove, or disable access to any Apple Software and Services without notice. Third-party software and services may not be compatible with Apple Software and its installation may affect their availability and usability. Users are responsible for maintaining the security of their Mac Computer, Supported Devices, Apple Account, Touch ID information, device passcodes, and other authentication credentials. The text discusses the use of personal authentication credentials in connection with services. It mentions the use of Adobe Color Profile software included with Apple software, but Adobe has no obligation to provide support. If a user chooses to pay in installments or purchase and pay later with a third party using Apple Pay, they are contracting with the third party. The user can use the fonts included with the Apple software to display and print content, but can only embed fonts in content if permitted by the embedding restrictions. The text discusses the terms of Apple's software license. It mentions that fonts can be embedded in content if allowed by the restrictions. Apple provides updates to its software, but is not a financial institution. The license also covers leasing for permitted developer services. Location data provided by Apple software is intended for basic navigation and planning purposes only. The Apple software cannot be exported or re-exported unless authorized by U.S. law and the laws of the jurisdiction where the software was obtained. Apple may provide access to third-party software or services as part of its software package, upgrades, updates, or supplements. Users are not allowed to exploit these services. The text is a part of a legal agreement, likely from Apple, stating that users are not allowed to exploit Gracenote data, software, or servers unless expressly permitted. It warns that sharing credentials with a third party could lead to unauthorized transactions. Users are also prohibited from using Apple software and services to spam, defraud, harass, or violate the rights of others. Apple is not responsible for any misuse by the user or any offensive messages received as a result of using their software and services. The text also mentions a link to Apple's legal sales support and implies a return policy. The text outlines terms and conditions for Apple's services and software. It states that to obtain a refund, the entire hardware/software package must be returned. Redistribution of Flight Data requires prior written consent from the FAA. It also clarifies that Apple does not guarantee uninterrupted or error-free service, continued availability, correction of defects, or compatibility with third-party software or services. Some software libraries and third-party software included with Apple Software are free and licensed under the GNU General Public License (GPL) or the GNU Library. The text discusses software licensed under the GNU General Public License (GPL) or the GNU Library/Lesser General Public License (LGPL). It specifies that the software documentation is licensed to U.S. Government end users only as commercial products and with the same rights as other end users. The text also states that Apple, Apple Payment Services, and their affiliates do not assume liability for any commerce activity made using Apple Pay or Wallet. Instead, users are advised to refer to their agreements with their card issuer, payment network, or merchant for any questions or disputes related to their supported cards and associated commerce activities. The text discusses any questions or disputes related to supported cards and associated commerce activities. It specifies that customers who are consumers (those using the Apple Software outside of their trade, business, or profession) may have legal rights in their country of residence that could prevent certain limitations from applying to them. A lessee subleasing the Apple Software, who is also considered a lessor under Section 3, must fully relinquish exclusive use for use only in a list of specified countries and regions. The text discusses the terms of use for Apple-branded software. It states that the software provided with a specific Apple-branded hardware product may not be compatible with other Apple-branded hardware models. The software can be used in virtual operating system environments, but only a single instance or copy can be virtualized. The Lessor, or provider, is responsible for ensuring that the Lessee, or user, complies with all license terms and assists Apple in enforcing compliance. The text also prohibits interference with Apple's verification, storage, or authentication mechanisms. The countries listed at the beginning of the text are not connected to the rest of the information. The text discusses the security mechanisms implemented by Apple in its software, services, and content. It mentions digital rights management and signing among these mechanisms. The text also refers to Apple's Wallet service, which is provided with certain implied or statutory warranties and conditions, including merchantability, satisfactory quality, fitness for a particular purpose, accuracy, quiet enjoyment, and non-infringement of third-party rights. Information will be sent to Apple to determine eligibility and prevent fraud. The Wallet service is subject to terms and conditions, which can be accessed through the device's settings.\n"
     ]
    }
   ],
   "source": [
    "# RAG Workflow\n",
    "def rag_summarization_with_chunks(uri, user, password):\n",
    "    # retrieve descriptions\n",
    "    kg = KnowledgeGraph(uri, user, password)\n",
    "    summaries = kg.get_all_summaries()\n",
    "    kg.close()\n",
    "\n",
    "    if not summaries:\n",
    "        print(\"No summaries found in the knowledge graph.\")\n",
    "        return \"No summaries found in the knowledge graph.\"\n",
    "\n",
    "    unique_summaries = list(set(summaries))\n",
    "    # print(\"Unique Descriptions Retrieved:\")\n",
    "    # for idx, summary in enumerate(unique_summaries, start=1):\n",
    "    #     print(f\"{idx}: {summary}\")\n",
    "\n",
    "    llm = ChatOpenAI(temperature=0, model_name=\"gpt-4\")\n",
    "    final_summary = summarize_in_chunks(unique_summaries, llm)\n",
    "    return final_summary\n",
    "\n",
    "# Perform RAG summarization with chunking\n",
    "apple_par_summary = rag_summarization_with_chunks(uri, user, password)\n",
    "print(\"\\nFinal Summary:\")\n",
    "print(apple_par_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Summary:\n",
      "- **Apple Software Usage**\n",
      "  - The Apple Software can be used on supported Mac Computers.\n",
      "  - Users agree to automatic downloads and installations of Apple Software Changes on their computers and peripheral devices.\n",
      "\n",
      "- **Volume or Maintenance License Program**\n",
      "  - The terms of the license determine the number of copies of the software that can be downloaded, installed, used, and run on Apple-branded hardware if the Apple Software is obtained under a volume or maintenance license program.\n",
      "  - The product is licensed under the MPEG-4 Visual Patent Portfolio License for personal and non-commercial use by a consumer.\n",
      "  - The license allows encoding and decoding of MPEG-4 video in compliance with the MPEG-4 Visual Standard and obtained from a video provider licensed by MPEG LA.\n",
      "\n",
      "- **Transfer of Software Between Apple-branded Computers**\n",
      "  - Use Setup/Migration Assistant for software transfer.\n",
      "  - Continued use of transferred software is allowed.\n",
      "\n",
      "- **Gracenote Software Functionality**\n",
      "  - Enables disc and/or file identification and obtains music-related information from online servers, including name, artist, track, and title (Gracenote Data).\n",
      "\n",
      "- **Provision of MPEG-4 Video**\n",
      "  - The software can provide MPEG-4 video.\n",
      "\n",
      "- **Termination of License and Use of Gracenote Servers**\n",
      "  - If your license terminates, you must stop using Gracenote Data, Gracenote Software, and Gracenote Servers.\n",
      "\n",
      "- **Suspension of Virtual Supported Cards**\n",
      "  - The ability to pay with virtual Supported Cards on your Mac Computer can be suspended by putting it into Lost Mode.\n",
      "\n",
      "- **Completion of Purchase**\n",
      "  - Purchases can be completed on your Mac Computer with a Supported Card associated with an active iCloud account.\n",
      "\n",
      "- **Information Usage**\n",
      "  - The information sent to Apple may be used in certain ways.\n",
      "\n",
      "- **Acknowledgement of Risks**\n",
      "  - Users must acknowledge that any failure or damage to Apple hardware resulting from modification of the Open-Sourced Components of the Apple Software is not covered by the Apple hardware warranty.\n",
      "\n",
      "- **Payment Information**\n",
      "  - On a MacBook Pro with built-in Touch ID, payment information will be provided in an encrypted format.\n",
      "\n",
      "- **Adobe Color Profile Software**\n",
      "  - Distributed with Apple Software and available for download from Adobe's website.\n",
      "\n",
      "- **Apple Pay Feature**\n",
      "  - Transactions are subject to the merchant's terms and conditions.\n",
      "\n",
      "- **Potential Risks**\n",
      "  - Risks include loss of profits, corruption or loss of data, failure to transmit or receive any data or information, including course instructions.\n",
      "\n",
      "- **Commercial Damages and Losses**\n",
      "  - Damages and losses can arise from use or inability to use Apple software or services, and also related to third-party software, applications, or services used with Apple software or services.\n",
      "\n",
      "- **Countries Mentioned and Supporting Wallet and Apple Pay**\n",
      "  - Countries mentioned include Zealand, Nicaragua, Norway, Panama, Peru, Philippines, Poland, Portugal, Puerto Rico, Romania, Singapore, Slovakia, Slovenia, South Korea, Spain, St. Lucia, St. Vincent, Sweden, Switzerland, Thailand, The Bahamas, Trinidad and Tobago, Turkey, UK, Uruguay, US, and Venezuela.\n",
      "\n",
      "- **Function of Wallet**\n",
      "  - Wallet allows storage of virtual representations of credit, debit, and prepaid cards, referred to as \"Supported Cards\", which can be used with Apple Pay.\n",
      "\n",
      "- **Using Apple Pay on Non-Safari Browsers**\n",
      "  - When using Apple Pay on a desktop web browser other than Safari, a Supported Card is required.\n",
      "\n",
      "- **Provisioning of Apple Software**\n",
      "  - The software is provisioned to supported iPhone or iPad devices.\n",
      "\n",
      "- **Consumer Rights in Australia**\n",
      "  - The license does not affect or intend to affect statutory rights under the Australian Consumer Law, including consumer guarantees.\n",
      "\n",
      "- **Apple Software Acknowledgements, Licensing Terms, and Disclaimers**\n",
      "  - These are contained in the electronic documentation for the Apple Software and are governed by their respective terms.\n",
      "\n",
      "- **Provisioning a Supported Corporate Card**\n",
      "  - The user represents that they are provisioning a supported corporate card with the authorization of their employer and is authorized to bind their employer to the Apple Pay & Wallet Terms.\n",
      "\n",
      "- **Apple Software Components**\n",
      "  - Certain components of the Apple Software, including third-party open source programs, have been or may be made available by Apple on its Open Source website.\n",
      "\n",
      "- **Use of Content Caching Features and Storage of Apple Eligible Content**\n",
      "  - User acknowledges and agrees to the terms of use and no additional rights are transferred to the user beyond those granted in the license terms.\n",
      "\n",
      "- **Conditions for Apple Software and Services**\n",
      "  - Conditions include use of Apple Pay or Wallet, decisions made by issuer, merchant, or third party related to a Supported Card, accrual or redemption of rewards or stored value associated with user's account, and funding or reloading of prepaid Supported Cards.\n",
      "\n",
      "- **Apple Eligible Content**\n",
      "  - Governed by specific license terms.\n",
      "\n",
      "- **Apple Software**\n",
      "  - User rights automatically terminate if applicable Apple license terms are violated and lessee's use of Apple Software must be immediately terminated upon discovery of violation.\n",
      "\n",
      "- **Breach and Notice from Apple**\n",
      "  - The text mentions a breach or a written notice from Apple regarding such a breach.\n",
      "\n",
      "- **Gracenote Software and Servers**\n",
      "  - No warranty is provided that the Gracenote Software or Gracenote Servers are error-free or that their functioning will be uninterrupted.\n",
      "\n",
      "- **Live Captions by Apple Software**\n",
      "  - The Apple Software\n"
     ]
    }
   ],
   "source": [
    "# RAG Workflow\n",
    "def rag_summarization_with_points(uri, user, password):\n",
    "    # retrieve descriptions\n",
    "    kg = KnowledgeGraph(uri, user, password)\n",
    "    summaries = kg.get_all_summaries()\n",
    "    kg.close()\n",
    "\n",
    "    if not summaries:\n",
    "        print(\"No summaries found in the knowledge graph.\")\n",
    "        return \"No summaries found in the knowledge graph.\"\n",
    "\n",
    "    unique_summaries = list(set(summaries))\n",
    "    # print(\"Unique Descriptions Retrieved:\")\n",
    "    # for idx, summary in enumerate(unique_summaries, start=1):\n",
    "    #     print(f\"{idx}: {summary}\")\n",
    "\n",
    "    llm = ChatOpenAI(temperature=0, model_name=\"gpt-4\")\n",
    "    final_summary = summarize_in_points(unique_summaries, llm)\n",
    "    return final_summary\n",
    "\n",
    "# Perform RAG summarization with chunking\n",
    "apple_points_summary = rag_summarization_with_points(uri, user, password)\n",
    "print(\"\\nFinal Summary:\")\n",
    "print(apple_points_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('apple_bullet_points.txt', \"w\") as file:\n",
    "        file.write(apple_points_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apple_points_summary2 = rag_summarization_with_points(uri, user, password)\n",
    "# print(\"\\nFinal Summary:\")\n",
    "# print(apple_points_summary2)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.21 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  },
  "vscode": {
   "interpreter": {
    "hash": "397704579725e15f5c7cb49fe5f0341eb7531c82d19f2c29d197e8b64ab5776b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
