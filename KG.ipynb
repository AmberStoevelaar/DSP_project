{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.9.21\n"
     ]
    }
   ],
   "source": [
    "from platform import python_version\n",
    "\n",
    "print(python_version())\n",
    "\n",
    "# %pip install langchain\n",
    "# %pip install -U langchain-community\n",
    "# %pip install pypdf\n",
    "# %pip install spacy\n",
    "# !python3.9 -m pip install spacy-llm\n",
    "# !python3.9 -m spacy download en_core_web_sm\n",
    "# %pip install scikit-learn\n",
    "# %pip install -U sentence-transformers\n",
    "# %pip install keybert\n",
    "# %pip install transformers sentence-transformers nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import langchain\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.docstore.document import Document\n",
    "import os\n",
    "\n",
    "# Directory containing your PDF files\n",
    "directory_path = 'TCs'\n",
    "\n",
    "# Initialize PyPDFLoader for each PDF in the directory\n",
    "loaders = [PyPDFLoader(os.path.join(directory_path, f)) for f in os.listdir(directory_path) if f.endswith('.pdf')]\n",
    "\n",
    "# Load documents from PDFs\n",
    "tc_docs = []\n",
    "for loader in loaders:\n",
    "    tc_docs.extend(loader.load())\n",
    "\n",
    "# Prepare the content and metadata for each news article as Document objects\n",
    "tc_data = [\n",
    "    Document(\n",
    "        page_content=doc.page_content,  # Assuming this is how you access the page content of the document\n",
    "        metadata={\n",
    "            \"source\": doc.metadata['source'].removeprefix('TCs'),  # Assuming this is the metadata format\n",
    "        }\n",
    "    )\n",
    "    for doc in tc_docs  # Assuming news_docs is a list of objects with page_content and metadata\n",
    "]\n",
    "\n",
    "data = tc_data[0].page_content\n",
    "# print(type(news_articles_data))\n",
    "\n",
    "page_contents = [doc.page_content for doc in tc_docs]\n",
    "combined = \"\".join(page_contents) \n",
    "type(combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500, chunk_overlap=60\n",
    ")\n",
    "# split_docs = text_splitter.split_text(data)\n",
    "split_docs = text_splitter.split_text(combined)\n",
    "print(len(split_docs))\n",
    "\n",
    "# function for functionality of pipeline\n",
    "def split_text(combined_text):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=500,\n",
    "        chunk_overlap=60\n",
    "    )\n",
    "    return text_splitter.split_text(combined_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entity & Relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/spacy_llm/pipeline/llm.py:143: UserWarning: Task supports sharding, but model does not provide context length. Data won't be sharded, prompt might exceed the model's context length. Set context length in your config. If you think spacy-llm should provide the context length for this model automatically, report this to https://github.com/explosion/spacy-llm/issues.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entity counts: Counter({'ORG': 263, 'GPE': 81, 'CARDINAL': 20, 'PERSON': 20,\n",
      "'ORDINAL': 17, 'LAW': 13, 'WORK_OF_ART': 10, 'PRODUCT': 1, 'DATE': 1, 'EVENT':\n",
      "1, 'TIME': 1})\n",
      "Relation counts: Counter({'is in': 43, 'related_to': 20, 'uses': 16, 'provides':\n",
      "14, 'is a type of': 9, 'supports': 6, 'includes': 6, 'runs_on': 5, 'is_a': 5,\n",
      "'associated_with': 5, 'refers_to': 5, 'has access to': 5, 'runs': 3,\n",
      "'affiliated_with': 3, 'contains': 3, 'included_with': 3, 'obtained_from': 2,\n",
      "'related to': 2, 'branded_by': 2, 'part_of': 2, 'governs': 2, 'involves': 2, 'is\n",
      "the same as': 2, 'licensed by': 2, 'compatible_with': 2, 'affects': 2,\n",
      "'provides_access_to': 2, 'requires': 2, 'does not endorse': 2, 'authorized by':\n",
      "2, 'owns': 1, 'governed by': 1, 'provided by': 1, 'accompanied by': 1, 'subject\n",
      "to': 1, 'set forth in': 1, 'ownership': 1, 'produced_by': 1, 'supported_on': 1,\n",
      "'downloads': 1, 'delivers': 1, 'delivered through': 1, 'has_license': 1,\n",
      "'stored_on': 1, 'has_rights_over': 1, 'use': 1, 'is used on': 1, 'is a part of':\n",
      "1, 'prohibits': 1, 'brand': 1, 'transfers software to': 1, 'offers': 1,\n",
      "'provides support for': 1, 'modification of': 1, 'is part of': 1, 'checks_with':\n",
      "1, 'owned_by': 1, 'agrees to': 1, 'installed on': 1, 'defines': 1, 'subleasing':\n",
      "1, 'complies_with': 1, 'agrees_to': 1, 'enforces_compliance': 1, 'used by': 1,\n",
      "'notifies': 1, 'regulates': 1, 'references': 1, 'responsibility': 1,\n",
      "'licensed_from': 1, 'provided_by': 1, 'provides_update': 1, 'branded': 1,\n",
      "'enables access to': 1, 'owned by': 1, 'cease_use': 1, 'disclaims': 1,\n",
      "'authorized representative of': 1, 'applies to': 1, 'perpetrates': 1, 'obtained\n",
      "under': 1, 'licensed to': 1, 'relates_to': 1, 'located_in': 1, 'licensed under':\n",
      "1, 'encoded by': 1, 'consent_required': 1, 'not liable': 1,\n",
      "'no_obligation_to_support': 1, 'distributed_with': 1, 'available_from': 1,\n",
      "'stored_in': 1, 'licensed_to': 1, 'is_feature_of': 1, 'is_type_of': 1,\n",
      "'is_part_of': 1, 'provides_service': 1, 'applies_to': 1, 'contracts_with': 1,\n",
      "'disclaims liability': 1, 'affiliation': 1})\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import spacy\n",
    "from collections import Counter\n",
    "from pathlib import Path\n",
    "from wasabi import msg\n",
    "from spacy_llm.util import assemble\n",
    "\n",
    "import spacy\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "# traditional spacy NER\n",
    "# def split_document_sent(text):\n",
    "#     nlp = spacy.load(\"en_core_web_sm\")\n",
    "#     doc = nlp(text)\n",
    "#     return [sent.text.strip() for sent in doc.sents] # referencial\n",
    "\n",
    "def summarize_section(text):\n",
    "    # Load SpaCy model\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    doc = nlp(text)\n",
    "\n",
    "    # Extract sentences\n",
    "    sentences = [sent.text.strip() for sent in doc.sents if sent.text.strip()]\n",
    "\n",
    "    # Check if there are meaningful sentences\n",
    "    if not sentences:\n",
    "        return \"No summary available\"\n",
    "\n",
    "    try:\n",
    "        # Compute sentence embeddings using TF-IDF\n",
    "        vectorizer = TfidfVectorizer(stop_words='english')\n",
    "        sentence_vectors = vectorizer.fit_transform(sentences).toarray()\n",
    "\n",
    "        # Calculate average vector and similarity\n",
    "        avg_vector = np.mean(sentence_vectors, axis=0).reshape(1, -1)\n",
    "        similarities = cosine_similarity(avg_vector, sentence_vectors)\n",
    "\n",
    "        # Select the most relevant sentence\n",
    "        most_relevant_index = np.argmax(similarities)\n",
    "        return sentences[most_relevant_index]\n",
    "\n",
    "    except ValueError:  # Handle cases with stop-words-only or empty input\n",
    "        return \"No summary available\"\n",
    "\n",
    "# spacy-llm relationship extraction\n",
    "# def process_text(nlp, text, verbose=False):\n",
    "#     doc = nlp(text)\n",
    "#     if verbose:\n",
    "#         msg.text(f\"Text: {doc.text}\")\n",
    "#         msg.text(f\"Entities: {[(ent.text, ent.label_) for ent in doc.ents]}\")\n",
    "#         msg.text(\"Relations:\")\n",
    "#         for r in doc._.rel:\n",
    "#             msg.text(f\"  - {doc.ents[r.dep]} [{r.relation}] {doc.ents[r.dest]}\")\n",
    "#     return doc\n",
    "\n",
    "def run_pipeline(combined, config_path, examples_path=None, verbose=False):\n",
    "    if not os.getenv(\"OPENAI_API_KEY\"):\n",
    "        msg.fail(\"OPENAI_API_KEY env variable was not found. Set it and try again.\", exits=1)\n",
    "\n",
    "    sections = split_text(combined)\n",
    "    nlp = assemble(config_path, overrides={} if examples_path is None else {\"paths.examples\": str(examples_path)})\n",
    "\n",
    "    # Initialize counters and storage\n",
    "    processed_data = []\n",
    "    entity_counts = Counter()\n",
    "    relation_counts = Counter()\n",
    "\n",
    "    for section in sections:\n",
    "        summary = summarize_section(section)\n",
    "        doc = nlp(summary)\n",
    "        entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
    "        relations = [(doc.ents[r.dep].text, r.relation, doc.ents[r.dest].text) for r in doc._.rel]\n",
    "\n",
    "        # Store processed data\n",
    "        processed_data.append({\n",
    "            'original_text': section,\n",
    "            'summary': summary,\n",
    "            'entities': entities,\n",
    "            'relations': relations})\n",
    "\n",
    "        # Update counters\n",
    "        entity_counts.update([ent[1] for ent in entities])\n",
    "        relation_counts.update([rel[1] for rel in relations])\n",
    "\n",
    "    # Export to JSON\n",
    "    with open('processed_data.json', 'w') as f:\n",
    "        json.dump(processed_data, f)\n",
    "\n",
    "    # Display summary\n",
    "    msg.text(f\"Entity counts: {entity_counts}\")\n",
    "    msg.text(f\"Relation counts: {relation_counts}\")\n",
    "\n",
    "# Set your configuration paths and flags\n",
    "config_path = Path(\"config.cfg\")\n",
    "examples_path = None  # or None if not using few-shot\n",
    "verbose = True\n",
    "\n",
    "# Run the pipeline\n",
    "file = run_pipeline(combined, config_path, None, verbose)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Addition of description per chunk of text as well as simplification "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To the extent that this software may be used to reproduce, modify, publish or \n",
      "distribute materials, it is licensed to you only for reproduction, modiï¬cation, publication and \n",
      "distribution of non-copyrighted materials, materials in which you own the copyright, or materials \n",
      "you are authorized or legally permitted to reproduce, modify, publish or distribute.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'IMPORTANT NOTE: To the extent that this software may be used to reproduce, modify, publish or \\ndistribute materials, it is licensed to you only for reproduction, modiï¬cation, publication and \\ndistribution of non-copyrighted materials, materials in which you own the copyright, or materials \\nyou are authorized or legally permitted to reproduce, modify, publish or distribute. If you are \\nuncertain about your right to copy, modify, publish or distribute any material, you should contact'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "def summarize_section(text):\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    doc = nlp(text)\n",
    "    sentences = [sent.text for sent in doc.sents]\n",
    "\n",
    "    vectorizer = TfidfVectorizer(stop_words='english')\n",
    "    # convert to array in order to perform cosine similarity\n",
    "    sentence_vectors = vectorizer.fit_transform(sentences).toarray()\n",
    "\n",
    "    avg_vector = np.mean(sentence_vectors, axis=0)\n",
    "    avg_vector = avg_vector.reshape(1, -1)\n",
    "    similarities = cosine_similarity(avg_vector, sentence_vectors)\n",
    "\n",
    "    # index with highest cosine similarity to the average vector\n",
    "    most_relevant_index = np.argmax(similarities)\n",
    "\n",
    "    return sentences[most_relevant_index]\n",
    "\n",
    "\n",
    "print(summarize_section(split_docs[2]))\n",
    "split_docs[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simplification with Lexical Simplification with BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/meryjoy/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "BertForMaskedLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From ðŸ‘‰v4.50ðŸ‘ˆ onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simplified Text: \n",
      "The user shall be deemed to have consent to the terms and conditions , referred to as the agreement. You agree to be bound by the contract under this contract.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForMaskedLM, AutoTokenizer\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import numpy as np\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Load models\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\", trust_remote_code=True)\n",
    "model = AutoModelForMaskedLM.from_pretrained(\"bert-base-uncased\", trust_remote_code=True)\n",
    "similarity_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "def mask_word(sentence, word):\n",
    "    \"\"\"Replace a word in the sentence with [MASK].\"\"\"\n",
    "    return sentence.replace(word, \"[MASK]\")\n",
    "\n",
    "def generate_candidates(masked_sentence, top_k=5):\n",
    "    \"\"\"Generate replacement candidates using MLM.\"\"\"\n",
    "    inputs = tokenizer(masked_sentence, return_tensors=\"pt\")\n",
    "    outputs = model(**inputs)\n",
    "    logits = outputs.logits\n",
    "\n",
    "    mask_token_index = (inputs.input_ids == tokenizer.mask_token_id).nonzero(as_tuple=True)[1]\n",
    "    mask_logits = logits[0, mask_token_index, :]\n",
    "    top_tokens = mask_logits.topk(top_k, dim=-1).indices[0].tolist()\n",
    "\n",
    "    candidates = [tokenizer.decode([token]).strip() for token in top_tokens]\n",
    "    return candidates\n",
    "\n",
    "def rank_candidates(sentence, word, candidates):\n",
    "    \"\"\"Rank candidates based on semantic similarity and simplicity.\"\"\"\n",
    "    original_embedding = similarity_model.encode(sentence)\n",
    "    rankings = []\n",
    "\n",
    "    for candidate in candidates:\n",
    "        replaced_sentence = sentence.replace(word, candidate)\n",
    "        candidate_embedding = similarity_model.encode(replaced_sentence)\n",
    "        similarity = util.cos_sim(original_embedding, candidate_embedding)[0][0].item()\n",
    "        rankings.append((candidate, similarity))\n",
    "\n",
    "    # Sort by similarity and word length (shorter words preferred for simplicity)\n",
    "    rankings.sort(key=lambda x: (-x[1], len(x[0])))\n",
    "    return [candidate for candidate, _ in rankings]\n",
    "\n",
    "def simplify_text(text, complexity_threshold=5):\n",
    "    \"\"\"Simplify text by replacing complex words.\"\"\"\n",
    "    simplified_sentences = []\n",
    "    sentences = sent_tokenize(text)\n",
    "\n",
    "    for sentence in sentences:\n",
    "        words = word_tokenize(sentence)\n",
    "        for word in words:\n",
    "            if len(word) > complexity_threshold:  # Detect complex words by length\n",
    "                masked_sentence = mask_word(sentence, word)\n",
    "                candidates = generate_candidates(masked_sentence)\n",
    "                ranked_candidates = rank_candidates(sentence, word, candidates)\n",
    "\n",
    "                if ranked_candidates:\n",
    "                    sentence = sentence.replace(word, ranked_candidates[0], 1)\n",
    "        simplified_sentences.append(sentence)\n",
    "\n",
    "    return \" \".join(simplified_sentences)\n",
    "\n",
    "# Example usage\n",
    "text = \"\"\"\n",
    "The user shall be deemed to have consented to the terms and conditions herein referred to as the agreement. \n",
    "You agree to be bound by the obligations under this contract.\n",
    "\"\"\"\n",
    "\n",
    "simplified_text = simplify_text(text)\n",
    "print(\"Simplified Text:\", simplified_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Still work ahead for this part of the project, might be for later, not enough time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Cypher Queries with eFLINT notation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_and_generate_queries2(json_data, file_path):\n",
    "    nodes = {}\n",
    "    relationships = []\n",
    "\n",
    "    # Enhanced mapping function for entity types based on eFLINT elements\n",
    "    def map_eflint_type(entity_type, entity_name):\n",
    "        # we are not really interested in numbers so we delete those\n",
    "        # in the case of links it is better that we classify them as FACT rather than ORG\n",
    "        if 'http://' in entity_type or 'https://' in entity_name:\n",
    "            return 'FACT'\n",
    "\n",
    "        if entity_type in ['CARDINAL', 'ORDINAL']:\n",
    "            return 'NUMBER'\n",
    "\n",
    "\n",
    "        if entity_type == 'ORG':\n",
    "            if 'terms' in entity_name.lower() or 'conditions' in entity_name.lower() or 'agreement' in entity_name.lower():\n",
    "                return 'DUTY'\n",
    "            else:\n",
    "                return 'ACTOR'\n",
    "\n",
    "        if 'section' in entity_name.lower() or 'section' in entity_type.lower():\n",
    "            return 'SECTION'\n",
    "        \n",
    "        mapping = {\n",
    "            'PERSON': 'ACTOR',\n",
    "            'EVENT': 'EVENT',\n",
    "            'LAW': 'DUTY',\n",
    "            'WORK_OF_ART': 'ACT',\n",
    "            'CONDITION': 'CONDITION',\n",
    "            'DATE': 'DATE',\n",
    "            # they sadly do not appear in the NER we performed but we can nevertheless include them to keep the eFLINT semantics\n",
    "            'CLAIMANT': 'CLAIMANT',\n",
    "            'HOLDER': 'HOLDER'\n",
    "        }\n",
    "        return mapping.get(entity_type, 'FACT') \n",
    "\n",
    "    # entities and relationships\n",
    "    for item in json_data:\n",
    "        entities = item.get('entities', [])\n",
    "        relations = item.get('relations', [])\n",
    "        context_text = item.get('context', '')  # Text surrounding the entities\n",
    "\n",
    "        # Generate section summary for the context\n",
    "        section_summary = summarize_section(context_text) if context_text else \"No summary available\"\n",
    "        \n",
    "        for entity in entities:\n",
    "            entity_name, entity_type = entity[:2]\n",
    "            mapped_type = map_eflint_type(entity_type, entity_name)\n",
    "            node_id = f\"{entity_name.replace(' ', '_')}_{mapped_type}\"\n",
    "            nodes[node_id] = {\n",
    "                'name': entity_name, \n",
    "                'type': mapped_type,\n",
    "                'description': section_summary  # Add the summary as a description\n",
    "            }\n",
    "        \n",
    "        for relation in relations:\n",
    "            src_id = f\"{relation[0].replace(' ', '_')}_{map_eflint_type(entities[0][1], entities[0][0])}\"\n",
    "            tgt_id = f\"{relation[2].replace(' ', '_')}_{map_eflint_type(entities[0][1], entities[0][0])}\"\n",
    "            relationship_type = relation[1].replace(' ', '_').replace('-', '_')\n",
    "            relationships.append((src_id, relationship_type, tgt_id))\n",
    "\n",
    "    node_queries = [\n",
    "        f\"MERGE (n:{data['type']} {{name: '{data['name']}', description: '{data['description']}'}}) SET n.id = '{node_id}'\"\n",
    "        for node_id, data in nodes.items() if data['type'] != 'NUMBER'\n",
    "    ]\n",
    "\n",
    "    relationship_queries = [\n",
    "        f\"MATCH (a), (b) WHERE a.id = '{rel[0]}' AND b.id = '{rel[2]}' \"\n",
    "        f\"MERGE (a)-[:{rel[1]}]->(b)\"\n",
    "        for rel in relationships\n",
    "    ]\n",
    "\n",
    "    queries = node_queries + relationship_queries\n",
    "    # Save queries to a text file\n",
    "    with open(file_path, 'w') as file:\n",
    "        for query in queries:\n",
    "            file.write(query + '\\n')\n",
    "\n",
    "    return queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_and_generate_queries(json_data, file_path):\n",
    "    nodes = {}\n",
    "    relationships = []\n",
    "\n",
    "    # Enhanced mapping function for entity types based on eFLINT elements\n",
    "    def map_eflint_type(entity_type, entity_name):\n",
    "        if 'http://' in entity_type or 'https://' in entity_name:\n",
    "            return 'FACT'\n",
    "        if entity_type in ['CARDINAL', 'ORDINAL']:\n",
    "            return 'NUMBER'\n",
    "        if entity_type == 'ORG':\n",
    "            if 'terms' in entity_name.lower() or 'conditions' in entity_name.lower() or 'agreement' in entity_name.lower():\n",
    "                return 'DUTY'\n",
    "            else:\n",
    "                return 'ACTOR'\n",
    "        if 'section' in entity_name.lower() or 'section' in entity_type.lower():\n",
    "            return 'SECTION'\n",
    "        mapping = {\n",
    "            'PERSON': 'ACTOR',\n",
    "            'EVENT': 'EVENT',\n",
    "            'LAW': 'DUTY',\n",
    "            'WORK_OF_ART': 'ACT',\n",
    "            'CONDITION': 'CONDITION',\n",
    "            'DATE': 'DATE',\n",
    "        }\n",
    "        return mapping.get(entity_type, 'FACT')\n",
    "\n",
    "    # Process summaries and associate them with entities\n",
    "    for item in json_data:\n",
    "        summary = item.get('summary', 'No summary available')\n",
    "        entities = item.get('entities', [])\n",
    "        relations = item.get('relations', [])\n",
    "\n",
    "        for entity in entities:\n",
    "            entity_name, entity_type = entity[:2]\n",
    "            mapped_type = map_eflint_type(entity_type, entity_name)\n",
    "            node_id = f\"{entity_name.replace(' ', '_')}_{mapped_type}\"\n",
    "            if node_id not in nodes:\n",
    "                nodes[node_id] = {\n",
    "                    'name': entity_name,\n",
    "                    'type': mapped_type,\n",
    "                    'descriptions': [summary],  # Start with the current summary\n",
    "                }\n",
    "            else:\n",
    "                nodes[node_id]['descriptions'].append(summary)  # Append additional summaries\n",
    "\n",
    "        # Add relationships if present\n",
    "        for relation in relations:\n",
    "            src_id = f\"{relation[0].replace(' ', '_')}_{map_eflint_type(entity_type, relation[0])}\"\n",
    "            tgt_id = f\"{relation[2].replace(' ', '_')}_{map_eflint_type(entity_type, relation[2])}\"\n",
    "            relationship_type = relation[1].replace(' ', '_').replace('-', '_')\n",
    "            relationships.append((src_id, relationship_type, tgt_id))\n",
    "\n",
    "    # Create Cypher queries for nodes\n",
    "    node_queries = [\n",
    "        f\"\"\"\n",
    "        MERGE (n:{data['type']} {{name: '{data['name']}'}})\n",
    "        SET n.id = '{node_id}', n.descriptions = {json.dumps(data['descriptions'])}\n",
    "        \"\"\"\n",
    "        for node_id, data in nodes.items() if data['type'] != 'NUMBER'\n",
    "    ]\n",
    "\n",
    "    # Create Cypher queries for relationships\n",
    "    relationship_queries = [\n",
    "        f\"\"\"\n",
    "        MATCH (a), (b)\n",
    "        WHERE a.id = '{rel[0]}' AND b.id = '{rel[2]}'\n",
    "        MERGE (a)-[:{rel[1]}]->(b)\n",
    "        \"\"\"\n",
    "        for rel in relationships\n",
    "    ]\n",
    "\n",
    "    queries = node_queries + relationship_queries\n",
    "\n",
    "    # Save queries to a text file\n",
    "    with open(file_path, 'w') as file:\n",
    "        for query in queries:\n",
    "            file.write(query + '\\n')\n",
    "\n",
    "    return queries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('processed_data.json', 'r') as file:\n",
    "    json_data = json.load(file)\n",
    "\n",
    "queries = classify_and_generate_queries(json_data, 'cypher_queries.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neo4j import GraphDatabase\n",
    "\n",
    "def execute_queries(queries, uri, user, password):\n",
    "    driver = GraphDatabase.driver(uri, auth=(user, password))\n",
    "    with driver.session() as session:\n",
    "        for query in queries:\n",
    "            session.run(query)\n",
    "    driver.close()\n",
    "\n",
    "# Example usage\n",
    "uri = \"neo4j://localhost:7687\"\n",
    "user = \"neo4j\"\n",
    "password = \"movies11\"\n",
    "\n",
    "execute_queries(queries, uri, user, password)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other option: \n",
    "### Using Langchain for Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install langchain_openai\n",
    "# %pip install langchain_experimental\n",
    "# %pip install neo4j langchain-neo4j\n",
    "# %pip install yfiles_jupyter_graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.getenv(\"NEO4J_URI\"):\n",
    "    print(\"nope\")\n",
    "if not os.getenv(\"NEO4J_USERNAME\"):\n",
    "    print(\"nope\")\n",
    "if not os.getenv(\"NEO4J_PASSWORD\"):\n",
    "    print(\"nope\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import TokenTextSplitter\n",
    "text_splitter = TokenTextSplitter(chunk_size=512, chunk_overlap=24)\n",
    "tc_document = text_splitter.split_documents(tc_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_experimental.graph_transformers import LLMGraphTransformer\n",
    "\n",
    "llm=ChatOpenAI(temperature=0, model_name=\"gpt-4o-mini\")\n",
    "     \n",
    "llm_transformer = LLMGraphTransformer(llm=llm)\n",
    "\n",
    "graph_documents = llm_transformer.convert_to_graph_documents(tc_document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_neo4j import Neo4jGraph\n",
    "\n",
    "graph = Neo4jGraph()\n",
    "graph.add_graph_documents(\n",
    "    graph_documents,\n",
    "    baseEntityLabel=True,\n",
    "    include_source=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from yfiles_jupyter_graphs import GraphWidget\n",
    "from neo4j import GraphDatabase\n",
    "\n",
    "default_cypher = \"MATCH (s)-[r:!MENTIONS]->(t) RETURN s,r,t LIMIT 80\"\n",
    "\n",
    "def showGraph(cypher: str = default_cypher):\n",
    "    # create a neo4j session to run queries\n",
    "    driver = GraphDatabase.driver(\n",
    "        uri = os.environ[\"NEO4J_URI\"],\n",
    "        auth = (os.environ[\"NEO4J_USERNAME\"],\n",
    "                os.environ[\"NEO4J_PASSWORD\"]))\n",
    "    session = driver.session()\n",
    "    widget = GraphWidget(graph = session.run(cypher).graph())\n",
    "    widget.node_label_mapping = 'id'\n",
    "    display(widget)\n",
    "    return widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# showGraph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarization based RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains.llm import LLMChain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "# Neo4j Connection\n",
    "class KnowledgeGraph:\n",
    "    def __init__(self, uri, user, password):\n",
    "        from neo4j import GraphDatabase\n",
    "        self.driver = GraphDatabase.driver(uri, auth=(user, password))\n",
    "    \n",
    "    def close(self):\n",
    "        self.driver.close()\n",
    "    \n",
    "    def get_all_summaries(self):\n",
    "        with self.driver.session() as session:\n",
    "            query = \"\"\"\n",
    "            MATCH (n)\n",
    "            RETURN DISTINCT n.descriptions AS descriptions\n",
    "            \"\"\"\n",
    "            results = session.run(query)\n",
    "            summaries = []\n",
    "            for record in results:\n",
    "                descriptions = record[\"descriptions\"]\n",
    "                if descriptions:\n",
    "                    summaries.extend(descriptions)\n",
    "            return list(set(summaries))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simplified Summarization Workflow\n",
    "def summarize_in_chunks(summaries, llm):\n",
    "    # summaries to single text\n",
    "    combined_text = \" \".join(summaries)\n",
    "\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=150)\n",
    "    split_texts = text_splitter.split_text(combined_text)\n",
    "\n",
    "    # summarize each split / chunk\n",
    "    chunk_prompt = PromptTemplate.from_template(\n",
    "        \"\"\"Summarize the following text while preserving key relationships, events, and timelines:\n",
    "        {text}\n",
    "        Helpful Answer:\"\"\"\n",
    "    )\n",
    "    llm_chain = LLMChain(llm=llm, prompt=chunk_prompt)\n",
    "\n",
    "    chunk_summaries = []\n",
    "    for chunk in split_texts:\n",
    "        summary = llm_chain.run({\"text\": chunk})\n",
    "        chunk_summaries.append(summary)\n",
    "\n",
    "    # combination into final summary\n",
    "    final_prompt = PromptTemplate.from_template(\n",
    "        \"\"\"Combine the following summaries into a cohesive final summary:\n",
    "        {text}\n",
    "        Helpful Answer:\"\"\"\n",
    "    )\n",
    "    final_chain = LLMChain(llm=llm, prompt=final_prompt)\n",
    "    final_summary = final_chain.run({\"text\": \" \".join(chunk_summaries)})\n",
    "\n",
    "    return final_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/6f/_m3vtyy92m94t_l4qyl157240000gn/T/ipykernel_37034/3427683880.py:17: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import ChatOpenAI``.\n",
      "  llm = ChatOpenAI(temperature=0, model_name=\"gpt-4\")\n",
      "/var/folders/6f/_m3vtyy92m94t_l4qyl157240000gn/T/ipykernel_37034/3619148993.py:15: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
      "  llm_chain = LLMChain(llm=llm, prompt=chunk_prompt)\n",
      "/var/folders/6f/_m3vtyy92m94t_l4qyl157240000gn/T/ipykernel_37034/3619148993.py:19: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  summary = llm_chain.run({\"text\": chunk})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Summary:\n",
      "The text outlines the terms and conditions for using Apple Software on a supported Mac Computer, including automatic downloads and installations of software changes. The number of copies that can be downloaded depends on the user's license with Apple. The software is licensed under the MPEG-4 Visual Patent Portfolio License for personal and non-commercial use. It also discusses the use of Gracenote software, licensed by MPEG LA, for personal and non-commercial activities, and the use of Setup/Migration Assistant for transferring software between Apple-branded computers. The text warns about potential risks such as loss of profits, corruption or loss of data, and failure to transmit or receive any data. It also mentions the Wallet feature, which allows the storage of virtual representations of credit, debit, and prepaid cards. The text also outlines the terms of use for Apple Pay & Wallet, stating that the user must have employer authorization to bind their employer to these terms. The text also discusses the terms of use for Apple's software, including AMR encoding and decoding functionality, which is not licensed for use in cellular communications infrastructure. Users are prohibited from altering any proprietary notices on the software. The terms also cover the use of Apple Pay and Apple Wallet, which are considered a \"Service\" under the license. The text discusses licensing terms for the use of Apple Software, particularly those that contain AVC encoding and/or decoding functionality. Commercial use of H.264/AVC requires additional licensing, which can be obtained from MPEG LA, LLC. The text outlines the terms of a license for Apple Software obtained from the Mac App Store or through a software update. The license allows for a one-time permanent transfer of all rights to the software to another party, under certain conditions: the software must be transferred with Apple-branded hardware, and the transfer must include all of the software. The software will periodically check with Apple for updates. For UK-based consumers, the license is governed by local laws. The license also prohibits reverse engineering, disassembling, decrypting, modifying, or creating derivative works of the software, unless such restrictions are prohibited by law. The text discusses the terms of Apple's software license. It mentions that fonts can be embedded in content if allowed by the font's embedding restrictions. It clarifies that updates to Apple software are included in the license. The text also states that Apple is not a financial institution and that location data provided by Apple software is for basic navigation and planning only. The software cannot be exported or re-exported unless authorized by U.S. law and the laws of the jurisdiction where the software was obtained. Apple may provide access to third-party software or services as part of the software package or as an upgrade, update, or supplement. Users are not allowed to exploit these services. The text outlines the terms of use for Gracenote data, software, and servers, stating that they should not be exploited except as expressly permitted. It warns that sharing credentials with a third party could lead to unauthorized transactions. The text also states that users should not use Apple Software and Services to spam, defraud, harass, abuse, stalk, threaten, defame or infringe on the rights of others. Apple is not responsible for any misuse by the user or any offensive or illegal messages received as a result of using their software and services. The terms of use can be found on Apple's legal sales support page. The text outlines terms and conditions for Apple's services and software. It states that to obtain a refund, the entire hardware/software package must be returned. Redistribution of Flight Data requires prior written consent from the FAA. It also mentions that Apple does not guarantee uninterrupted or error-free services, continued availability of services, correction of defects, or compatibility with third-party software or services. Some software libraries and third-party software included with Apple Software are free and licensed under the GNU General Public License (GPL) or the GNU Library. The text discusses software licensed under the GNU General Public License (GPL) or the GNU Library/Lesser General Public License (LGPL). It specifies that the software documentation is licensed to U.S. Government end users only as commercial products and with the same rights as other end users. The text also states that Apple, Apple Payment Services, and their affiliates do not assume liability for any commerce activity made using Apple Pay or Wallet. Instead, users are advised to refer to their agreements with their card issuer, payment network, or merchant for any questions or disputes related to their supported cards and associated commerce activities. The text discusses any questions or disputes related to supported cards and associated commerce activities. It specifies that customers who are consumers (those using the Apple Software outside of their trade, business, or profession) may have legal rights in their country of residence that could prevent certain limitations from applying to them. If a lessee subleases the Apple Software, they must fully relinquish exclusive use for use only in specified countries and regions, including Argentina, Aruba, Australia, Austria, Barbados, Belgium, Bermuda, Brazil, Bulgaria, Canada, Cayman Islands, Chile, China mainland, Hong Kong, Taiwan, Colombia, Cyprus, Czech Republic, Denmark, Dominican Republic, Ecuador, El Salvador, Finland, France, Germany, Greece, Grenada, Guatemala, Hungary, Iceland, India, Indonesia, and Ireland. The text discusses the terms of use for Apple-branded software. It states that the software provided with a specific Apple-branded hardware product may not be compatible with other Apple-branded hardware models. The software can be used in virtual operating system environments, but only a single instance or copy can be virtualized. The Lessor, or provider, is responsible for ensuring that the Lessee, or user, complies with all license terms and assists Apple in enforcing compliance. The text also prohibits interference with Apple's verification, storage, or authentication mechanisms. The countries listed at the beginning of the text do not seem to be directly related to the rest of the information. The text discusses the security mechanisms implemented by Apple in its software, services, and content. It mentions digital rights management and signing among these mechanisms. The text also refers to Apple's Wallet service, which is provided with certain implied or statutory warranties and conditions, including merchantability, satisfactory quality, fitness for a particular purpose, accuracy, quiet enjoyment, and non-infringement of third-party rights. Information will be sent to Apple to determine eligibility and prevent fraud. The Wallet service is subject to terms and conditions, which can be accessed through the settings of supported devices.\n"
     ]
    }
   ],
   "source": [
    "# RAG Workflow\n",
    "def rag_summarization_with_chunks(uri, user, password):\n",
    "    # retrieve descriptions\n",
    "    kg = KnowledgeGraph(uri, user, password)\n",
    "    summaries = kg.get_all_summaries()\n",
    "    kg.close()\n",
    "\n",
    "    if not summaries:\n",
    "        print(\"No summaries found in the knowledge graph.\")\n",
    "        return \"No summaries found in the knowledge graph.\"\n",
    "\n",
    "    unique_summaries = list(set(summaries))\n",
    "    # print(\"Unique Descriptions Retrieved:\")\n",
    "    # for idx, summary in enumerate(unique_summaries, start=1):\n",
    "    #     print(f\"{idx}: {summary}\")\n",
    "\n",
    "    llm = ChatOpenAI(temperature=0, model_name=\"gpt-4\")\n",
    "    final_summary = summarize_in_chunks(unique_summaries, llm)\n",
    "    return final_summary\n",
    "\n",
    "# Perform RAG summarization with chunking\n",
    "final_summary = rag_summarization_with_chunks(uri, user, password)\n",
    "print(\"\\nFinal Summary:\")\n",
    "print(final_summary)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.21 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  },
  "vscode": {
   "interpreter": {
    "hash": "397704579725e15f5c7cb49fe5f0341eb7531c82d19f2c29d197e8b64ab5776b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
