{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.9.21\n"
     ]
    }
   ],
   "source": [
    "from platform import python_version\n",
    "\n",
    "print(python_version())\n",
    "\n",
    "# %pip install langchain\n",
    "# %pip install -U langchain-community\n",
    "# %pip install pypdf\n",
    "# %pip install spacy\n",
    "# !python3.9 -m pip install spacy-llm\n",
    "# !python3.9 -m spacy download en_core_web_sm\n",
    "# %pip install scikit-learn\n",
    "# %pip install -U sentence-transformers\n",
    "# %pip install keybert\n",
    "# %pip install transformers sentence-transformers nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import langchain\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.docstore.document import Document\n",
    "import os\n",
    "\n",
    "# Directory containing your PDF files\n",
    "directory_path = 'TCs'\n",
    "\n",
    "# Initialize PyPDFLoader for each PDF in the directory\n",
    "loaders = [PyPDFLoader(os.path.join(directory_path, f)) for f in os.listdir(directory_path) if f.endswith('.pdf')]\n",
    "\n",
    "# Load documents from PDFs\n",
    "tc_docs = []\n",
    "for loader in loaders:\n",
    "    tc_docs.extend(loader.load())\n",
    "\n",
    "# Prepare the content and metadata for each news article as Document objects\n",
    "tc_data = [\n",
    "    Document(\n",
    "        page_content=doc.page_content,  # Assuming this is how you access the page content of the document\n",
    "        metadata={\n",
    "            \"source\": doc.metadata['source'].removeprefix('TCs'),  # Assuming this is the metadata format\n",
    "        }\n",
    "    )\n",
    "    for doc in tc_docs  # Assuming news_docs is a list of objects with page_content and metadata\n",
    "]\n",
    "\n",
    "data = tc_data[0].page_content\n",
    "# print(type(news_articles_data))\n",
    "\n",
    "page_contents = [doc.page_content for doc in tc_docs]\n",
    "combined = \"\".join(page_contents) \n",
    "type(combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500, chunk_overlap=60\n",
    ")\n",
    "# split_docs = text_splitter.split_text(data)\n",
    "split_docs = text_splitter.split_text(combined)\n",
    "print(len(split_docs))\n",
    "\n",
    "# function for functionality of pipeline\n",
    "def split_text(combined_text):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=500,\n",
    "        chunk_overlap=60\n",
    "    )\n",
    "    return text_splitter.split_text(combined_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entity & Relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/spacy_llm/pipeline/llm.py:143: UserWarning: Task supports sharding, but model does not provide context length. Data won't be sharded, prompt might exceed the model's context length. Set context length in your config. If you think spacy-llm should provide the context length for this model automatically, report this to https://github.com/explosion/spacy-llm/issues.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entity counts: Counter({'ORG': 263, 'GPE': 81, 'CARDINAL': 20, 'PERSON': 20,\n",
      "'ORDINAL': 17, 'LAW': 13, 'WORK_OF_ART': 10, 'PRODUCT': 1, 'DATE': 1, 'EVENT':\n",
      "1, 'TIME': 1})\n",
      "Relation counts: Counter({'located_in': 44, 'related_to': 23, 'uses': 23,\n",
      "'provides': 13, 'is_a': 6, 'includes': 6, 'supports': 6, 'refers_to': 6,\n",
      "'affiliated_with': 5, 'has access to': 5, 'is a type of': 4, 'licensed_by': 3,\n",
      "'runs': 3, 'is related to': 3, 'affects': 3, 'applies to': 3, 'included_with':\n",
      "3, 'associated_with': 3, 'governs': 2, 'runs_on': 2, 'produces': 2, 'is\n",
      "compatible with': 2, 'part_of': 2, 'does not endorse': 2, 'provides_service': 2,\n",
      "'owns': 1, 'accompanied by': 1, 'obtained_from': 1, 'subject to': 1, 'set forth\n",
      "in': 1, 'related to': 1, 'ownership': 1, 'produced_by': 1, 'supported_on': 1,\n",
      "'branded_by': 1, 'downloads': 1, 'delivers': 1, 'facilitates': 1, 'authorizes':\n",
      "1, 'stored_on': 1, 'governed_by': 1, 'has_rights_over': 1, 'license_for': 1,\n",
      "'branded_as': 1, 'software_for': 1, 'prohibits': 1, 'brand_association': 1,\n",
      "'time_related_to': 1, 'transfers software to': 1, 'offers': 1, 'modifies': 1,\n",
      "'modification of': 1, 'part of': 1, 'is part of': 1, 'checks_with': 1, 'is a\n",
      "subsidiary of': 1, 'is a product of': 1, 'leases': 1, 'installed_on': 1,\n",
      "'defines': 1, 'subleases': 1, 'complies_with': 1, 'agrees_to': 1, 'enforces': 1,\n",
      "'used_by': 1, 'notifies': 1, 'regulates': 1, 'provides access to': 1,\n",
      "'references': 1, 'is not responsible for': 1, 'license_to': 1, 'source_of': 1,\n",
      "'brand_of': 1, 'provides_update': 1, 'branded': 1, 'enables access to': 1,\n",
      "'owned_by': 1, 'involves': 1, 'cease_use': 1, 'disclaims': 1, 'authorized\n",
      "representative of': 1, 'authorized by': 1, 'obtained under': 1, 'licensed to':\n",
      "1, 'signed_by': 1, 'licensed_under': 1, 'licensed under': 1, 'licensed by': 1,\n",
      "'encoded by': 1, 'contains': 1, 'compliance with': 1, 'consent_required': 1,\n",
      "'not liable': 1, 'obligation': 1, 'distributed_with': 1, 'available_from': 1,\n",
      "'licensed_to': 1, 'is_feature_of': 1, 'is_type_of': 1, 'is provided by': 1,\n",
      "'authorized to bind': 1, 'requires': 1, 'applies_to': 1, 'contracts_with': 1,\n",
      "'disclaims liability': 1})\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import spacy\n",
    "from collections import Counter\n",
    "from pathlib import Path\n",
    "from wasabi import msg\n",
    "from spacy_llm.util import assemble\n",
    "\n",
    "import spacy\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "# traditional spacy NER\n",
    "# def split_document_sent(text):\n",
    "#     nlp = spacy.load(\"en_core_web_sm\")\n",
    "#     doc = nlp(text)\n",
    "#     return [sent.text.strip() for sent in doc.sents] # referencial\n",
    "\n",
    "def summarize_section(text):\n",
    "    # Load SpaCy model\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    doc = nlp(text)\n",
    "\n",
    "    # Extract sentences\n",
    "    sentences = [sent.text.strip() for sent in doc.sents if sent.text.strip()]\n",
    "\n",
    "    # Check if there are meaningful sentences\n",
    "    if not sentences:\n",
    "        return \"No summary available\"\n",
    "\n",
    "    try:\n",
    "        # Compute sentence embeddings using TF-IDF\n",
    "        vectorizer = TfidfVectorizer(stop_words='english')\n",
    "        sentence_vectors = vectorizer.fit_transform(sentences).toarray()\n",
    "\n",
    "        # Calculate average vector and similarity\n",
    "        avg_vector = np.mean(sentence_vectors, axis=0).reshape(1, -1)\n",
    "        similarities = cosine_similarity(avg_vector, sentence_vectors)\n",
    "\n",
    "        # Select the most relevant sentence\n",
    "        most_relevant_index = np.argmax(similarities)\n",
    "        return sentences[most_relevant_index]\n",
    "\n",
    "    except ValueError:  # Handle cases with stop-words-only or empty input\n",
    "        return \"No summary available\"\n",
    "\n",
    "# spacy-llm relationship extraction\n",
    "# def process_text(nlp, text, verbose=False):\n",
    "#     doc = nlp(text)\n",
    "#     if verbose:\n",
    "#         msg.text(f\"Text: {doc.text}\")\n",
    "#         msg.text(f\"Entities: {[(ent.text, ent.label_) for ent in doc.ents]}\")\n",
    "#         msg.text(\"Relations:\")\n",
    "#         for r in doc._.rel:\n",
    "#             msg.text(f\"  - {doc.ents[r.dep]} [{r.relation}] {doc.ents[r.dest]}\")\n",
    "#     return doc\n",
    "\n",
    "def run_pipeline(combined, config_path, examples_path=None, verbose=False):\n",
    "    if not os.getenv(\"OPENAI_API_KEY\"):\n",
    "        msg.fail(\"OPENAI_API_KEY env variable was not found. Set it and try again.\", exits=1)\n",
    "\n",
    "    sections = split_text(combined)\n",
    "    nlp = assemble(config_path, overrides={} if examples_path is None else {\"paths.examples\": str(examples_path)})\n",
    "\n",
    "    # Initialize counters and storage\n",
    "    processed_data = []\n",
    "    entity_counts = Counter()\n",
    "    relation_counts = Counter()\n",
    "\n",
    "    for section in sections:\n",
    "        summary = summarize_section(section)\n",
    "        doc = nlp(summary)\n",
    "        entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
    "        relations = [(doc.ents[r.dep].text, r.relation, doc.ents[r.dest].text) for r in doc._.rel]\n",
    "\n",
    "        # Store processed data\n",
    "        processed_data.append({\n",
    "            'original_text': section,\n",
    "            'summary': summary,\n",
    "            'entities': entities,\n",
    "            'relations': relations})\n",
    "\n",
    "        # Update counters\n",
    "        entity_counts.update([ent[1] for ent in entities])\n",
    "        relation_counts.update([rel[1] for rel in relations])\n",
    "\n",
    "    # Export to JSON\n",
    "    with open('processed_data.json', 'w') as f:\n",
    "        json.dump(processed_data, f)\n",
    "\n",
    "    # Display summary\n",
    "    msg.text(f\"Entity counts: {entity_counts}\")\n",
    "    msg.text(f\"Relation counts: {relation_counts}\")\n",
    "\n",
    "# Set your configuration paths and flags\n",
    "config_path = Path(\"config.cfg\")\n",
    "examples_path = None  # or None if not using few-shot\n",
    "verbose = True\n",
    "\n",
    "# Run the pipeline\n",
    "file = run_pipeline(combined, config_path, None, verbose)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Addition of description per chunk of text as well as simplification "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To the extent that this software may be used to reproduce, modify, publish or \n",
      "distribute materials, it is licensed to you only for reproduction, modiﬁcation, publication and \n",
      "distribution of non-copyrighted materials, materials in which you own the copyright, or materials \n",
      "you are authorized or legally permitted to reproduce, modify, publish or distribute.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'IMPORTANT NOTE: To the extent that this software may be used to reproduce, modify, publish or \\ndistribute materials, it is licensed to you only for reproduction, modiﬁcation, publication and \\ndistribution of non-copyrighted materials, materials in which you own the copyright, or materials \\nyou are authorized or legally permitted to reproduce, modify, publish or distribute. If you are \\nuncertain about your right to copy, modify, publish or distribute any material, you should contact'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "def summarize_section(text):\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    doc = nlp(text)\n",
    "    sentences = [sent.text for sent in doc.sents]\n",
    "\n",
    "    vectorizer = TfidfVectorizer(stop_words='english')\n",
    "    # convert to array in order to perform cosine similarity\n",
    "    sentence_vectors = vectorizer.fit_transform(sentences).toarray()\n",
    "\n",
    "    avg_vector = np.mean(sentence_vectors, axis=0)\n",
    "    avg_vector = avg_vector.reshape(1, -1)\n",
    "    similarities = cosine_similarity(avg_vector, sentence_vectors)\n",
    "\n",
    "    # index with highest cosine similarity to the average vector\n",
    "    most_relevant_index = np.argmax(similarities)\n",
    "\n",
    "    return sentences[most_relevant_index]\n",
    "\n",
    "\n",
    "print(summarize_section(split_docs[2]))\n",
    "split_docs[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simplification with Lexical Simplification with BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/meryjoy/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simplified Text: \n",
      "The user shall be deemed to have consent to the terms and conditions , referred to as the agreement. You agree to be bound by the contract under this contract.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForMaskedLM, AutoTokenizer\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import numpy as np\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Load models\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\", trust_remote_code=True)\n",
    "model = AutoModelForMaskedLM.from_pretrained(\"bert-base-uncased\", trust_remote_code=True)\n",
    "similarity_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "def mask_word(sentence, word):\n",
    "    \"\"\"Replace a word in the sentence with [MASK].\"\"\"\n",
    "    return sentence.replace(word, \"[MASK]\")\n",
    "\n",
    "def generate_candidates(masked_sentence, top_k=5):\n",
    "    \"\"\"Generate replacement candidates using MLM.\"\"\"\n",
    "    inputs = tokenizer(masked_sentence, return_tensors=\"pt\")\n",
    "    outputs = model(**inputs)\n",
    "    logits = outputs.logits\n",
    "\n",
    "    mask_token_index = (inputs.input_ids == tokenizer.mask_token_id).nonzero(as_tuple=True)[1]\n",
    "    mask_logits = logits[0, mask_token_index, :]\n",
    "    top_tokens = mask_logits.topk(top_k, dim=-1).indices[0].tolist()\n",
    "\n",
    "    candidates = [tokenizer.decode([token]).strip() for token in top_tokens]\n",
    "    return candidates\n",
    "\n",
    "def rank_candidates(sentence, word, candidates):\n",
    "    \"\"\"Rank candidates based on semantic similarity and simplicity.\"\"\"\n",
    "    original_embedding = similarity_model.encode(sentence)\n",
    "    rankings = []\n",
    "\n",
    "    for candidate in candidates:\n",
    "        replaced_sentence = sentence.replace(word, candidate)\n",
    "        candidate_embedding = similarity_model.encode(replaced_sentence)\n",
    "        similarity = util.cos_sim(original_embedding, candidate_embedding)[0][0].item()\n",
    "        rankings.append((candidate, similarity))\n",
    "\n",
    "    # Sort by similarity and word length (shorter words preferred for simplicity)\n",
    "    rankings.sort(key=lambda x: (-x[1], len(x[0])))\n",
    "    return [candidate for candidate, _ in rankings]\n",
    "\n",
    "def simplify_text(text, complexity_threshold=5):\n",
    "    \"\"\"Simplify text by replacing complex words.\"\"\"\n",
    "    simplified_sentences = []\n",
    "    sentences = sent_tokenize(text)\n",
    "\n",
    "    for sentence in sentences:\n",
    "        words = word_tokenize(sentence)\n",
    "        for word in words:\n",
    "            if len(word) > complexity_threshold:  # Detect complex words by length\n",
    "                masked_sentence = mask_word(sentence, word)\n",
    "                candidates = generate_candidates(masked_sentence)\n",
    "                ranked_candidates = rank_candidates(sentence, word, candidates)\n",
    "\n",
    "                if ranked_candidates:\n",
    "                    sentence = sentence.replace(word, ranked_candidates[0], 1)\n",
    "        simplified_sentences.append(sentence)\n",
    "\n",
    "    return \" \".join(simplified_sentences)\n",
    "\n",
    "# Example usage\n",
    "text = \"\"\"\n",
    "The user shall be deemed to have consented to the terms and conditions herein referred to as the agreement. \n",
    "You agree to be bound by the obligations under this contract.\n",
    "\"\"\"\n",
    "\n",
    "simplified_text = simplify_text(text)\n",
    "print(\"Simplified Text:\", simplified_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Still work ahead for this part of the project, might be for later, not enough time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Cypher Queries with eFLINT notation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_and_generate_queries2(json_data, file_path):\n",
    "    nodes = {}\n",
    "    relationships = []\n",
    "\n",
    "    # Enhanced mapping function for entity types based on eFLINT elements\n",
    "    def map_eflint_type(entity_type, entity_name):\n",
    "        # we are not really interested in numbers so we delete those\n",
    "        # in the case of links it is better that we classify them as FACT rather than ORG\n",
    "        if 'http://' in entity_type or 'https://' in entity_name:\n",
    "            return 'FACT'\n",
    "\n",
    "        if entity_type in ['CARDINAL', 'ORDINAL']:\n",
    "            return 'NUMBER'\n",
    "\n",
    "\n",
    "        if entity_type == 'ORG':\n",
    "            if 'terms' in entity_name.lower() or 'conditions' in entity_name.lower() or 'agreement' in entity_name.lower():\n",
    "                return 'DUTY'\n",
    "            else:\n",
    "                return 'ACTOR'\n",
    "\n",
    "        if 'section' in entity_name.lower() or 'section' in entity_type.lower():\n",
    "            return 'SECTION'\n",
    "        \n",
    "        mapping = {\n",
    "            'PERSON': 'ACTOR',\n",
    "            'EVENT': 'EVENT',\n",
    "            'LAW': 'DUTY',\n",
    "            'WORK_OF_ART': 'ACT',\n",
    "            'CONDITION': 'CONDITION',\n",
    "            'DATE': 'DATE',\n",
    "            # they sadly do not appear in the NER we performed but we can nevertheless include them to keep the eFLINT semantics\n",
    "            'CLAIMANT': 'CLAIMANT',\n",
    "            'HOLDER': 'HOLDER'\n",
    "        }\n",
    "        return mapping.get(entity_type, 'FACT') \n",
    "\n",
    "    # entities and relationships\n",
    "    for item in json_data:\n",
    "        entities = item.get('entities', [])\n",
    "        relations = item.get('relations', [])\n",
    "        context_text = item.get('context', '')  # Text surrounding the entities\n",
    "\n",
    "        # Generate section summary for the context\n",
    "        section_summary = summarize_section(context_text) if context_text else \"No summary available\"\n",
    "        \n",
    "        for entity in entities:\n",
    "            entity_name, entity_type = entity[:2]\n",
    "            mapped_type = map_eflint_type(entity_type, entity_name)\n",
    "            node_id = f\"{entity_name.replace(' ', '_')}_{mapped_type}\"\n",
    "            nodes[node_id] = {\n",
    "                'name': entity_name, \n",
    "                'type': mapped_type,\n",
    "                'description': section_summary  # Add the summary as a description\n",
    "            }\n",
    "        \n",
    "        for relation in relations:\n",
    "            src_id = f\"{relation[0].replace(' ', '_')}_{map_eflint_type(entities[0][1], entities[0][0])}\"\n",
    "            tgt_id = f\"{relation[2].replace(' ', '_')}_{map_eflint_type(entities[0][1], entities[0][0])}\"\n",
    "            relationship_type = relation[1].replace(' ', '_').replace('-', '_')\n",
    "            relationships.append((src_id, relationship_type, tgt_id))\n",
    "\n",
    "    node_queries = [\n",
    "        f\"MERGE (n:{data['type']} {{name: '{data['name']}', description: '{data['description']}'}}) SET n.id = '{node_id}'\"\n",
    "        for node_id, data in nodes.items() if data['type'] != 'NUMBER'\n",
    "    ]\n",
    "\n",
    "    relationship_queries = [\n",
    "        f\"MATCH (a), (b) WHERE a.id = '{rel[0]}' AND b.id = '{rel[2]}' \"\n",
    "        f\"MERGE (a)-[:{rel[1]}]->(b)\"\n",
    "        for rel in relationships\n",
    "    ]\n",
    "\n",
    "    queries = node_queries + relationship_queries\n",
    "    # Save queries to a text file\n",
    "    with open(file_path, 'w') as file:\n",
    "        for query in queries:\n",
    "            file.write(query + '\\n')\n",
    "\n",
    "    return queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_and_generate_queries(json_data, file_path):\n",
    "    nodes = {}\n",
    "    relationships = []\n",
    "\n",
    "    # Enhanced mapping function for entity types based on eFLINT elements\n",
    "    def map_eflint_type(entity_type, entity_name):\n",
    "        if 'http://' in entity_type or 'https://' in entity_name:\n",
    "            return 'FACT'\n",
    "        if entity_type in ['CARDINAL', 'ORDINAL']:\n",
    "            return 'NUMBER'\n",
    "        if entity_type == 'ORG':\n",
    "            if 'terms' in entity_name.lower() or 'conditions' in entity_name.lower() or 'agreement' in entity_name.lower():\n",
    "                return 'DUTY'\n",
    "            else:\n",
    "                return 'ACTOR'\n",
    "        if 'section' in entity_name.lower() or 'section' in entity_type.lower():\n",
    "            return 'SECTION'\n",
    "        mapping = {\n",
    "            'PERSON': 'ACTOR',\n",
    "            'EVENT': 'EVENT',\n",
    "            'LAW': 'DUTY',\n",
    "            'WORK_OF_ART': 'ACT',\n",
    "            'CONDITION': 'CONDITION',\n",
    "            'DATE': 'DATE',\n",
    "        }\n",
    "        return mapping.get(entity_type, 'FACT')\n",
    "\n",
    "    # Process summaries and associate them with entities\n",
    "    for item in json_data:\n",
    "        summary = item.get('summary', 'No summary available')\n",
    "        entities = item.get('entities', [])\n",
    "        relations = item.get('relations', [])\n",
    "\n",
    "        for entity in entities:\n",
    "            entity_name, entity_type = entity[:2]\n",
    "            mapped_type = map_eflint_type(entity_type, entity_name)\n",
    "            node_id = f\"{entity_name.replace(' ', '_')}_{mapped_type}\"\n",
    "            if node_id not in nodes:\n",
    "                nodes[node_id] = {\n",
    "                    'name': entity_name,\n",
    "                    'type': mapped_type,\n",
    "                    'descriptions': [summary],  # Start with the current summary\n",
    "                }\n",
    "            else:\n",
    "                nodes[node_id]['descriptions'].append(summary)  # Append additional summaries\n",
    "\n",
    "        # Add relationships if present\n",
    "        for relation in relations:\n",
    "            src_id = f\"{relation[0].replace(' ', '_')}_{map_eflint_type(entity_type, relation[0])}\"\n",
    "            tgt_id = f\"{relation[2].replace(' ', '_')}_{map_eflint_type(entity_type, relation[2])}\"\n",
    "            relationship_type = relation[1].replace(' ', '_').replace('-', '_')\n",
    "            relationships.append((src_id, relationship_type, tgt_id))\n",
    "\n",
    "    # Create Cypher queries for nodes\n",
    "    node_queries = [\n",
    "        f\"\"\"\n",
    "        MERGE (n:{data['type']} {{name: '{data['name']}'}})\n",
    "        SET n.id = '{node_id}', n.descriptions = {json.dumps(data['descriptions'])}\n",
    "        \"\"\"\n",
    "        for node_id, data in nodes.items() if data['type'] != 'NUMBER'\n",
    "    ]\n",
    "\n",
    "    # Create Cypher queries for relationships\n",
    "    relationship_queries = [\n",
    "        f\"\"\"\n",
    "        MATCH (a), (b)\n",
    "        WHERE a.id = '{rel[0]}' AND b.id = '{rel[2]}'\n",
    "        MERGE (a)-[:{rel[1]}]->(b)\n",
    "        \"\"\"\n",
    "        for rel in relationships\n",
    "    ]\n",
    "\n",
    "    queries = node_queries + relationship_queries\n",
    "\n",
    "    # Save queries to a text file\n",
    "    with open(file_path, 'w') as file:\n",
    "        for query in queries:\n",
    "            file.write(query + '\\n')\n",
    "\n",
    "    return queries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('processed_data.json', 'r') as file:\n",
    "    json_data = json.load(file)\n",
    "\n",
    "queries = classify_and_generate_queries(json_data, 'cypher_queries.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neo4j import GraphDatabase\n",
    "\n",
    "def execute_queries(queries, uri, user, password):\n",
    "    driver = GraphDatabase.driver(uri, auth=(user, password))\n",
    "    with driver.session() as session:\n",
    "        for query in queries:\n",
    "            session.run(query)\n",
    "    driver.close()\n",
    "\n",
    "# Example usage\n",
    "uri = \"neo4j://localhost:7687\"\n",
    "user = \"neo4j\"\n",
    "password = \"movies11\"\n",
    "\n",
    "execute_queries(queries, uri, user, password)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other option: \n",
    "### Using Langchain for Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install langchain_openai\n",
    "# %pip install langchain_experimental\n",
    "# %pip install neo4j langchain-neo4j\n",
    "# %pip install yfiles_jupyter_graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.getenv(\"NEO4J_URI\"):\n",
    "    print(\"nope\")\n",
    "if not os.getenv(\"NEO4J_USERNAME\"):\n",
    "    print(\"nope\")\n",
    "if not os.getenv(\"NEO4J_PASSWORD\"):\n",
    "    print(\"nope\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import TokenTextSplitter\n",
    "text_splitter = TokenTextSplitter(chunk_size=512, chunk_overlap=24)\n",
    "tc_document = text_splitter.split_documents(tc_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_experimental.graph_transformers import LLMGraphTransformer\n",
    "\n",
    "llm=ChatOpenAI(temperature=0, model_name=\"gpt-4o-mini\")\n",
    "     \n",
    "llm_transformer = LLMGraphTransformer(llm=llm)\n",
    "\n",
    "graph_documents = llm_transformer.convert_to_graph_documents(tc_document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_neo4j import Neo4jGraph\n",
    "\n",
    "graph = Neo4jGraph()\n",
    "graph.add_graph_documents(\n",
    "    graph_documents,\n",
    "    baseEntityLabel=True,\n",
    "    include_source=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from yfiles_jupyter_graphs import GraphWidget\n",
    "from neo4j import GraphDatabase\n",
    "\n",
    "default_cypher = \"MATCH (s)-[r:!MENTIONS]->(t) RETURN s,r,t LIMIT 80\"\n",
    "\n",
    "def showGraph(cypher: str = default_cypher):\n",
    "    # create a neo4j session to run queries\n",
    "    driver = GraphDatabase.driver(\n",
    "        uri = os.environ[\"NEO4J_URI\"],\n",
    "        auth = (os.environ[\"NEO4J_USERNAME\"],\n",
    "                os.environ[\"NEO4J_PASSWORD\"]))\n",
    "    session = driver.session()\n",
    "    widget = GraphWidget(graph = session.run(cypher).graph())\n",
    "    widget.node_label_mapping = 'id'\n",
    "    display(widget)\n",
    "    return widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# showGraph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarization based RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_neo4j import Neo4jVector\n",
    "from langchain_community.vectorstores.neo4j_vector import remove_lucene_chars\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_openai import OpenAI\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import Tuple, List, Optional\n",
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establishing a connection to the Neo4j Graph\n",
    "vector_index = Neo4jVector.from_existing_graph(\n",
    "    OpenAIEmbeddings(),\n",
    "    search_type=\"hybrid\",\n",
    "    node_label=\"Document\",\n",
    "    text_node_properties=[\"text\"],\n",
    "    embedding_node_property=\"embedding\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the graph query for fulltext index if not already existing\n",
    "graph.query(\"CREATE FULLTEXT INDEX entity IF NOT EXISTS FOR (e:__Entity__) ON EACH [e.id]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BaseModel for extracting entities\n",
    "class Entities(BaseModel):\n",
    "    \"\"\"Identifying information about entities.\"\"\"\n",
    "    names: List[str] = Field(\n",
    "        ...,\n",
    "        description=\"All the person, organization, or business entities that appear in the text\",\n",
    "    )\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"Extracting key entities and information for summarization.\"),\n",
    "        (\"human\", \"Summarize the following content using key entities and relationships: {context}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "llm = OpenAI() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to generate fulltext search query\n",
    "def generate_full_text_query(input: str) -> str:\n",
    "    full_text_query = \"\"\n",
    "    words = [el for el in remove_lucene_chars(input).split() if el]\n",
    "    for word in words[:-1]:\n",
    "        full_text_query += f\"{word}~2 AND\"\n",
    "    full_text_query += f\"{words[-1]}~2\"\n",
    "    return full_text_query.strip()\n",
    "\n",
    "# very basic retriever\n",
    "def structured_retriever(context: str) -> str:\n",
    "    result = \"\"\n",
    "    entities = entity_chain.invoke({\"context\": context})\n",
    "    for entity in entities.names:\n",
    "        response = graph.query(\n",
    "            \"\"\"CALL db.index.fulltext.queryNodes('entity', $query, {limit:2})\n",
    "            YIELD node,score\n",
    "            CALL {\n",
    "                WITH node\n",
    "                MATCH (node)-[r:!MENTIONS]->(neighbor)\n",
    "                RETURN node.id + ' - ' + type(r) + ' -> ' + neighbor.id AS output\n",
    "                UNION ALL\n",
    "                WITH node\n",
    "                MATCH (node)<-[r:!MENTIONS]-(neighbor)\n",
    "                RETURN neighbor.id + ' - ' + type(r) + ' -> ' + node.id AS output\n",
    "            }\n",
    "            RETURN output LIMIT 50\n",
    "            \"\"\",\n",
    "            {\"query\": generate_full_text_query(entity)},\n",
    "        )\n",
    "        result += \"\\n\".join([el['output'] for el in response])\n",
    "    return result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.21 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  },
  "vscode": {
   "interpreter": {
    "hash": "397704579725e15f5c7cb49fe5f0341eb7531c82d19f2c29d197e8b64ab5776b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
